{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/validation/한미반도체.csv')\n",
    "df.rename(columns={'GSTC_CODE': 'code',\n",
    "                   'STCK_PRPR': '종가',\n",
    "                   'CNTG_VOL': '거래량'},\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_number = 96993634\n",
    "df['거래량'] = df['거래량'] / stock_number * 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/validation/한미반도체allTemp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0개 처리 완료\n",
      "1000개 처리 완료\n",
      "2000개 처리 완료\n",
      "3000개 처리 완료\n",
      "4000개 처리 완료\n",
      "5000개 처리 완료\n",
      "6000개 처리 완료\n",
      "7000개 처리 완료\n",
      "8000개 처리 완료\n",
      "9000개 처리 완료\n",
      "10000개 처리 완료\n",
      "11000개 처리 완료\n",
      "12000개 처리 완료\n",
      "13000개 처리 완료\n",
      "14000개 처리 완료\n",
      "15000개 처리 완료\n",
      "16000개 처리 완료\n",
      "17000개 처리 완료\n",
      "18000개 처리 완료\n",
      "19000개 처리 완료\n",
      "20000개 처리 완료\n",
      "21000개 처리 완료\n",
      "22000개 처리 완료\n",
      "23000개 처리 완료\n",
      "24000개 처리 완료\n",
      "25000개 처리 완료\n",
      "26000개 처리 완료\n",
      "27000개 처리 완료\n",
      "28000개 처리 완료\n",
      "29000개 처리 완료\n",
      "30000개 처리 완료\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     40\u001b[0m     processed_row \u001b[38;5;241m=\u001b[39m divide_volumes(row)\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mprocessed_row\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cnt \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m개 처리 완료\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final\\lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final\\lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final\\lib\\site-packages\\pandas\\io\\common.py:157\u001b[0m, in \u001b[0;36mIOHandles.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    153\u001b[0m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m     traceback: TracebackType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final\\lib\\site-packages\\pandas\\io\\common.py:144\u001b[0m, in \u001b[0;36mIOHandles.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles:\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# ##    정렬\n",
    "# df['날짜'] = pd.to_datetime(df['날짜'], format='%Y%m%d')\n",
    "# df = df.sort_values(by=['code','날짜','시간'])\n",
    "# df = df.reset_index(drop=True)\n",
    "# df = df.iloc[:,1:]\n",
    "\n",
    "# ##    난수 처리\n",
    "# df = df.dropna(subset=['MA120'])\n",
    "\n",
    "# df.to_csv('all.csv')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "##      거래량 스케일링\n",
    "financeList = pd.read_csv('FinanceListPreprocess.csv')\n",
    "name = financeList['종목코드'].tolist()\n",
    "volSum = financeList['상장주식수'].tolist()\n",
    "finance_dict = dict(zip(name, volSum))\n",
    "df = df.iloc[:, 1:]\n",
    "df['code'] = df['code'].astype(str).str.zfill(6)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def divide_volumes(row):\n",
    "    try:\n",
    "        divisor = 15054186\n",
    "        row[['거래량']] = row[['거래량']] * 1000000 / divisor\n",
    "        row['거래량'] = round(row['거래량'], 2)\n",
    "    except KeyError:\n",
    "        print(f\"코드 {row['code']}를 찾지 못했습니다.\")\n",
    "        input()\n",
    "    return row\n",
    "\n",
    "output_file = 'allTemp.csv'\n",
    "df.head(0).to_csv(output_file, index=False)\n",
    "cnt = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    processed_row = divide_volumes(row)\n",
    "    processed_row.to_frame().T.to_csv(output_file, index=False, mode='a', header=False)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(f'{cnt}개 처리 완료')\n",
    "    cnt += 1\n",
    "\n",
    "print(\"데이터 처리 완료\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 112.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n##      난수 및 의미 없는 데이터 삭제\\ndf = pd.read_csv(\\'allVector.csv\\')\\ndf.columns = [f\"Column{i+1}\" for i in range(df.shape[1])]\\nmask = (df.iloc[:, :-1] == 0.0).all(axis=1)\\nnum_zero_rows = mask.sum()\\nprint(f\"마지막 열의 값을 제외하고 다른 값들이 모두 0.0인 행의 갯수: {num_zero_rows}\")\\ndf = df[~mask]\\ndf = df.dropna()\\n \\n##    라벨 비율 맞추기\\nvalue_counts = df[\\'Column53\\'].value_counts()\\nprint(\"각 값의 갯수:\")\\nprint(value_counts)\\n\\nmin_count = value_counts.min()\\nbalanced_df_list = []\\nfor value in value_counts.index:\\n    subset = df[df[\\'Column53\\'] == value].sample(n=min_count, random_state=42)\\n    balanced_df_list.append(subset)\\ndf = pd.concat(balanced_df_list).reset_index(drop=True)\\nprint(len(df))\\ndf.to_csv(\\'readyAll1.csv\\')\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "# stock_names = ['CJ제일제당', 'KT', 'SK텔레콤', 'SK하이닉스', '농심', '삼성전자', '삼양식품', '한미반도체']\n",
    "stock_names = ['한미반도체']\n",
    "for stock_name in stock_names:\n",
    "    df = pd.read_csv(f'./data/validation/{stock_name}allTemp.csv')\n",
    "    df=df[['code','종가', '거래량']]\n",
    "    df.rename(columns={'종가' : 'close','거래량' : 'volume',}, inplace=True)\n",
    "\n",
    "\n",
    "    # Unique 코드 및 카운트 계산\n",
    "    counts = df['code'].value_counts()\n",
    "    counts_dict = counts.to_dict()\n",
    "\n",
    "    # 결과를 저장할 리스트\n",
    "    results = []\n",
    "\n",
    "    for key in tqdm(counts_dict.keys()):\n",
    "        value = counts_dict[key]\n",
    "        for i in range(value - 12):   # 예측 분 -1\n",
    "            start_idx = counts_dict[key] - value + i\n",
    "            end_idx = start_idx + 9  # 묶을 분 -1\n",
    "\n",
    "            # 벡터화하여 연산\n",
    "            vectorList = []\n",
    "            for col in ['close','volume']:\n",
    "                diff = df[col].values[start_idx:end_idx] - df[col].values[start_idx + 1:end_idx + 1]\n",
    "                vectorList.extend(round(d, 2) for d in diff)\n",
    "\n",
    "            openValue = df['close'][start_idx + 12] - df['close'][start_idx + 9]\n",
    "            if openValue > 0:\n",
    "                vectorList.append(2.0)\n",
    "            elif openValue == 0:\n",
    "                vectorList.append(1.0)\n",
    "            else:\n",
    "                vectorList.append(0.0)\n",
    "\n",
    "            results.append(vectorList)\n",
    "\n",
    "    # 결과를 DataFrame으로 변환\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # CSV 파일에 저장\n",
    "    results_df.to_csv(f'./data/validation/preprocessed_{stock_name}.csv', index=False)\n",
    "'''\n",
    "##      난수 및 의미 없는 데이터 삭제\n",
    "df = pd.read_csv('allVector.csv')\n",
    "df.columns = [f\"Column{i+1}\" for i in range(df.shape[1])]\n",
    "mask = (df.iloc[:, :-1] == 0.0).all(axis=1)\n",
    "num_zero_rows = mask.sum()\n",
    "print(f\"마지막 열의 값을 제외하고 다른 값들이 모두 0.0인 행의 갯수: {num_zero_rows}\")\n",
    "df = df[~mask]\n",
    "df = df.dropna()\n",
    " \n",
    "##    라벨 비율 맞추기\n",
    "value_counts = df['Column53'].value_counts()\n",
    "print(\"각 값의 갯수:\")\n",
    "print(value_counts)\n",
    "\n",
    "min_count = value_counts.min()\n",
    "balanced_df_list = []\n",
    "for value in value_counts.index:\n",
    "    subset = df[df['Column53'] == value].sample(n=min_count, random_state=42)\n",
    "    balanced_df_list.append(subset)\n",
    "df = pd.concat(balanced_df_list).reset_index(drop=True)\n",
    "print(len(df))\n",
    "df.to_csv('readyAll1.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available : True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import BatchNorm1d\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "print(f'GPU available : {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1    2    3      4      5      6      7    8\n",
      "0      -200  800 -100 -100  59.83  24.80  12.91  12.31  0.0\n",
      "1       800 -100 -100  100  24.80  12.91  12.31  16.17  0.0\n",
      "2      -100 -100  100  200  12.91  12.31  16.17 -20.34  1.0\n",
      "3      -100  100  200    0  12.31  16.17 -20.34   0.81  2.0\n",
      "4       100  200    0  100  16.17 -20.34   0.81  14.80  2.0\n",
      "...     ...  ...  ...  ...    ...    ...    ...    ...  ...\n",
      "186827    0 -300  100    0  14.29  -3.82   0.61  -2.47  2.0\n",
      "186828 -300  100    0 -100  -3.82   0.61  -2.47   2.43  0.0\n",
      "186829  100    0 -100  100   0.61  -2.47   2.43   0.28  0.0\n",
      "186830    0 -100  100    0  -2.47   2.43   0.28 -13.39  0.0\n",
      "186831 -100  100    0 -100   2.43   0.28 -13.39   9.63  1.0\n",
      "\n",
      "[186832 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('C:/big18/dl-dev/dl-dev/project/allVector.csv')\n",
    "# df = df.iloc[:,2:]\n",
    "# df = df.astype('float16')\n",
    "\n",
    "print(df)\n",
    "# 노드 특징 행렬 (Node feature matrix)\n",
    "x = torch.tensor(df.iloc[:,:-1].values, dtype=torch.float)\n",
    "\n",
    "# 엣지 리스트 (Edge list)\n",
    "edge_index = []\n",
    "num_rows = len(df)\n",
    "for i in range(num_rows - 1):\n",
    "    edge_index.append([i, i + 1])\n",
    "    edge_index.append([i + 1, i])\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# 레이블\n",
    "y = torch.tensor(df['8'].values, dtype=torch.long)\n",
    "\n",
    "# 그래프 데이터 객체\n",
    "# graph_data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# 데이터셋 분리\n",
    "train_indices, val_indices = train_test_split(range(num_rows), test_size=0.2, random_state=42)\n",
    "train_mask = torch.zeros(num_rows, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_rows, dtype=torch.bool)\n",
    "train_mask[train_indices] = 1\n",
    "val_mask[val_indices] = 1\n",
    "\n",
    "train_data = Data(x=x, edge_index=edge_index, y=y)\n",
    "train_data.train_mask = train_mask\n",
    "train_data.val_mask = val_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      복잡 모델\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.bn1 = BatchNorm1d(16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.bn2 = BatchNorm1d(32)\n",
    "        self.conv3 = GCNConv(32, 64)\n",
    "        self.bn3 = BatchNorm1d(64)\n",
    "        self.conv4 = GCNConv(64, num_classes)\n",
    "        # self.bn4 = BatchNorm1d(128)\n",
    "        # self.conv5 = GCNConv(128, num_classes)\n",
    "        # self.bn5 = BatchNorm1d(256)\n",
    "        # self.conv6 = GCNConv(256, num_classes)\n",
    "        # self.bn6 = BatchNorm1d(512)\n",
    "        # self.conv7 = GCNConv(512, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.gelu(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.gelu(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv4(x, edge_index)\n",
    "        # x = self.bn4(x)\n",
    "        # x = F.gelu(x)\n",
    "        # x = self.dropout(x)\n",
    "        \n",
    "        # x = self.conv5(x, edge_index)\n",
    "        # x = self.bn5(x)\n",
    "        # x = F.gelu(x)\n",
    "        # x = self.dropout(x)\n",
    "        \n",
    "        # x = self.conv6(x, edge_index)\n",
    "        # x = self.bn6(x)\n",
    "        # x = F.gelu(x)\n",
    "        # x = self.dropout(x)\n",
    "        \n",
    "        # x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 모델 초기화\n",
    "model = GCN(num_node_features=x.size(1), num_classes=len(y.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1562, Train Accuracy: 0.3733     Val Loss: 5.8250, Val Accuracy: 0.4244    Lr : 0.100000\n",
      "Epoch 10, Loss: 0.9698, Train Accuracy: 0.5346     Val Loss: 0.9904, Val Accuracy: 0.5166    Lr : 0.100000\n",
      "Epoch 20, Loss: 0.8993, Train Accuracy: 0.5726     Val Loss: 1.0015, Val Accuracy: 0.5382    Lr : 0.100000\n",
      "Epoch 30, Loss: 0.8643, Train Accuracy: 0.5887     Val Loss: 0.9111, Val Accuracy: 0.5509    Lr : 0.100000\n",
      "Epoch 40, Loss: 0.8392, Train Accuracy: 0.6013     Val Loss: 0.8869, Val Accuracy: 0.5776    Lr : 0.100000\n",
      "Epoch 50, Loss: 0.8260, Train Accuracy: 0.6129     Val Loss: 0.8979, Val Accuracy: 0.5647    Lr : 0.100000\n",
      "Epoch 60, Loss: 0.8060, Train Accuracy: 0.6227     Val Loss: 0.9544, Val Accuracy: 0.5152    Lr : 0.100000\n",
      "Epoch 70, Loss: 0.7825, Train Accuracy: 0.6355     Val Loss: 0.9136, Val Accuracy: 0.5421    Lr : 0.100000\n",
      "Epoch 80, Loss: 0.7904, Train Accuracy: 0.6377     Val Loss: 0.9202, Val Accuracy: 0.5426    Lr : 0.100000\n",
      "Epoch 90, Loss: 0.7489, Train Accuracy: 0.6557     Val Loss: 0.8237, Val Accuracy: 0.6082    Lr : 0.100000\n",
      "Epoch 100, Loss: 0.7593, Train Accuracy: 0.6485     Val Loss: 0.8339, Val Accuracy: 0.6026    Lr : 0.100000\n",
      "Epoch 110, Loss: 0.7355, Train Accuracy: 0.6643     Val Loss: 0.7371, Val Accuracy: 0.6609    Lr : 0.100000\n",
      "Epoch 120, Loss: 0.7981, Train Accuracy: 0.6318     Val Loss: 0.8434, Val Accuracy: 0.6153    Lr : 0.100000\n",
      "Epoch 130, Loss: 0.7486, Train Accuracy: 0.6542     Val Loss: 0.8330, Val Accuracy: 0.6147    Lr : 0.100000\n",
      "Epoch 140, Loss: 0.7181, Train Accuracy: 0.6739     Val Loss: 0.8171, Val Accuracy: 0.6231    Lr : 0.100000\n",
      "Epoch 150, Loss: 0.7121, Train Accuracy: 0.6770     Val Loss: 0.7268, Val Accuracy: 0.6663    Lr : 0.100000\n",
      "Epoch 160, Loss: 0.7223, Train Accuracy: 0.6746     Val Loss: 0.7347, Val Accuracy: 0.6630    Lr : 0.100000\n",
      "Epoch 170, Loss: 0.7081, Train Accuracy: 0.6785     Val Loss: 0.7188, Val Accuracy: 0.6685    Lr : 0.100000\n",
      "Epoch 180, Loss: 0.7211, Train Accuracy: 0.6717     Val Loss: 0.7351, Val Accuracy: 0.6511    Lr : 0.100000\n",
      "Epoch 190, Loss: 0.7280, Train Accuracy: 0.6688     Val Loss: 0.7357, Val Accuracy: 0.6599    Lr : 0.100000\n",
      "Epoch 200, Loss: 0.7053, Train Accuracy: 0.6812     Val Loss: 0.7446, Val Accuracy: 0.6510    Lr : 0.100000\n",
      "Epoch 210, Loss: 0.7246, Train Accuracy: 0.6744     Val Loss: 0.7086, Val Accuracy: 0.6731    Lr : 0.100000\n",
      "Epoch 220, Loss: 0.7028, Train Accuracy: 0.6840     Val Loss: 0.7223, Val Accuracy: 0.6804    Lr : 0.100000\n",
      "Epoch 230, Loss: 0.7780, Train Accuracy: 0.6438     Val Loss: 0.8287, Val Accuracy: 0.6016    Lr : 0.100000\n",
      "Epoch 240, Loss: 0.7390, Train Accuracy: 0.6650     Val Loss: 0.7564, Val Accuracy: 0.6442    Lr : 0.100000\n",
      "Epoch 250, Loss: 0.8277, Train Accuracy: 0.6122     Val Loss: 0.7983, Val Accuracy: 0.6231    Lr : 0.100000\n",
      "Epoch 260, Loss: 0.7411, Train Accuracy: 0.6547     Val Loss: 0.7769, Val Accuracy: 0.6386    Lr : 0.100000\n",
      "Epoch 270, Loss: 0.7098, Train Accuracy: 0.6794     Val Loss: 0.8870, Val Accuracy: 0.6063    Lr : 0.100000\n",
      "Epoch 280, Loss: 0.7278, Train Accuracy: 0.6686     Val Loss: 0.9214, Val Accuracy: 0.5482    Lr : 0.100000\n",
      "Epoch 290, Loss: 0.6973, Train Accuracy: 0.6845     Val Loss: 0.7608, Val Accuracy: 0.6344    Lr : 0.100000\n",
      "Epoch 300, Loss: 0.7315, Train Accuracy: 0.6657     Val Loss: 0.7831, Val Accuracy: 0.6484    Lr : 0.100000\n",
      "Epoch 310, Loss: 0.7117, Train Accuracy: 0.6776     Val Loss: 0.9821, Val Accuracy: 0.6015    Lr : 0.100000\n",
      "Epoch 320, Loss: 0.6946, Train Accuracy: 0.6871     Val Loss: 0.8047, Val Accuracy: 0.6343    Lr : 0.100000\n",
      "Epoch 330, Loss: 0.7668, Train Accuracy: 0.6485     Val Loss: 0.7884, Val Accuracy: 0.6431    Lr : 0.100000\n",
      "Epoch 340, Loss: 0.7105, Train Accuracy: 0.6753     Val Loss: 0.7439, Val Accuracy: 0.6609    Lr : 0.100000\n",
      "Epoch 350, Loss: 0.6949, Train Accuracy: 0.6849     Val Loss: 0.6970, Val Accuracy: 0.6848    Lr : 0.100000\n",
      "Epoch 360, Loss: 0.7065, Train Accuracy: 0.6790     Val Loss: 0.7298, Val Accuracy: 0.6643    Lr : 0.100000\n",
      "Epoch 370, Loss: 0.7020, Train Accuracy: 0.6845     Val Loss: 0.7397, Val Accuracy: 0.6661    Lr : 0.100000\n",
      "Epoch 380, Loss: 0.6969, Train Accuracy: 0.6859     Val Loss: 0.7336, Val Accuracy: 0.6663    Lr : 0.100000\n",
      "Epoch 390, Loss: 0.6893, Train Accuracy: 0.6878     Val Loss: 0.7229, Val Accuracy: 0.6627    Lr : 0.100000\n",
      "Epoch 400, Loss: 0.7260, Train Accuracy: 0.6710     Val Loss: 0.7300, Val Accuracy: 0.6708    Lr : 0.100000\n",
      "Epoch 410, Loss: 0.7073, Train Accuracy: 0.6826     Val Loss: 0.7032, Val Accuracy: 0.6806    Lr : 0.100000\n",
      "Epoch 420, Loss: 0.6870, Train Accuracy: 0.6910     Val Loss: 0.7172, Val Accuracy: 0.6788    Lr : 0.100000\n",
      "Epoch 430, Loss: 0.7051, Train Accuracy: 0.6789     Val Loss: 0.7031, Val Accuracy: 0.6829    Lr : 0.100000\n",
      "Epoch 440, Loss: 0.6897, Train Accuracy: 0.6910     Val Loss: 0.7260, Val Accuracy: 0.6724    Lr : 0.100000\n",
      "Epoch 450, Loss: 0.7181, Train Accuracy: 0.6778     Val Loss: 0.7158, Val Accuracy: 0.6861    Lr : 0.100000\n",
      "Epoch 460, Loss: 0.6939, Train Accuracy: 0.6876     Val Loss: 0.6872, Val Accuracy: 0.6895    Lr : 0.100000\n",
      "Epoch 470, Loss: 0.6852, Train Accuracy: 0.6934     Val Loss: 0.6827, Val Accuracy: 0.6914    Lr : 0.100000\n",
      "Epoch 480, Loss: 0.6970, Train Accuracy: 0.6821     Val Loss: 0.7281, Val Accuracy: 0.6663    Lr : 0.100000\n",
      "Epoch 490, Loss: 0.6804, Train Accuracy: 0.6921     Val Loss: 0.6941, Val Accuracy: 0.6862    Lr : 0.100000\n",
      "Epoch 500, Loss: 0.6785, Train Accuracy: 0.6934     Val Loss: 0.6840, Val Accuracy: 0.6945    Lr : 0.100000\n",
      "Epoch 510, Loss: 0.7259, Train Accuracy: 0.6709     Val Loss: 0.7808, Val Accuracy: 0.6518    Lr : 0.100000\n",
      "New best model saved with val accuracy: 0.6518\n",
      "Epoch 520, Loss: 0.7007, Train Accuracy: 0.6818     Val Loss: 0.7306, Val Accuracy: 0.6609    Lr : 0.100000\n",
      "New best model saved with val accuracy: 0.6609\n",
      "Epoch 530, Loss: 0.6856, Train Accuracy: 0.6924     Val Loss: 0.6815, Val Accuracy: 0.6938    Lr : 0.100000\n",
      "New best model saved with val accuracy: 0.6938\n",
      "Epoch 540, Loss: 0.6754, Train Accuracy: 0.6969     Val Loss: 0.6898, Val Accuracy: 0.6918    Lr : 0.100000\n",
      "Epoch 550, Loss: 0.7169, Train Accuracy: 0.6639     Val Loss: 0.7262, Val Accuracy: 0.6770    Lr : 0.100000\n",
      "Epoch 560, Loss: 0.6924, Train Accuracy: 0.6862     Val Loss: 0.9831, Val Accuracy: 0.5867    Lr : 0.100000\n",
      "Epoch 570, Loss: 0.6753, Train Accuracy: 0.6970     Val Loss: 0.6849, Val Accuracy: 0.6883    Lr : 0.100000\n",
      "Epoch 580, Loss: 0.6802, Train Accuracy: 0.6935     Val Loss: 0.6944, Val Accuracy: 0.6865    Lr : 0.100000\n",
      "Epoch 590, Loss: 0.6761, Train Accuracy: 0.6952     Val Loss: 0.8135, Val Accuracy: 0.6212    Lr : 0.100000\n",
      "Epoch 600, Loss: 0.6976, Train Accuracy: 0.6889     Val Loss: 0.7267, Val Accuracy: 0.6724    Lr : 0.100000\n",
      "Epoch 610, Loss: 0.6827, Train Accuracy: 0.6954     Val Loss: 0.7030, Val Accuracy: 0.6850    Lr : 0.100000\n",
      "Epoch 620, Loss: 0.7100, Train Accuracy: 0.6807     Val Loss: 0.7040, Val Accuracy: 0.6799    Lr : 0.100000\n",
      "Epoch 630, Loss: 0.6803, Train Accuracy: 0.6918     Val Loss: 0.7294, Val Accuracy: 0.6702    Lr : 0.100000\n",
      "Epoch 640, Loss: 0.6904, Train Accuracy: 0.6896     Val Loss: 0.6942, Val Accuracy: 0.6808    Lr : 0.100000\n",
      "Epoch 650, Loss: 0.6753, Train Accuracy: 0.6969     Val Loss: 0.6934, Val Accuracy: 0.6785    Lr : 0.100000\n",
      "Epoch 660, Loss: 0.6942, Train Accuracy: 0.6828     Val Loss: 0.7215, Val Accuracy: 0.6848    Lr : 0.100000\n",
      "Epoch 670, Loss: 0.6789, Train Accuracy: 0.6953     Val Loss: 0.6903, Val Accuracy: 0.6814    Lr : 0.100000\n",
      "Epoch 680, Loss: 0.7297, Train Accuracy: 0.6667     Val Loss: 0.7829, Val Accuracy: 0.6396    Lr : 0.100000\n",
      "Epoch 690, Loss: 0.6887, Train Accuracy: 0.6898     Val Loss: 0.7005, Val Accuracy: 0.6800    Lr : 0.100000\n",
      "Epoch 700, Loss: 0.6763, Train Accuracy: 0.6966     Val Loss: 0.6837, Val Accuracy: 0.6892    Lr : 0.100000\n",
      "Epoch 710, Loss: 0.7196, Train Accuracy: 0.6775     Val Loss: 0.8436, Val Accuracy: 0.5996    Lr : 0.100000\n",
      "Epoch 720, Loss: 0.7022, Train Accuracy: 0.6861     Val Loss: 0.7395, Val Accuracy: 0.6588    Lr : 0.100000\n",
      "Epoch 730, Loss: 0.6787, Train Accuracy: 0.6973     Val Loss: 0.7767, Val Accuracy: 0.6416    Lr : 0.100000\n",
      "Epoch 740, Loss: 0.6795, Train Accuracy: 0.6977     Val Loss: 0.6884, Val Accuracy: 0.6956    Lr : 0.100000\n",
      "New best model saved with val accuracy: 0.6956\n",
      "Epoch 750, Loss: 0.6900, Train Accuracy: 0.6917     Val Loss: 0.6857, Val Accuracy: 0.6927    Lr : 0.100000\n",
      "Epoch 760, Loss: 0.6742, Train Accuracy: 0.7000     Val Loss: 0.7105, Val Accuracy: 0.6682    Lr : 0.100000\n",
      "Epoch 770, Loss: 0.7293, Train Accuracy: 0.6728     Val Loss: 0.7133, Val Accuracy: 0.6794    Lr : 0.100000\n",
      "Epoch 780, Loss: 0.7217, Train Accuracy: 0.6735     Val Loss: 0.8463, Val Accuracy: 0.6324    Lr : 0.100000\n",
      "Epoch 790, Loss: 0.6970, Train Accuracy: 0.6854     Val Loss: 0.6696, Val Accuracy: 0.6990    Lr : 0.100000\n",
      "New best model saved with val accuracy: 0.6990\n",
      "Epoch 800, Loss: 0.6764, Train Accuracy: 0.6977     Val Loss: 0.8110, Val Accuracy: 0.6322    Lr : 0.100000\n",
      "Epoch 810, Loss: 0.6716, Train Accuracy: 0.7015     Val Loss: 0.7768, Val Accuracy: 0.6356    Lr : 0.100000\n",
      "Epoch 820, Loss: 0.6891, Train Accuracy: 0.6920     Val Loss: 0.6748, Val Accuracy: 0.6951    Lr : 0.100000\n",
      "Epoch 830, Loss: 0.6884, Train Accuracy: 0.6969     Val Loss: 0.7330, Val Accuracy: 0.6696    Lr : 0.100000\n",
      "Epoch 840, Loss: 0.6935, Train Accuracy: 0.6849     Val Loss: 1.3762, Val Accuracy: 0.5638    Lr : 0.100000\n",
      "Epoch 850, Loss: 0.6746, Train Accuracy: 0.6978     Val Loss: 0.7073, Val Accuracy: 0.6819    Lr : 0.100000\n",
      "Epoch 860, Loss: 0.6677, Train Accuracy: 0.7011     Val Loss: 0.6757, Val Accuracy: 0.6978    Lr : 0.100000\n",
      "Epoch 870, Loss: 0.7048, Train Accuracy: 0.6859     Val Loss: 0.8464, Val Accuracy: 0.5909    Lr : 0.100000\n",
      "Epoch 880, Loss: 0.6850, Train Accuracy: 0.6926     Val Loss: 0.9157, Val Accuracy: 0.5922    Lr : 0.100000\n",
      "Epoch 890, Loss: 0.6701, Train Accuracy: 0.6996     Val Loss: 0.6913, Val Accuracy: 0.6890    Lr : 0.100000\n",
      "Epoch 900, Loss: 0.6818, Train Accuracy: 0.6945     Val Loss: 0.7074, Val Accuracy: 0.6820    Lr : 0.100000\n",
      "Epoch 910, Loss: 0.6695, Train Accuracy: 0.6989     Val Loss: 0.7160, Val Accuracy: 0.6726    Lr : 0.100000\n",
      "Epoch 920, Loss: 0.6769, Train Accuracy: 0.6965     Val Loss: 0.7361, Val Accuracy: 0.6664    Lr : 0.100000\n",
      "Epoch 930, Loss: 0.7954, Train Accuracy: 0.6392     Val Loss: 1.6919, Val Accuracy: 0.5613    Lr : 0.100000\n",
      "Epoch 940, Loss: 0.7525, Train Accuracy: 0.6543     Val Loss: 0.8561, Val Accuracy: 0.6132    Lr : 0.100000\n",
      "Epoch 950, Loss: 0.7202, Train Accuracy: 0.6714     Val Loss: 0.8935, Val Accuracy: 0.5903    Lr : 0.100000\n",
      "Epoch 960, Loss: 0.6987, Train Accuracy: 0.6841     Val Loss: 0.8254, Val Accuracy: 0.6286    Lr : 0.100000\n",
      "Epoch 970, Loss: 0.6935, Train Accuracy: 0.6867     Val Loss: 0.7490, Val Accuracy: 0.6538    Lr : 0.100000\n",
      "Epoch 980, Loss: 0.6867, Train Accuracy: 0.6896     Val Loss: 0.7310, Val Accuracy: 0.6813    Lr : 0.100000\n",
      "Epoch 990, Loss: 0.6787, Train Accuracy: 0.6924     Val Loss: 0.7158, Val Accuracy: 0.6743    Lr : 0.100000\n",
      "Epoch 1000, Loss: 0.6797, Train Accuracy: 0.6944     Val Loss: 0.7121, Val Accuracy: 0.6788    Lr : 0.090000\n",
      "Epoch 1010, Loss: 0.6793, Train Accuracy: 0.6967     Val Loss: 0.7169, Val Accuracy: 0.6645    Lr : 0.090000\n",
      "Epoch 1020, Loss: 0.6707, Train Accuracy: 0.6994     Val Loss: 0.6831, Val Accuracy: 0.6915    Lr : 0.090000\n",
      "Epoch 1030, Loss: 0.6973, Train Accuracy: 0.6856     Val Loss: 0.8619, Val Accuracy: 0.6287    Lr : 0.090000\n",
      "Epoch 1040, Loss: 0.6788, Train Accuracy: 0.6938     Val Loss: 0.7437, Val Accuracy: 0.6510    Lr : 0.090000\n",
      "Epoch 1050, Loss: 0.6707, Train Accuracy: 0.6980     Val Loss: 0.6651, Val Accuracy: 0.6997    Lr : 0.090000\n",
      "New best model saved with val accuracy: 0.6997\n",
      "Epoch 1060, Loss: 0.6946, Train Accuracy: 0.6837     Val Loss: 0.7838, Val Accuracy: 0.6240    Lr : 0.090000\n",
      "Epoch 1070, Loss: 0.6740, Train Accuracy: 0.6970     Val Loss: 0.7067, Val Accuracy: 0.6743    Lr : 0.090000\n",
      "Epoch 1080, Loss: 0.6649, Train Accuracy: 0.7009     Val Loss: 0.7928, Val Accuracy: 0.6341    Lr : 0.090000\n",
      "Epoch 1090, Loss: 0.6785, Train Accuracy: 0.6958     Val Loss: 0.7920, Val Accuracy: 0.6380    Lr : 0.090000\n",
      "Epoch 1100, Loss: 0.6698, Train Accuracy: 0.7005     Val Loss: 0.6998, Val Accuracy: 0.6712    Lr : 0.090000\n",
      "Epoch 1110, Loss: 0.6661, Train Accuracy: 0.7021     Val Loss: 0.6876, Val Accuracy: 0.6915    Lr : 0.090000\n",
      "Epoch 1120, Loss: 0.6954, Train Accuracy: 0.6891     Val Loss: 0.9024, Val Accuracy: 0.6199    Lr : 0.090000\n",
      "Epoch 1130, Loss: 0.6740, Train Accuracy: 0.6966     Val Loss: 1.1254, Val Accuracy: 0.6114    Lr : 0.090000\n",
      "Epoch 1140, Loss: 0.6697, Train Accuracy: 0.6987     Val Loss: 0.7266, Val Accuracy: 0.6796    Lr : 0.090000\n",
      "Epoch 1150, Loss: 0.6752, Train Accuracy: 0.6968     Val Loss: 0.7177, Val Accuracy: 0.6877    Lr : 0.090000\n",
      "Epoch 1160, Loss: 0.6743, Train Accuracy: 0.6979     Val Loss: 0.7160, Val Accuracy: 0.6736    Lr : 0.090000\n",
      "Epoch 1170, Loss: 0.6637, Train Accuracy: 0.7045     Val Loss: 0.6886, Val Accuracy: 0.6881    Lr : 0.090000\n",
      "Epoch 1180, Loss: 0.6702, Train Accuracy: 0.7008     Val Loss: 0.6720, Val Accuracy: 0.7088    Lr : 0.090000\n",
      "New best model saved with val accuracy: 0.7088\n",
      "Epoch 1190, Loss: 0.6961, Train Accuracy: 0.6838     Val Loss: 0.8589, Val Accuracy: 0.6340    Lr : 0.090000\n",
      "Epoch 1200, Loss: 0.6758, Train Accuracy: 0.6999     Val Loss: 0.7920, Val Accuracy: 0.6575    Lr : 0.090000\n",
      "Epoch 1210, Loss: 0.6902, Train Accuracy: 0.6917     Val Loss: 0.8005, Val Accuracy: 0.6349    Lr : 0.090000\n",
      "Epoch 1220, Loss: 0.6823, Train Accuracy: 0.6923     Val Loss: 0.9083, Val Accuracy: 0.5653    Lr : 0.090000\n",
      "Epoch 1230, Loss: 0.6770, Train Accuracy: 0.6954     Val Loss: 0.7190, Val Accuracy: 0.6727    Lr : 0.090000\n",
      "Epoch 1240, Loss: 0.6707, Train Accuracy: 0.7015     Val Loss: 0.7124, Val Accuracy: 0.6678    Lr : 0.090000\n",
      "Epoch 1250, Loss: 0.6643, Train Accuracy: 0.7018     Val Loss: 0.6809, Val Accuracy: 0.6966    Lr : 0.090000\n",
      "Epoch 1260, Loss: 0.6766, Train Accuracy: 0.6994     Val Loss: 0.7326, Val Accuracy: 0.6589    Lr : 0.090000\n",
      "Epoch 1270, Loss: 0.6668, Train Accuracy: 0.7035     Val Loss: 0.6749, Val Accuracy: 0.6977    Lr : 0.090000\n",
      "Epoch 1280, Loss: 0.6724, Train Accuracy: 0.6977     Val Loss: 0.8000, Val Accuracy: 0.6248    Lr : 0.090000\n",
      "Epoch 1290, Loss: 0.6839, Train Accuracy: 0.6918     Val Loss: 0.7483, Val Accuracy: 0.6612    Lr : 0.090000\n",
      "Epoch 1300, Loss: 0.7213, Train Accuracy: 0.6791     Val Loss: 0.7607, Val Accuracy: 0.6396    Lr : 0.090000\n",
      "Epoch 1310, Loss: 0.6797, Train Accuracy: 0.6949     Val Loss: 1.1217, Val Accuracy: 0.5559    Lr : 0.090000\n",
      "Epoch 1320, Loss: 0.6659, Train Accuracy: 0.7045     Val Loss: 0.7348, Val Accuracy: 0.6773    Lr : 0.090000\n",
      "Epoch 1330, Loss: 0.6686, Train Accuracy: 0.7012     Val Loss: 0.6915, Val Accuracy: 0.6940    Lr : 0.090000\n",
      "Epoch 1340, Loss: 0.6722, Train Accuracy: 0.7022     Val Loss: 0.6752, Val Accuracy: 0.6963    Lr : 0.090000\n",
      "Epoch 1350, Loss: 0.6829, Train Accuracy: 0.6951     Val Loss: 0.6720, Val Accuracy: 0.7014    Lr : 0.090000\n",
      "Epoch 1360, Loss: 0.6730, Train Accuracy: 0.6970     Val Loss: 0.6776, Val Accuracy: 0.6961    Lr : 0.090000\n",
      "Epoch 1370, Loss: 0.6645, Train Accuracy: 0.7019     Val Loss: 0.6578, Val Accuracy: 0.7078    Lr : 0.090000\n",
      "Epoch 1380, Loss: 0.6583, Train Accuracy: 0.7058     Val Loss: 0.6754, Val Accuracy: 0.6889    Lr : 0.090000\n",
      "Epoch 1390, Loss: 0.7004, Train Accuracy: 0.6886     Val Loss: 0.7929, Val Accuracy: 0.6329    Lr : 0.090000\n",
      "Epoch 1400, Loss: 0.6776, Train Accuracy: 0.6944     Val Loss: 1.5389, Val Accuracy: 0.5238    Lr : 0.090000\n",
      "Epoch 1410, Loss: 0.6719, Train Accuracy: 0.6994     Val Loss: 0.6998, Val Accuracy: 0.6926    Lr : 0.090000\n",
      "Epoch 1420, Loss: 0.6594, Train Accuracy: 0.7053     Val Loss: 0.7617, Val Accuracy: 0.6358    Lr : 0.090000\n",
      "Epoch 1430, Loss: 0.7179, Train Accuracy: 0.6783     Val Loss: 0.8735, Val Accuracy: 0.6202    Lr : 0.090000\n",
      "Epoch 1440, Loss: 0.6827, Train Accuracy: 0.6959     Val Loss: 0.7106, Val Accuracy: 0.6819    Lr : 0.090000\n",
      "Epoch 1450, Loss: 0.6649, Train Accuracy: 0.7022     Val Loss: 1.1666, Val Accuracy: 0.5808    Lr : 0.090000\n",
      "Epoch 1460, Loss: 0.6600, Train Accuracy: 0.7049     Val Loss: 0.6748, Val Accuracy: 0.6983    Lr : 0.090000\n",
      "Epoch 1470, Loss: 0.6705, Train Accuracy: 0.7037     Val Loss: 0.7197, Val Accuracy: 0.6666    Lr : 0.090000\n",
      "Epoch 1480, Loss: 0.7047, Train Accuracy: 0.6792     Val Loss: 0.6943, Val Accuracy: 0.6858    Lr : 0.090000\n",
      "Epoch 1490, Loss: 0.6737, Train Accuracy: 0.6968     Val Loss: 1.0338, Val Accuracy: 0.5828    Lr : 0.090000\n",
      "Epoch 1500, Loss: 0.6766, Train Accuracy: 0.6968     Val Loss: 0.8269, Val Accuracy: 0.6309    Lr : 0.090000\n",
      "Epoch 1510, Loss: 0.6739, Train Accuracy: 0.6978     Val Loss: 0.6850, Val Accuracy: 0.6926    Lr : 0.090000\n",
      "Epoch 1520, Loss: 0.6683, Train Accuracy: 0.6996     Val Loss: 0.6867, Val Accuracy: 0.6853    Lr : 0.090000\n",
      "Epoch 1530, Loss: 0.6619, Train Accuracy: 0.7038     Val Loss: 0.6599, Val Accuracy: 0.7079    Lr : 0.090000\n",
      "Epoch 1540, Loss: 0.6606, Train Accuracy: 0.7041     Val Loss: 0.6489, Val Accuracy: 0.7112    Lr : 0.090000\n",
      "New best model saved with val accuracy: 0.7112\n",
      "Epoch 1550, Loss: 0.7052, Train Accuracy: 0.6845     Val Loss: 0.7999, Val Accuracy: 0.6242    Lr : 0.090000\n",
      "Epoch 1560, Loss: 0.6694, Train Accuracy: 0.6993     Val Loss: 1.5028, Val Accuracy: 0.4917    Lr : 0.090000\n",
      "Epoch 1570, Loss: 0.6597, Train Accuracy: 0.7065     Val Loss: 0.7235, Val Accuracy: 0.6564    Lr : 0.090000\n",
      "Epoch 1580, Loss: 0.6692, Train Accuracy: 0.7032     Val Loss: 0.9004, Val Accuracy: 0.5929    Lr : 0.090000\n",
      "Epoch 1590, Loss: 0.6717, Train Accuracy: 0.6983     Val Loss: 0.7386, Val Accuracy: 0.6762    Lr : 0.090000\n",
      "Epoch 1600, Loss: 0.6645, Train Accuracy: 0.7035     Val Loss: 0.6678, Val Accuracy: 0.7031    Lr : 0.090000\n",
      "Epoch 1610, Loss: 0.6877, Train Accuracy: 0.6876     Val Loss: 0.8005, Val Accuracy: 0.6334    Lr : 0.090000\n",
      "Epoch 1620, Loss: 0.6824, Train Accuracy: 0.6938     Val Loss: 0.6970, Val Accuracy: 0.6877    Lr : 0.090000\n",
      "Epoch 1630, Loss: 0.6701, Train Accuracy: 0.7013     Val Loss: 0.8137, Val Accuracy: 0.6494    Lr : 0.090000\n",
      "Epoch 1640, Loss: 0.6711, Train Accuracy: 0.6992     Val Loss: 0.7252, Val Accuracy: 0.6788    Lr : 0.090000\n",
      "Epoch 1650, Loss: 0.6633, Train Accuracy: 0.7043     Val Loss: 0.6794, Val Accuracy: 0.6994    Lr : 0.090000\n",
      "Epoch 1660, Loss: 0.6914, Train Accuracy: 0.6914     Val Loss: 0.7413, Val Accuracy: 0.6604    Lr : 0.090000\n",
      "Epoch 1670, Loss: 0.6753, Train Accuracy: 0.6973     Val Loss: 0.7380, Val Accuracy: 0.6668    Lr : 0.090000\n",
      "Epoch 1680, Loss: 0.6665, Train Accuracy: 0.7026     Val Loss: 0.6825, Val Accuracy: 0.6947    Lr : 0.090000\n",
      "Epoch 1690, Loss: 0.6686, Train Accuracy: 0.7023     Val Loss: 0.6634, Val Accuracy: 0.7021    Lr : 0.090000\n",
      "Epoch 1700, Loss: 0.6673, Train Accuracy: 0.7005     Val Loss: 0.6627, Val Accuracy: 0.7054    Lr : 0.090000\n",
      "Epoch 1710, Loss: 0.6793, Train Accuracy: 0.6970     Val Loss: 0.6842, Val Accuracy: 0.6951    Lr : 0.090000\n",
      "Epoch 1720, Loss: 0.6736, Train Accuracy: 0.7008     Val Loss: 0.7131, Val Accuracy: 0.6685    Lr : 0.090000\n",
      "Epoch 1730, Loss: 0.6818, Train Accuracy: 0.6952     Val Loss: 0.6890, Val Accuracy: 0.6976    Lr : 0.090000\n",
      "Epoch 1740, Loss: 0.6693, Train Accuracy: 0.7018     Val Loss: 0.7076, Val Accuracy: 0.6801    Lr : 0.090000\n",
      "Epoch 1750, Loss: 0.6700, Train Accuracy: 0.7018     Val Loss: 0.7227, Val Accuracy: 0.6628    Lr : 0.090000\n",
      "Epoch 1760, Loss: 0.6601, Train Accuracy: 0.7053     Val Loss: 0.6571, Val Accuracy: 0.7142    Lr : 0.090000\n",
      "New best model saved with val accuracy: 0.7142\n",
      "Epoch 1770, Loss: 0.6821, Train Accuracy: 0.6945     Val Loss: 0.7140, Val Accuracy: 0.6870    Lr : 0.090000\n",
      "Epoch 1780, Loss: 0.6654, Train Accuracy: 0.7033     Val Loss: 0.7765, Val Accuracy: 0.6440    Lr : 0.090000\n",
      "Epoch 1790, Loss: 0.6612, Train Accuracy: 0.7052     Val Loss: 0.6748, Val Accuracy: 0.6971    Lr : 0.090000\n",
      "Epoch 1800, Loss: 0.6940, Train Accuracy: 0.6882     Val Loss: 0.8043, Val Accuracy: 0.6409    Lr : 0.090000\n",
      "Epoch 1810, Loss: 0.6759, Train Accuracy: 0.6950     Val Loss: 0.6810, Val Accuracy: 0.6940    Lr : 0.090000\n",
      "Epoch 1820, Loss: 0.6615, Train Accuracy: 0.7045     Val Loss: 0.6691, Val Accuracy: 0.6967    Lr : 0.090000\n",
      "Epoch 1830, Loss: 0.6647, Train Accuracy: 0.7012     Val Loss: 0.6796, Val Accuracy: 0.7051    Lr : 0.090000\n",
      "Epoch 1840, Loss: 0.7298, Train Accuracy: 0.6708     Val Loss: 0.8977, Val Accuracy: 0.6382    Lr : 0.090000\n",
      "Epoch 1850, Loss: 0.6862, Train Accuracy: 0.6883     Val Loss: 1.1973, Val Accuracy: 0.5974    Lr : 0.090000\n",
      "Epoch 1860, Loss: 0.6680, Train Accuracy: 0.7015     Val Loss: 0.9892, Val Accuracy: 0.6124    Lr : 0.090000\n",
      "Epoch 1870, Loss: 0.6595, Train Accuracy: 0.7060     Val Loss: 0.7334, Val Accuracy: 0.6561    Lr : 0.090000\n",
      "Epoch 1880, Loss: 0.6622, Train Accuracy: 0.7047     Val Loss: 0.7469, Val Accuracy: 0.6449    Lr : 0.090000\n",
      "Epoch 1890, Loss: 0.6607, Train Accuracy: 0.7040     Val Loss: 0.6914, Val Accuracy: 0.6796    Lr : 0.090000\n",
      "Epoch 1900, Loss: 0.6668, Train Accuracy: 0.7047     Val Loss: 0.7028, Val Accuracy: 0.6815    Lr : 0.090000\n",
      "Epoch 1910, Loss: 0.6656, Train Accuracy: 0.7053     Val Loss: 0.7291, Val Accuracy: 0.6855    Lr : 0.090000\n",
      "Epoch 1920, Loss: 0.7002, Train Accuracy: 0.6838     Val Loss: 0.8141, Val Accuracy: 0.6331    Lr : 0.090000\n",
      "Epoch 1930, Loss: 0.6804, Train Accuracy: 0.6944     Val Loss: 0.7491, Val Accuracy: 0.6607    Lr : 0.090000\n",
      "Epoch 1940, Loss: 0.6603, Train Accuracy: 0.7065     Val Loss: 0.6960, Val Accuracy: 0.6693    Lr : 0.090000\n",
      "Epoch 1950, Loss: 0.6758, Train Accuracy: 0.6954     Val Loss: 0.7120, Val Accuracy: 0.6718    Lr : 0.090000\n",
      "Epoch 1960, Loss: 0.6618, Train Accuracy: 0.7027     Val Loss: 0.6712, Val Accuracy: 0.7052    Lr : 0.090000\n",
      "Epoch 1970, Loss: 0.6618, Train Accuracy: 0.7046     Val Loss: 0.7003, Val Accuracy: 0.6888    Lr : 0.090000\n",
      "Epoch 1980, Loss: 0.7255, Train Accuracy: 0.6832     Val Loss: 0.7532, Val Accuracy: 0.6579    Lr : 0.090000\n",
      "Epoch 1990, Loss: 0.6787, Train Accuracy: 0.6941     Val Loss: 0.7291, Val Accuracy: 0.6703    Lr : 0.090000\n",
      "Epoch 2000, Loss: 0.6609, Train Accuracy: 0.7048     Val Loss: 0.6917, Val Accuracy: 0.6856    Lr : 0.081000\n",
      "Epoch 2010, Loss: 0.6581, Train Accuracy: 0.7056     Val Loss: 0.6839, Val Accuracy: 0.6677    Lr : 0.081000\n",
      "Epoch 2020, Loss: 0.6656, Train Accuracy: 0.7022     Val Loss: 0.6739, Val Accuracy: 0.7035    Lr : 0.081000\n",
      "Epoch 2030, Loss: 0.6584, Train Accuracy: 0.7076     Val Loss: 0.7646, Val Accuracy: 0.6534    Lr : 0.081000\n",
      "Epoch 2040, Loss: 0.6583, Train Accuracy: 0.7050     Val Loss: 0.7377, Val Accuracy: 0.6698    Lr : 0.081000\n",
      "Epoch 2050, Loss: 0.6665, Train Accuracy: 0.7025     Val Loss: 0.6698, Val Accuracy: 0.6947    Lr : 0.081000\n",
      "Epoch 2060, Loss: 0.6689, Train Accuracy: 0.7012     Val Loss: 0.6767, Val Accuracy: 0.7028    Lr : 0.081000\n",
      "Epoch 2070, Loss: 0.6840, Train Accuracy: 0.6898     Val Loss: 1.8031, Val Accuracy: 0.5233    Lr : 0.081000\n",
      "Epoch 2080, Loss: 0.6587, Train Accuracy: 0.7043     Val Loss: 0.8885, Val Accuracy: 0.6073    Lr : 0.081000\n",
      "Epoch 2090, Loss: 0.6557, Train Accuracy: 0.7073     Val Loss: 1.1049, Val Accuracy: 0.5710    Lr : 0.081000\n",
      "Epoch 2100, Loss: 0.6547, Train Accuracy: 0.7080     Val Loss: 0.6611, Val Accuracy: 0.7105    Lr : 0.081000\n",
      "Epoch 2110, Loss: 0.6734, Train Accuracy: 0.7031     Val Loss: 0.6630, Val Accuracy: 0.7044    Lr : 0.081000\n",
      "Epoch 2120, Loss: 0.6703, Train Accuracy: 0.7016     Val Loss: 0.6662, Val Accuracy: 0.6953    Lr : 0.081000\n",
      "Epoch 2130, Loss: 0.6608, Train Accuracy: 0.7063     Val Loss: 0.6678, Val Accuracy: 0.6947    Lr : 0.081000\n",
      "Epoch 2140, Loss: 0.7145, Train Accuracy: 0.6877     Val Loss: 0.7921, Val Accuracy: 0.6393    Lr : 0.081000\n",
      "Epoch 2150, Loss: 0.6766, Train Accuracy: 0.6973     Val Loss: 0.8639, Val Accuracy: 0.6542    Lr : 0.081000\n",
      "Epoch 2160, Loss: 0.6610, Train Accuracy: 0.7047     Val Loss: 0.6699, Val Accuracy: 0.7017    Lr : 0.081000\n",
      "Epoch 2170, Loss: 0.6534, Train Accuracy: 0.7093     Val Loss: 0.7674, Val Accuracy: 0.6376    Lr : 0.081000\n",
      "Epoch 2180, Loss: 0.7101, Train Accuracy: 0.6878     Val Loss: 0.8610, Val Accuracy: 0.6114    Lr : 0.081000\n",
      "Epoch 2190, Loss: 0.6833, Train Accuracy: 0.6924     Val Loss: 0.9156, Val Accuracy: 0.6063    Lr : 0.081000\n",
      "Epoch 2200, Loss: 0.6698, Train Accuracy: 0.7002     Val Loss: 0.7614, Val Accuracy: 0.6435    Lr : 0.081000\n",
      "Epoch 2210, Loss: 0.6615, Train Accuracy: 0.7042     Val Loss: 0.6969, Val Accuracy: 0.6577    Lr : 0.081000\n",
      "Epoch 2220, Loss: 0.6558, Train Accuracy: 0.7082     Val Loss: 0.6951, Val Accuracy: 0.6818    Lr : 0.081000\n",
      "Epoch 2230, Loss: 0.6599, Train Accuracy: 0.7054     Val Loss: 0.7054, Val Accuracy: 0.6895    Lr : 0.081000\n",
      "Epoch 2240, Loss: 0.6676, Train Accuracy: 0.7021     Val Loss: 0.6986, Val Accuracy: 0.6867    Lr : 0.081000\n",
      "Epoch 2250, Loss: 0.6651, Train Accuracy: 0.7053     Val Loss: 0.7420, Val Accuracy: 0.6721    Lr : 0.081000\n",
      "Epoch 2260, Loss: 0.6590, Train Accuracy: 0.7084     Val Loss: 0.7537, Val Accuracy: 0.6635    Lr : 0.081000\n",
      "Epoch 2270, Loss: 0.6614, Train Accuracy: 0.7037     Val Loss: 0.7865, Val Accuracy: 0.6610    Lr : 0.081000\n",
      "Epoch 2280, Loss: 0.6764, Train Accuracy: 0.6965     Val Loss: 1.0397, Val Accuracy: 0.5998    Lr : 0.081000\n",
      "Epoch 2290, Loss: 0.6663, Train Accuracy: 0.7028     Val Loss: 0.7085, Val Accuracy: 0.6872    Lr : 0.081000\n",
      "Epoch 2300, Loss: 0.6553, Train Accuracy: 0.7084     Val Loss: 0.6516, Val Accuracy: 0.7101    Lr : 0.081000\n",
      "Epoch 2310, Loss: 0.6692, Train Accuracy: 0.6990     Val Loss: 0.6720, Val Accuracy: 0.7075    Lr : 0.081000\n",
      "Epoch 2320, Loss: 0.6723, Train Accuracy: 0.7038     Val Loss: 0.6857, Val Accuracy: 0.6992    Lr : 0.081000\n",
      "Epoch 2330, Loss: 0.6677, Train Accuracy: 0.7050     Val Loss: 0.6938, Val Accuracy: 0.6912    Lr : 0.081000\n",
      "Epoch 2340, Loss: 0.6589, Train Accuracy: 0.7090     Val Loss: 0.6762, Val Accuracy: 0.6910    Lr : 0.081000\n",
      "Epoch 2350, Loss: 0.7211, Train Accuracy: 0.6732     Val Loss: 0.7414, Val Accuracy: 0.6765    Lr : 0.081000\n",
      "Epoch 2360, Loss: 0.7046, Train Accuracy: 0.6812     Val Loss: 2.9959, Val Accuracy: 0.4476    Lr : 0.081000\n",
      "Epoch 2370, Loss: 0.6777, Train Accuracy: 0.6959     Val Loss: 1.1327, Val Accuracy: 0.5859    Lr : 0.081000\n",
      "Epoch 2380, Loss: 0.6642, Train Accuracy: 0.7041     Val Loss: 1.1675, Val Accuracy: 0.5476    Lr : 0.081000\n",
      "Epoch 2390, Loss: 0.6572, Train Accuracy: 0.7074     Val Loss: 0.7128, Val Accuracy: 0.6769    Lr : 0.081000\n",
      "Epoch 2400, Loss: 0.6602, Train Accuracy: 0.7057     Val Loss: 0.7224, Val Accuracy: 0.6659    Lr : 0.081000\n",
      "Epoch 2410, Loss: 0.6749, Train Accuracy: 0.6986     Val Loss: 0.6855, Val Accuracy: 0.6913    Lr : 0.081000\n",
      "Epoch 2420, Loss: 0.6579, Train Accuracy: 0.7054     Val Loss: 0.6799, Val Accuracy: 0.6995    Lr : 0.081000\n",
      "Epoch 2430, Loss: 0.6583, Train Accuracy: 0.7053     Val Loss: 0.6859, Val Accuracy: 0.6882    Lr : 0.081000\n",
      "Epoch 2440, Loss: 0.6958, Train Accuracy: 0.6860     Val Loss: 0.7776, Val Accuracy: 0.6679    Lr : 0.081000\n",
      "Epoch 2450, Loss: 0.6868, Train Accuracy: 0.6911     Val Loss: 0.7650, Val Accuracy: 0.6435    Lr : 0.081000\n",
      "Epoch 2460, Loss: 0.6646, Train Accuracy: 0.7009     Val Loss: 0.6641, Val Accuracy: 0.6985    Lr : 0.081000\n",
      "Epoch 2470, Loss: 0.6571, Train Accuracy: 0.7081     Val Loss: 0.6745, Val Accuracy: 0.6941    Lr : 0.081000\n",
      "Epoch 2480, Loss: 0.6671, Train Accuracy: 0.7004     Val Loss: 0.7111, Val Accuracy: 0.6741    Lr : 0.081000\n",
      "Epoch 2490, Loss: 0.6543, Train Accuracy: 0.7101     Val Loss: 0.6707, Val Accuracy: 0.7038    Lr : 0.081000\n",
      "Epoch 2500, Loss: 0.7025, Train Accuracy: 0.6820     Val Loss: 0.7572, Val Accuracy: 0.6523    Lr : 0.081000\n",
      "Epoch 2510, Loss: 0.6783, Train Accuracy: 0.6948     Val Loss: 0.7377, Val Accuracy: 0.6771    Lr : 0.081000\n",
      "Epoch 2520, Loss: 0.6618, Train Accuracy: 0.7039     Val Loss: 0.6806, Val Accuracy: 0.6947    Lr : 0.081000\n",
      "Epoch 2530, Loss: 0.6574, Train Accuracy: 0.7045     Val Loss: 0.7028, Val Accuracy: 0.6601    Lr : 0.081000\n",
      "Epoch 2540, Loss: 0.6744, Train Accuracy: 0.6980     Val Loss: 0.6956, Val Accuracy: 0.6979    Lr : 0.081000\n",
      "Epoch 2550, Loss: 0.6601, Train Accuracy: 0.7052     Val Loss: 0.6720, Val Accuracy: 0.7017    Lr : 0.081000\n",
      "Epoch 2560, Loss: 0.6559, Train Accuracy: 0.7069     Val Loss: 0.6619, Val Accuracy: 0.6980    Lr : 0.081000\n",
      "Epoch 2570, Loss: 0.7324, Train Accuracy: 0.6682     Val Loss: 0.8311, Val Accuracy: 0.6094    Lr : 0.081000\n",
      "Epoch 2580, Loss: 0.6900, Train Accuracy: 0.6887     Val Loss: 1.0103, Val Accuracy: 0.6079    Lr : 0.081000\n",
      "Epoch 2590, Loss: 0.6646, Train Accuracy: 0.7028     Val Loss: 0.7622, Val Accuracy: 0.6591    Lr : 0.081000\n",
      "Epoch 2600, Loss: 0.6550, Train Accuracy: 0.7079     Val Loss: 0.7127, Val Accuracy: 0.6769    Lr : 0.081000\n",
      "Epoch 2610, Loss: 0.6814, Train Accuracy: 0.6975     Val Loss: 1.6969, Val Accuracy: 0.4589    Lr : 0.081000\n",
      "Epoch 2620, Loss: 0.6623, Train Accuracy: 0.7030     Val Loss: 0.6831, Val Accuracy: 0.6925    Lr : 0.081000\n",
      "Epoch 2630, Loss: 0.6543, Train Accuracy: 0.7080     Val Loss: 0.6859, Val Accuracy: 0.6799    Lr : 0.081000\n",
      "Epoch 2640, Loss: 0.6747, Train Accuracy: 0.7032     Val Loss: 0.8740, Val Accuracy: 0.6305    Lr : 0.081000\n",
      "Epoch 2650, Loss: 0.6600, Train Accuracy: 0.7058     Val Loss: 0.6822, Val Accuracy: 0.6993    Lr : 0.081000\n",
      "Epoch 2660, Loss: 0.6869, Train Accuracy: 0.6925     Val Loss: 0.6669, Val Accuracy: 0.7106    Lr : 0.081000\n",
      "Epoch 2670, Loss: 0.6598, Train Accuracy: 0.7054     Val Loss: 0.6721, Val Accuracy: 0.7019    Lr : 0.081000\n",
      "Epoch 2680, Loss: 0.6825, Train Accuracy: 0.6961     Val Loss: 0.6737, Val Accuracy: 0.6982    Lr : 0.081000\n",
      "Epoch 2690, Loss: 0.6684, Train Accuracy: 0.7046     Val Loss: 0.6632, Val Accuracy: 0.7035    Lr : 0.081000\n",
      "Epoch 2700, Loss: 0.6605, Train Accuracy: 0.7089     Val Loss: 0.7526, Val Accuracy: 0.6637    Lr : 0.081000\n",
      "Epoch 2710, Loss: 0.6846, Train Accuracy: 0.6915     Val Loss: 0.8301, Val Accuracy: 0.6397    Lr : 0.081000\n",
      "Epoch 2720, Loss: 0.6657, Train Accuracy: 0.7051     Val Loss: 0.7816, Val Accuracy: 0.6469    Lr : 0.081000\n",
      "Epoch 2730, Loss: 0.6582, Train Accuracy: 0.7071     Val Loss: 0.6643, Val Accuracy: 0.7099    Lr : 0.081000\n",
      "Epoch 2740, Loss: 0.6599, Train Accuracy: 0.7070     Val Loss: 0.6985, Val Accuracy: 0.6892    Lr : 0.081000\n",
      "Epoch 2750, Loss: 0.6788, Train Accuracy: 0.6957     Val Loss: 0.7235, Val Accuracy: 0.6650    Lr : 0.081000\n",
      "Epoch 2760, Loss: 0.6590, Train Accuracy: 0.7072     Val Loss: 0.7251, Val Accuracy: 0.6746    Lr : 0.081000\n",
      "Early stopping at epoch 2760\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)  #   optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)  # 학습률&정규화 조정 (model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "\n",
    "# 조기 종료 설정\n",
    "early_stopping_patience = 100    # *10\n",
    "early_stopping_counter = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "# 정확도 계산 함수\n",
    "def accuracy(pred, labels):\n",
    "    _, pred_classes = pred.max(dim=1)\n",
    "    correct = (pred_classes == labels).sum().item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "model.train()\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data)\n",
    "    loss = F.nll_loss(out[train_data.train_mask], train_data.y[train_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # 평가 모드로 전환하여 테스트 셋에서의 성능을 평가\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(train_data)\n",
    "        val_loss = F.nll_loss(val_out[train_data.val_mask], train_data.y[train_data.val_mask])\n",
    "        val_acc = accuracy(val_out[train_data.val_mask], train_data.y[train_data.val_mask])\n",
    "\n",
    "    # 모델을 다시 훈련 모드로 전환\n",
    "    model.train()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc = accuracy(out[train_data.train_mask], train_data.y[train_data.train_mask])\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}     Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_acc:.4f}    Lr : {current_lr:.6f}')\n",
    "        if epoch > 500:\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), 'gcn.pth')\n",
    "                early_stopping_counter = 0\n",
    "                print(f'New best model saved with val accuracy: {best_acc:.4f}')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F06\\AppData\\Local\\Temp\\ipykernel_22572\\2458309604.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('C:/big18/dl-dev/dl-dev/gcn.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[8964 3881  965]\n",
      " [2302 5421 2577]\n",
      " [ 873 3549 8835]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.65      0.69     13810\n",
      "           1       0.42      0.53      0.47     10300\n",
      "           2       0.71      0.67      0.69     13257\n",
      "\n",
      "    accuracy                           0.62     37367\n",
      "   macro avg       0.62      0.61      0.62     37367\n",
      "weighted avg       0.64      0.62      0.63     37367\n",
      "\n",
      "Final Eval Loss: 0.8692, Final Eval Accuracy: 0.6214\n"
     ]
    }
   ],
   "source": [
    "# 학습 완료 후 최종 평가\n",
    "model.load_state_dict(torch.load('C:/big18/dl-dev/dl-dev/gcn.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_out = model(train_data)\n",
    "    final_loss = F.nll_loss(final_out[train_data.val_mask], train_data.y[train_data.val_mask])\n",
    "    final_acc = accuracy(final_out[train_data.val_mask], train_data.y[train_data.val_mask])\n",
    "    val_out = model(train_data)\n",
    "    val_pred = val_out[train_data.val_mask].max(dim=1)[1].cpu().numpy()\n",
    "    val_true = train_data.y[train_data.val_mask].cpu().numpy()\n",
    "\n",
    "conf_matrix = confusion_matrix(val_true, val_pred)\n",
    "class_report = classification_report(val_true, val_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "print(f'Final Eval Loss: {final_loss.item():.4f}, Final Eval Accuracy: {final_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

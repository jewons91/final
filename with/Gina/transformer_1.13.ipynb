{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20분 단위로 묶기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149454, 20, 3)\n",
      "(149454,)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 전처리\n",
    "def create_classification_targets(data, window_size=20, pred_offset=3):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window_size - pred_offset + 1):\n",
    "        sequence = data.iloc[i:i+window_size]\n",
    "        scaled_sequence = scale_within_sequence(sequence)\n",
    "        X.append(scaled_sequence)\n",
    "        # 3분 뒤의 Close 가격 변화에 따른 클래스 설정\n",
    "        future_close = data.iloc[i+window_size+pred_offset-1]['Close']\n",
    "        current_close = data.iloc[i+window_size-1]['Close']\n",
    "        \n",
    "        # 가격 변화에 따라 클래스 설정: 상승(2), 보합(1), 하락(0)\n",
    "        if future_close > current_close:\n",
    "            y.append(2)  # 상승\n",
    "        elif future_close < current_close:\n",
    "            y.append(0)  # 하락\n",
    "        else:\n",
    "            y.append(1)  # 보합\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 데이터 불러오기 및 전처리\n",
    "data = pd.read_csv('./000660.csv', parse_dates=['Unnamed: 0'])\n",
    "data = data.rename(columns={'Unnamed: 0': 'Time', '매수량': 'BuyVolume', '매도량': 'SellVolume', '종가': 'Close', '저가': 'Low'})\n",
    "data['Power'] = data['BuyVolume'] - data['SellVolume']\n",
    "columns = ['Close', 'Power', 'Low']\n",
    "data = data[columns].dropna()\n",
    "\n",
    "# 데이터 정규화\n",
    "# Power 전체 스케일\n",
    "volume_scaler = RobustScaler()\n",
    "data['Scaled_Power'] = volume_scaler.fit_transform(data[['Power']])\n",
    "\n",
    "# 시퀀스내 스케일 종가, 저가\n",
    "def scale_within_sequence(sequence):\n",
    "    # 종가 스케일 시퀀스내\n",
    "    close_scaler = RobustScaler()\n",
    "    close_scaled = close_scaler.fit_transform(sequence[['Close']].values)\n",
    "    \n",
    "    # 저가 스케일 시퀀스내\n",
    "    low_scaled = close_scaler.transform(sequence[['Low']].values)\n",
    "    \n",
    "    # 거래량 미리 스케일값 사용\n",
    "    power_scaled = sequence[['Scaled_Power']].values\n",
    "    \n",
    "    return np.hstack([close_scaled, low_scaled, power_scaled])\n",
    "\n",
    "\n",
    "X, y = create_classification_targets(data)\n",
    "\n",
    "# 학습/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)  # (samples, 10, 3)\n",
    "print(y_train.shape)  # (samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52587 45742 51125]\n"
     ]
    }
   ],
   "source": [
    "# y 클래스 데이터 분포 확인\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 가중치 계산\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Positional Encoding 구현\n",
    "def positional_encoding(max_len, d_model):\n",
    "    pos = np.arange(max_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "\n",
    "    # 짝수 인덱스에 대해 sin 적용\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 홀수 인덱스에 대해 cos 적용\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    return pos_encoding\n",
    "\n",
    "def add_positional_encoding(inputs, max_len, d_model):\n",
    "    pos_encoding = positional_encoding(max_len, d_model)\n",
    "    pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    pos_encoded_inputs = inputs + pos_encoding[:tf.shape(inputs)[1], :]\n",
    "    return pos_encoded_inputs\n",
    "\n",
    "# 3. Transformer 블록 구현\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"gelu\", kernel_initializer='he_normal'), layers.Dense(d_model)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output1 = self.att1(inputs, inputs)\n",
    "        attn_output1 = self.dropout1(attn_output1)\n",
    "        out1 = self.layernorm1(inputs + attn_output1)\n",
    "        # attn_output2 = self.att2(out1, out1)\n",
    "        # attn_output2 = self.dropout2(attn_output2)\n",
    "        # out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout1(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# 4. Transformer 모델 구성\n",
    "def create_transformer_model(input_shape, num_heads, ff_dim, d_model, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # 임베딩 및 위치 인코딩\n",
    "    embedding_layer = layers.Dense(d_model, kernel_initializer='he_normal')(inputs)\n",
    "    pos_encoded_inputs = add_positional_encoding(embedding_layer, max_len=input_shape[0], d_model=d_model)\n",
    "\n",
    "    # Transformer 블록 적용\n",
    "    transformer_block = TransformerBlock(d_model, num_heads, ff_dim)\n",
    "    x = transformer_block(pos_encoded_inputs)\n",
    "    \n",
    "    # 출력 레이어\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation=\"gelu\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Positional Encoding 구현\n",
    "def positional_encoding(max_len, d_model):\n",
    "    pos = np.arange(max_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "\n",
    "    # 짝수 인덱스에 대해 sin 적용\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 홀수 인덱스에 대해 cos 적용\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    return pos_encoding\n",
    "\n",
    "def add_positional_encoding(inputs, max_len, d_model):\n",
    "    pos_encoding = positional_encoding(max_len, d_model)\n",
    "    pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    pos_encoded_inputs = inputs + pos_encoding[:tf.shape(inputs)[1], :]\n",
    "    return pos_encoded_inputs\n",
    "\n",
    "# 3. Transformer 블록 구현\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"gelu\", kernel_initializer='he_normal'), layers.Dense(d_model)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output1 = self.att1(inputs, inputs)\n",
    "        attn_output1 = self.dropout1(attn_output1)\n",
    "        out1 = self.layernorm1(inputs + attn_output1)\n",
    "        # attn_output2 = self.att2(out1, out1)\n",
    "        # attn_output2 = self.dropout2(attn_output2)\n",
    "        # out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout1(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# 4. Transformer 모델 구성\n",
    "def create_transformer_model(input_shape, num_heads, ff_dim, d_model, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # 임베딩 및 위치 인코딩\n",
    "    embedding_layer = layers.Dense(d_model, kernel_initializer='he_normal')(inputs)\n",
    "    pos_encoded_inputs = add_positional_encoding(embedding_layer, max_len=input_shape[0], d_model=d_model)\n",
    "\n",
    "    # Transformer 블록 적용\n",
    "    transformer_block = TransformerBlock(d_model, num_heads, ff_dim)\n",
    "    x = transformer_block(pos_encoded_inputs)\n",
    "    \n",
    "    # 출력 레이어\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation=\"gelu\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=100,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model_5.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.8, \n",
    "    patience=50, \n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 3)]      0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20, 128)      512         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (3,)                0           ['dense[0][0]']                  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.stack (TFOpLambda)          (2,)                 0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.strided_slice (TFOpLambda)  (20, 128)            0           ['tf.stack[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 20, 128)     0           ['dense[0][0]',                  \n",
      " da)                                                              'tf.strided_slice[0][0]']       \n",
      "                                                                                                  \n",
      " transformer_block (Transformer  (None, 20, 128)     297344      ['tf.__operators__.add[0][0]']   \n",
      " Block)                                                                                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['transformer_block[0][0]']      \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          16512       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3)            387         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 314,755\n",
      "Trainable params: 314,755\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0900 - accuracy: 0.3714\n",
      "Epoch 1: val_loss improved from inf to 1.10197, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 21s 16ms/step - loss: 1.0900 - accuracy: 0.3714 - val_loss: 1.1020 - val_accuracy: 0.3576 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0848 - accuracy: 0.3756\n",
      "Epoch 2: val_loss improved from 1.10197 to 1.08216, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0848 - accuracy: 0.3757 - val_loss: 1.0822 - val_accuracy: 0.3813 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0816 - accuracy: 0.3819\n",
      "Epoch 3: val_loss improved from 1.08216 to 1.05721, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0816 - accuracy: 0.3819 - val_loss: 1.0572 - val_accuracy: 0.4306 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0557 - accuracy: 0.4279\n",
      "Epoch 4: val_loss improved from 1.05721 to 1.05224, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0557 - accuracy: 0.4279 - val_loss: 1.0522 - val_accuracy: 0.4307 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0489 - accuracy: 0.4394\n",
      "Epoch 5: val_loss improved from 1.05224 to 1.04453, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0489 - accuracy: 0.4394 - val_loss: 1.0445 - val_accuracy: 0.4432 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0472 - accuracy: 0.4441\n",
      "Epoch 6: val_loss did not improve from 1.04453\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0472 - accuracy: 0.4441 - val_loss: 1.0477 - val_accuracy: 0.4465 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0465 - accuracy: 0.4437\n",
      "Epoch 7: val_loss improved from 1.04453 to 1.04383, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0465 - accuracy: 0.4437 - val_loss: 1.0438 - val_accuracy: 0.4485 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0446 - accuracy: 0.4446\n",
      "Epoch 8: val_loss improved from 1.04383 to 1.04271, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0446 - accuracy: 0.4446 - val_loss: 1.0427 - val_accuracy: 0.4445 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0440 - accuracy: 0.4460\n",
      "Epoch 9: val_loss did not improve from 1.04271\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0440 - accuracy: 0.4460 - val_loss: 1.0444 - val_accuracy: 0.4457 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0436 - accuracy: 0.4461\n",
      "Epoch 10: val_loss did not improve from 1.04271\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0436 - accuracy: 0.4462 - val_loss: 1.0430 - val_accuracy: 0.4460 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0416 - accuracy: 0.4487\n",
      "Epoch 11: val_loss improved from 1.04271 to 1.03680, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0416 - accuracy: 0.4487 - val_loss: 1.0368 - val_accuracy: 0.4543 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0412 - accuracy: 0.4487\n",
      "Epoch 12: val_loss did not improve from 1.03680\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0412 - accuracy: 0.4487 - val_loss: 1.0382 - val_accuracy: 0.4522 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0414 - accuracy: 0.4493\n",
      "Epoch 13: val_loss did not improve from 1.03680\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0414 - accuracy: 0.4493 - val_loss: 1.0386 - val_accuracy: 0.4549 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0410 - accuracy: 0.4483\n",
      "Epoch 14: val_loss did not improve from 1.03680\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0410 - accuracy: 0.4484 - val_loss: 1.0380 - val_accuracy: 0.4510 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0404 - accuracy: 0.4502\n",
      "Epoch 15: val_loss improved from 1.03680 to 1.03409, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0403 - accuracy: 0.4502 - val_loss: 1.0341 - val_accuracy: 0.4572 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0404 - accuracy: 0.4504\n",
      "Epoch 16: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0404 - accuracy: 0.4504 - val_loss: 1.0381 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0395 - accuracy: 0.4493\n",
      "Epoch 17: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0395 - accuracy: 0.4493 - val_loss: 1.0381 - val_accuracy: 0.4489 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0396 - accuracy: 0.4506\n",
      "Epoch 18: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0396 - accuracy: 0.4507 - val_loss: 1.0404 - val_accuracy: 0.4488 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0392 - accuracy: 0.4507\n",
      "Epoch 19: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0392 - accuracy: 0.4507 - val_loss: 1.0407 - val_accuracy: 0.4504 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0393 - accuracy: 0.4503\n",
      "Epoch 20: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0392 - accuracy: 0.4504 - val_loss: 1.0403 - val_accuracy: 0.4516 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0404 - accuracy: 0.4489\n",
      "Epoch 21: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0403 - accuracy: 0.4490 - val_loss: 1.0465 - val_accuracy: 0.4384 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0395 - accuracy: 0.4502\n",
      "Epoch 22: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0395 - accuracy: 0.4502 - val_loss: 1.0344 - val_accuracy: 0.4578 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0390 - accuracy: 0.4512\n",
      "Epoch 23: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0390 - accuracy: 0.4512 - val_loss: 1.0367 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0381 - accuracy: 0.4527\n",
      "Epoch 24: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0381 - accuracy: 0.4527 - val_loss: 1.0367 - val_accuracy: 0.4509 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0381 - accuracy: 0.4517\n",
      "Epoch 25: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0382 - accuracy: 0.4517 - val_loss: 1.0397 - val_accuracy: 0.4511 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0389 - accuracy: 0.4495\n",
      "Epoch 26: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0389 - accuracy: 0.4495 - val_loss: 1.0404 - val_accuracy: 0.4470 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0376 - accuracy: 0.4513\n",
      "Epoch 27: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0377 - accuracy: 0.4512 - val_loss: 1.0397 - val_accuracy: 0.4486 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0387 - accuracy: 0.4502\n",
      "Epoch 28: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0387 - accuracy: 0.4502 - val_loss: 1.0425 - val_accuracy: 0.4460 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0381 - accuracy: 0.4519\n",
      "Epoch 29: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0381 - accuracy: 0.4520 - val_loss: 1.0364 - val_accuracy: 0.4548 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0379 - accuracy: 0.4528\n",
      "Epoch 30: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0380 - accuracy: 0.4527 - val_loss: 1.0374 - val_accuracy: 0.4516 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0375 - accuracy: 0.4522\n",
      "Epoch 31: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0375 - accuracy: 0.4521 - val_loss: 1.0372 - val_accuracy: 0.4502 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0382 - accuracy: 0.4502\n",
      "Epoch 32: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0382 - accuracy: 0.4503 - val_loss: 1.0355 - val_accuracy: 0.4518 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0383 - accuracy: 0.4499\n",
      "Epoch 33: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0383 - accuracy: 0.4499 - val_loss: 1.0355 - val_accuracy: 0.4536 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0378 - accuracy: 0.4516\n",
      "Epoch 34: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0378 - accuracy: 0.4516 - val_loss: 1.0384 - val_accuracy: 0.4568 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0368 - accuracy: 0.4527\n",
      "Epoch 35: val_loss did not improve from 1.03409\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0367 - accuracy: 0.4527 - val_loss: 1.0369 - val_accuracy: 0.4532 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0374 - accuracy: 0.4531\n",
      "Epoch 36: val_loss improved from 1.03409 to 1.03260, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0374 - accuracy: 0.4531 - val_loss: 1.0326 - val_accuracy: 0.4588 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0369 - accuracy: 0.4545\n",
      "Epoch 37: val_loss did not improve from 1.03260\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0369 - accuracy: 0.4545 - val_loss: 1.0415 - val_accuracy: 0.4468 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0377 - accuracy: 0.4510\n",
      "Epoch 38: val_loss did not improve from 1.03260\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0378 - accuracy: 0.4509 - val_loss: 1.0344 - val_accuracy: 0.4576 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0365 - accuracy: 0.4534\n",
      "Epoch 39: val_loss did not improve from 1.03260\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0365 - accuracy: 0.4533 - val_loss: 1.0380 - val_accuracy: 0.4483 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0370 - accuracy: 0.4524\n",
      "Epoch 40: val_loss did not improve from 1.03260\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0370 - accuracy: 0.4524 - val_loss: 1.0344 - val_accuracy: 0.4565 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0369 - accuracy: 0.4528\n",
      "Epoch 41: val_loss improved from 1.03260 to 1.03246, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0368 - accuracy: 0.4528 - val_loss: 1.0325 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.4543\n",
      "Epoch 42: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0364 - accuracy: 0.4543 - val_loss: 1.0372 - val_accuracy: 0.4479 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0366 - accuracy: 0.4538\n",
      "Epoch 43: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0366 - accuracy: 0.4538 - val_loss: 1.0405 - val_accuracy: 0.4430 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0365 - accuracy: 0.4533\n",
      "Epoch 44: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0365 - accuracy: 0.4533 - val_loss: 1.0354 - val_accuracy: 0.4546 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0360 - accuracy: 0.4533\n",
      "Epoch 45: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0360 - accuracy: 0.4534 - val_loss: 1.0341 - val_accuracy: 0.4581 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0361 - accuracy: 0.4535\n",
      "Epoch 46: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0361 - accuracy: 0.4535 - val_loss: 1.0326 - val_accuracy: 0.4575 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0361 - accuracy: 0.4539\n",
      "Epoch 47: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0361 - accuracy: 0.4539 - val_loss: 1.0390 - val_accuracy: 0.4470 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0363 - accuracy: 0.4521\n",
      "Epoch 48: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0363 - accuracy: 0.4521 - val_loss: 1.0392 - val_accuracy: 0.4485 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.4537\n",
      "Epoch 49: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0359 - accuracy: 0.4537 - val_loss: 1.0385 - val_accuracy: 0.4521 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.4548\n",
      "Epoch 50: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0358 - accuracy: 0.4549 - val_loss: 1.0380 - val_accuracy: 0.4455 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0358 - accuracy: 0.4528\n",
      "Epoch 51: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0358 - accuracy: 0.4528 - val_loss: 1.0367 - val_accuracy: 0.4567 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0360 - accuracy: 0.4536\n",
      "Epoch 52: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0361 - accuracy: 0.4535 - val_loss: 1.0431 - val_accuracy: 0.4577 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.4540\n",
      "Epoch 53: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0360 - accuracy: 0.4539 - val_loss: 1.0335 - val_accuracy: 0.4596 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0357 - accuracy: 0.4545\n",
      "Epoch 54: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0357 - accuracy: 0.4545 - val_loss: 1.0326 - val_accuracy: 0.4595 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.4544\n",
      "Epoch 55: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0360 - accuracy: 0.4543 - val_loss: 1.0333 - val_accuracy: 0.4546 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0354 - accuracy: 0.4532\n",
      "Epoch 56: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0354 - accuracy: 0.4532 - val_loss: 1.0357 - val_accuracy: 0.4515 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0362 - accuracy: 0.4516\n",
      "Epoch 57: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0362 - accuracy: 0.4517 - val_loss: 1.0337 - val_accuracy: 0.4532 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0358 - accuracy: 0.4528\n",
      "Epoch 58: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0358 - accuracy: 0.4528 - val_loss: 1.0361 - val_accuracy: 0.4520 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0356 - accuracy: 0.4536\n",
      "Epoch 59: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0356 - accuracy: 0.4536 - val_loss: 1.0334 - val_accuracy: 0.4566 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.4538\n",
      "Epoch 60: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4538 - val_loss: 1.0397 - val_accuracy: 0.4535 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0361 - accuracy: 0.4524\n",
      "Epoch 61: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0361 - accuracy: 0.4524 - val_loss: 1.0365 - val_accuracy: 0.4503 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0365 - accuracy: 0.4537\n",
      "Epoch 62: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0365 - accuracy: 0.4537 - val_loss: 1.0342 - val_accuracy: 0.4587 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0356 - accuracy: 0.4544\n",
      "Epoch 63: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0356 - accuracy: 0.4543 - val_loss: 1.0379 - val_accuracy: 0.4464 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0358 - accuracy: 0.4548\n",
      "Epoch 64: val_loss did not improve from 1.03246\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0358 - accuracy: 0.4549 - val_loss: 1.0348 - val_accuracy: 0.4569 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4540\n",
      "Epoch 65: val_loss improved from 1.03246 to 1.03210, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0355 - accuracy: 0.4540 - val_loss: 1.0321 - val_accuracy: 0.4593 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.4527\n",
      "Epoch 66: val_loss improved from 1.03210 to 1.03200, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0356 - accuracy: 0.4526 - val_loss: 1.0320 - val_accuracy: 0.4557 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0360 - accuracy: 0.4538\n",
      "Epoch 67: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0360 - accuracy: 0.4538 - val_loss: 1.0357 - val_accuracy: 0.4577 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.4546\n",
      "Epoch 68: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0357 - accuracy: 0.4546 - val_loss: 1.0334 - val_accuracy: 0.4554 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0358 - accuracy: 0.4545\n",
      "Epoch 69: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0358 - accuracy: 0.4545 - val_loss: 1.0339 - val_accuracy: 0.4535 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0355 - accuracy: 0.4538\n",
      "Epoch 70: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0355 - accuracy: 0.4538 - val_loss: 1.0445 - val_accuracy: 0.4396 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0358 - accuracy: 0.4540\n",
      "Epoch 71: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0358 - accuracy: 0.4540 - val_loss: 1.0404 - val_accuracy: 0.4482 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0360 - accuracy: 0.4540\n",
      "Epoch 72: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0360 - accuracy: 0.4540 - val_loss: 1.0337 - val_accuracy: 0.4567 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4539\n",
      "Epoch 73: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0354 - accuracy: 0.4539 - val_loss: 1.0383 - val_accuracy: 0.4519 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.4539\n",
      "Epoch 74: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0353 - accuracy: 0.4540 - val_loss: 1.0375 - val_accuracy: 0.4539 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0360 - accuracy: 0.4529\n",
      "Epoch 75: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0360 - accuracy: 0.4529 - val_loss: 1.0376 - val_accuracy: 0.4469 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4532\n",
      "Epoch 76: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0355 - accuracy: 0.4531 - val_loss: 1.0324 - val_accuracy: 0.4584 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4536\n",
      "Epoch 77: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0356 - accuracy: 0.4534 - val_loss: 1.0362 - val_accuracy: 0.4538 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.4529\n",
      "Epoch 78: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0357 - accuracy: 0.4529 - val_loss: 1.0343 - val_accuracy: 0.4542 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.4546\n",
      "Epoch 79: val_loss did not improve from 1.03200\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0364 - accuracy: 0.4546 - val_loss: 1.0327 - val_accuracy: 0.4566 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0352 - accuracy: 0.4559\n",
      "Epoch 80: val_loss improved from 1.03200 to 1.03191, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0352 - accuracy: 0.4559 - val_loss: 1.0319 - val_accuracy: 0.4613 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4543\n",
      "Epoch 81: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4543 - val_loss: 1.0355 - val_accuracy: 0.4541 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.4538\n",
      "Epoch 82: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0353 - accuracy: 0.4537 - val_loss: 1.0346 - val_accuracy: 0.4567 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4540\n",
      "Epoch 83: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0354 - accuracy: 0.4540 - val_loss: 1.0394 - val_accuracy: 0.4488 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4535\n",
      "Epoch 84: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4535 - val_loss: 1.0368 - val_accuracy: 0.4470 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4546\n",
      "Epoch 85: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4546 - val_loss: 1.0345 - val_accuracy: 0.4574 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4546\n",
      "Epoch 86: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0352 - accuracy: 0.4546 - val_loss: 1.0347 - val_accuracy: 0.4556 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4537\n",
      "Epoch 87: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0354 - accuracy: 0.4537 - val_loss: 1.0377 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0353 - accuracy: 0.4537\n",
      "Epoch 88: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0353 - accuracy: 0.4537 - val_loss: 1.0382 - val_accuracy: 0.4527 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4533\n",
      "Epoch 89: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0352 - accuracy: 0.4533 - val_loss: 1.0329 - val_accuracy: 0.4571 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4545\n",
      "Epoch 90: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4546 - val_loss: 1.0331 - val_accuracy: 0.4571 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0358 - accuracy: 0.4546\n",
      "Epoch 91: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0358 - accuracy: 0.4546 - val_loss: 1.0341 - val_accuracy: 0.4535 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0353 - accuracy: 0.4541\n",
      "Epoch 92: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0353 - accuracy: 0.4541 - val_loss: 1.0407 - val_accuracy: 0.4418 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4554\n",
      "Epoch 93: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4554 - val_loss: 1.0329 - val_accuracy: 0.4559 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4550\n",
      "Epoch 94: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4551 - val_loss: 1.0339 - val_accuracy: 0.4556 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4553\n",
      "Epoch 95: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4553 - val_loss: 1.0357 - val_accuracy: 0.4576 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4561\n",
      "Epoch 96: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4561 - val_loss: 1.0337 - val_accuracy: 0.4514 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4546\n",
      "Epoch 97: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0351 - accuracy: 0.4545 - val_loss: 1.0328 - val_accuracy: 0.4609 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4545\n",
      "Epoch 98: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4545 - val_loss: 1.0343 - val_accuracy: 0.4540 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4549\n",
      "Epoch 99: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4548 - val_loss: 1.0326 - val_accuracy: 0.4547 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4533\n",
      "Epoch 100: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0354 - accuracy: 0.4532 - val_loss: 1.0390 - val_accuracy: 0.4518 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4544\n",
      "Epoch 101: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0352 - accuracy: 0.4544 - val_loss: 1.0389 - val_accuracy: 0.4456 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4548\n",
      "Epoch 102: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4548 - val_loss: 1.0358 - val_accuracy: 0.4560 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4561\n",
      "Epoch 103: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0345 - accuracy: 0.4561 - val_loss: 1.0367 - val_accuracy: 0.4557 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0342 - accuracy: 0.4554\n",
      "Epoch 104: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4554 - val_loss: 1.0358 - val_accuracy: 0.4538 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4558\n",
      "Epoch 105: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4557 - val_loss: 1.0410 - val_accuracy: 0.4461 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4553\n",
      "Epoch 106: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0349 - accuracy: 0.4553 - val_loss: 1.0320 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4546\n",
      "Epoch 107: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0347 - accuracy: 0.4546 - val_loss: 1.0396 - val_accuracy: 0.4443 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4554\n",
      "Epoch 108: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0349 - accuracy: 0.4553 - val_loss: 1.0345 - val_accuracy: 0.4529 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0354 - accuracy: 0.4548\n",
      "Epoch 109: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0354 - accuracy: 0.4548 - val_loss: 1.0335 - val_accuracy: 0.4591 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0350 - accuracy: 0.4553\n",
      "Epoch 110: val_loss did not improve from 1.03191\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4553 - val_loss: 1.0350 - val_accuracy: 0.4550 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.4535\n",
      "Epoch 111: val_loss improved from 1.03191 to 1.03153, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0353 - accuracy: 0.4534 - val_loss: 1.0315 - val_accuracy: 0.4604 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4543\n",
      "Epoch 112: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0355 - accuracy: 0.4542 - val_loss: 1.0359 - val_accuracy: 0.4510 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4559\n",
      "Epoch 113: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0351 - accuracy: 0.4559 - val_loss: 1.0332 - val_accuracy: 0.4563 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4539\n",
      "Epoch 114: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0352 - accuracy: 0.4539 - val_loss: 1.0352 - val_accuracy: 0.4478 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4552\n",
      "Epoch 115: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0350 - accuracy: 0.4553 - val_loss: 1.0386 - val_accuracy: 0.4540 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0347 - accuracy: 0.4541\n",
      "Epoch 116: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0347 - accuracy: 0.4541 - val_loss: 1.0420 - val_accuracy: 0.4516 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4548\n",
      "Epoch 117: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0352 - accuracy: 0.4547 - val_loss: 1.0337 - val_accuracy: 0.4529 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0371 - accuracy: 0.4530\n",
      "Epoch 118: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0371 - accuracy: 0.4530 - val_loss: 1.0362 - val_accuracy: 0.4513 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0357 - accuracy: 0.4546\n",
      "Epoch 119: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0357 - accuracy: 0.4546 - val_loss: 1.0356 - val_accuracy: 0.4515 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4543\n",
      "Epoch 120: val_loss did not improve from 1.03153\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0355 - accuracy: 0.4542 - val_loss: 1.0361 - val_accuracy: 0.4544 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4542\n",
      "Epoch 121: val_loss improved from 1.03153 to 1.03084, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0355 - accuracy: 0.4542 - val_loss: 1.0308 - val_accuracy: 0.4586 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0371 - accuracy: 0.4533\n",
      "Epoch 122: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0371 - accuracy: 0.4533 - val_loss: 1.0364 - val_accuracy: 0.4527 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0356 - accuracy: 0.4549\n",
      "Epoch 123: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0356 - accuracy: 0.4549 - val_loss: 1.0324 - val_accuracy: 0.4557 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4559\n",
      "Epoch 124: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0355 - accuracy: 0.4559 - val_loss: 1.0360 - val_accuracy: 0.4520 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0348 - accuracy: 0.4558\n",
      "Epoch 125: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4558 - val_loss: 1.0326 - val_accuracy: 0.4568 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4548\n",
      "Epoch 126: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0349 - accuracy: 0.4549 - val_loss: 1.0476 - val_accuracy: 0.4505 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4542\n",
      "Epoch 127: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0354 - accuracy: 0.4542 - val_loss: 1.0331 - val_accuracy: 0.4589 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4559\n",
      "Epoch 128: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0355 - accuracy: 0.4560 - val_loss: 1.0382 - val_accuracy: 0.4530 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0355 - accuracy: 0.4548\n",
      "Epoch 129: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0355 - accuracy: 0.4548 - val_loss: 1.0374 - val_accuracy: 0.4570 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.4551\n",
      "Epoch 130: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0359 - accuracy: 0.4551 - val_loss: 1.0389 - val_accuracy: 0.4561 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0362 - accuracy: 0.4548\n",
      "Epoch 131: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0363 - accuracy: 0.4548 - val_loss: 1.0317 - val_accuracy: 0.4639 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0353 - accuracy: 0.4551\n",
      "Epoch 132: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0353 - accuracy: 0.4551 - val_loss: 1.0345 - val_accuracy: 0.4552 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4540\n",
      "Epoch 133: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0352 - accuracy: 0.4540 - val_loss: 1.0337 - val_accuracy: 0.4542 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4558\n",
      "Epoch 134: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0349 - accuracy: 0.4558 - val_loss: 1.0404 - val_accuracy: 0.4543 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.4535\n",
      "Epoch 135: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0353 - accuracy: 0.4535 - val_loss: 1.0340 - val_accuracy: 0.4575 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0356 - accuracy: 0.4533\n",
      "Epoch 136: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0357 - accuracy: 0.4533 - val_loss: 1.0321 - val_accuracy: 0.4582 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4541\n",
      "Epoch 137: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0354 - accuracy: 0.4541 - val_loss: 1.0319 - val_accuracy: 0.4613 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4546\n",
      "Epoch 138: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0352 - accuracy: 0.4545 - val_loss: 1.0366 - val_accuracy: 0.4530 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.4543\n",
      "Epoch 139: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0364 - accuracy: 0.4544 - val_loss: 1.0376 - val_accuracy: 0.4537 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.4548\n",
      "Epoch 140: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0359 - accuracy: 0.4547 - val_loss: 1.0394 - val_accuracy: 0.4527 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.4532\n",
      "Epoch 141: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0353 - accuracy: 0.4532 - val_loss: 1.0320 - val_accuracy: 0.4563 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0353 - accuracy: 0.4555\n",
      "Epoch 142: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0353 - accuracy: 0.4555 - val_loss: 1.0355 - val_accuracy: 0.4519 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4552\n",
      "Epoch 143: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4552 - val_loss: 1.0372 - val_accuracy: 0.4479 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.4552\n",
      "Epoch 144: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0353 - accuracy: 0.4552 - val_loss: 1.0328 - val_accuracy: 0.4550 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4545\n",
      "Epoch 145: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0355 - accuracy: 0.4545 - val_loss: 1.0347 - val_accuracy: 0.4578 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4543\n",
      "Epoch 146: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4543 - val_loss: 1.0341 - val_accuracy: 0.4558 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4541\n",
      "Epoch 147: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4541 - val_loss: 1.0364 - val_accuracy: 0.4539 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4542\n",
      "Epoch 148: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0354 - accuracy: 0.4542 - val_loss: 1.0326 - val_accuracy: 0.4597 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0346 - accuracy: 0.4549\n",
      "Epoch 149: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0346 - accuracy: 0.4549 - val_loss: 1.0337 - val_accuracy: 0.4577 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4559\n",
      "Epoch 150: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0350 - accuracy: 0.4558 - val_loss: 1.0333 - val_accuracy: 0.4579 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4543\n",
      "Epoch 151: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4544 - val_loss: 1.0346 - val_accuracy: 0.4555 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.4544\n",
      "Epoch 152: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4544 - val_loss: 1.0345 - val_accuracy: 0.4547 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4539\n",
      "Epoch 153: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0347 - accuracy: 0.4539 - val_loss: 1.0345 - val_accuracy: 0.4544 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.4539\n",
      "Epoch 154: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0357 - accuracy: 0.4538 - val_loss: 1.0394 - val_accuracy: 0.4549 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.4542\n",
      "Epoch 155: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0353 - accuracy: 0.4542 - val_loss: 1.0328 - val_accuracy: 0.4562 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4545\n",
      "Epoch 156: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0348 - accuracy: 0.4545 - val_loss: 1.0346 - val_accuracy: 0.4552 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4545\n",
      "Epoch 157: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4545 - val_loss: 1.0329 - val_accuracy: 0.4561 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4557\n",
      "Epoch 158: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0348 - accuracy: 0.4556 - val_loss: 1.0352 - val_accuracy: 0.4543 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.4561\n",
      "Epoch 159: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4561 - val_loss: 1.0358 - val_accuracy: 0.4511 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4549\n",
      "Epoch 160: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0353 - accuracy: 0.4548 - val_loss: 1.0328 - val_accuracy: 0.4561 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0350 - accuracy: 0.4539\n",
      "Epoch 161: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0350 - accuracy: 0.4539 - val_loss: 1.0322 - val_accuracy: 0.4574 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4550\n",
      "Epoch 162: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0347 - accuracy: 0.4551 - val_loss: 1.0342 - val_accuracy: 0.4597 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4557\n",
      "Epoch 163: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0353 - accuracy: 0.4557 - val_loss: 1.0353 - val_accuracy: 0.4551 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4557\n",
      "Epoch 164: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0349 - accuracy: 0.4556 - val_loss: 1.0395 - val_accuracy: 0.4508 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4546\n",
      "Epoch 165: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0350 - accuracy: 0.4545 - val_loss: 1.0319 - val_accuracy: 0.4577 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4549\n",
      "Epoch 166: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0348 - accuracy: 0.4548 - val_loss: 1.0374 - val_accuracy: 0.4556 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4544\n",
      "Epoch 167: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0352 - accuracy: 0.4544 - val_loss: 1.0335 - val_accuracy: 0.4559 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0348 - accuracy: 0.4548\n",
      "Epoch 168: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0348 - accuracy: 0.4548 - val_loss: 1.0371 - val_accuracy: 0.4517 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4553\n",
      "Epoch 169: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4553 - val_loss: 1.0320 - val_accuracy: 0.4588 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4539\n",
      "Epoch 170: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0352 - accuracy: 0.4540 - val_loss: 1.0321 - val_accuracy: 0.4630 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4565\n",
      "Epoch 171: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 18s 16ms/step - loss: 1.0343 - accuracy: 0.4564 - val_loss: 1.0348 - val_accuracy: 0.4539 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4578\n",
      "Epoch 172: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0338 - accuracy: 0.4578 - val_loss: 1.0322 - val_accuracy: 0.4558 - lr: 8.0000e-04\n",
      "Epoch 173/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0337 - accuracy: 0.4568\n",
      "Epoch 173: val_loss did not improve from 1.03084\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0337 - accuracy: 0.4568 - val_loss: 1.0322 - val_accuracy: 0.4544 - lr: 8.0000e-04\n",
      "Epoch 174/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4562\n",
      "Epoch 174: val_loss improved from 1.03084 to 1.03033, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0335 - accuracy: 0.4563 - val_loss: 1.0303 - val_accuracy: 0.4618 - lr: 8.0000e-04\n",
      "Epoch 175/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0337 - accuracy: 0.4556\n",
      "Epoch 175: val_loss did not improve from 1.03033\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0337 - accuracy: 0.4555 - val_loss: 1.0311 - val_accuracy: 0.4580 - lr: 8.0000e-04\n",
      "Epoch 176/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4553\n",
      "Epoch 176: val_loss did not improve from 1.03033\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0343 - accuracy: 0.4553 - val_loss: 1.0330 - val_accuracy: 0.4612 - lr: 8.0000e-04\n",
      "Epoch 177/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4554\n",
      "Epoch 177: val_loss improved from 1.03033 to 1.03019, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0338 - accuracy: 0.4554 - val_loss: 1.0302 - val_accuracy: 0.4643 - lr: 8.0000e-04\n",
      "Epoch 178/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4563\n",
      "Epoch 178: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0338 - accuracy: 0.4563 - val_loss: 1.0339 - val_accuracy: 0.4557 - lr: 8.0000e-04\n",
      "Epoch 179/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4547\n",
      "Epoch 179: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4547 - val_loss: 1.0323 - val_accuracy: 0.4613 - lr: 8.0000e-04\n",
      "Epoch 180/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4557\n",
      "Epoch 180: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0342 - accuracy: 0.4558 - val_loss: 1.0340 - val_accuracy: 0.4581 - lr: 8.0000e-04\n",
      "Epoch 181/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4559\n",
      "Epoch 181: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0348 - accuracy: 0.4559 - val_loss: 1.0336 - val_accuracy: 0.4569 - lr: 8.0000e-04\n",
      "Epoch 182/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4554\n",
      "Epoch 182: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0349 - accuracy: 0.4554 - val_loss: 1.0407 - val_accuracy: 0.4477 - lr: 8.0000e-04\n",
      "Epoch 183/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4559\n",
      "Epoch 183: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4559 - val_loss: 1.0366 - val_accuracy: 0.4524 - lr: 8.0000e-04\n",
      "Epoch 184/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4558\n",
      "Epoch 184: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0342 - accuracy: 0.4558 - val_loss: 1.0386 - val_accuracy: 0.4523 - lr: 8.0000e-04\n",
      "Epoch 185/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4549\n",
      "Epoch 185: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0344 - accuracy: 0.4549 - val_loss: 1.0359 - val_accuracy: 0.4553 - lr: 8.0000e-04\n",
      "Epoch 186/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4548\n",
      "Epoch 186: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4549 - val_loss: 1.0341 - val_accuracy: 0.4563 - lr: 8.0000e-04\n",
      "Epoch 187/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4553\n",
      "Epoch 187: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4553 - val_loss: 1.0325 - val_accuracy: 0.4596 - lr: 8.0000e-04\n",
      "Epoch 188/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4562\n",
      "Epoch 188: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0340 - accuracy: 0.4562 - val_loss: 1.0327 - val_accuracy: 0.4579 - lr: 8.0000e-04\n",
      "Epoch 189/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4551\n",
      "Epoch 189: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4550 - val_loss: 1.0454 - val_accuracy: 0.4420 - lr: 8.0000e-04\n",
      "Epoch 190/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4553\n",
      "Epoch 190: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0344 - accuracy: 0.4553 - val_loss: 1.0480 - val_accuracy: 0.4374 - lr: 8.0000e-04\n",
      "Epoch 191/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4560\n",
      "Epoch 191: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4560 - val_loss: 1.0364 - val_accuracy: 0.4576 - lr: 8.0000e-04\n",
      "Epoch 192/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0352 - accuracy: 0.4551\n",
      "Epoch 192: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0352 - accuracy: 0.4551 - val_loss: 1.0347 - val_accuracy: 0.4563 - lr: 8.0000e-04\n",
      "Epoch 193/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0353 - accuracy: 0.4552\n",
      "Epoch 193: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0353 - accuracy: 0.4552 - val_loss: 1.0318 - val_accuracy: 0.4583 - lr: 8.0000e-04\n",
      "Epoch 194/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4548\n",
      "Epoch 194: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4548 - val_loss: 1.0351 - val_accuracy: 0.4572 - lr: 8.0000e-04\n",
      "Epoch 195/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4560\n",
      "Epoch 195: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4560 - val_loss: 1.0350 - val_accuracy: 0.4575 - lr: 8.0000e-04\n",
      "Epoch 196/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4545\n",
      "Epoch 196: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4545 - val_loss: 1.0338 - val_accuracy: 0.4552 - lr: 8.0000e-04\n",
      "Epoch 197/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4550\n",
      "Epoch 197: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0349 - accuracy: 0.4549 - val_loss: 1.0316 - val_accuracy: 0.4575 - lr: 8.0000e-04\n",
      "Epoch 198/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4561\n",
      "Epoch 198: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4562 - val_loss: 1.0329 - val_accuracy: 0.4583 - lr: 8.0000e-04\n",
      "Epoch 199/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4568\n",
      "Epoch 199: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4568 - val_loss: 1.0335 - val_accuracy: 0.4559 - lr: 8.0000e-04\n",
      "Epoch 200/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4552\n",
      "Epoch 200: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4552 - val_loss: 1.0344 - val_accuracy: 0.4566 - lr: 8.0000e-04\n",
      "Epoch 201/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4548\n",
      "Epoch 201: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0344 - accuracy: 0.4548 - val_loss: 1.0309 - val_accuracy: 0.4580 - lr: 8.0000e-04\n",
      "Epoch 202/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4545\n",
      "Epoch 202: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0342 - accuracy: 0.4544 - val_loss: 1.0324 - val_accuracy: 0.4604 - lr: 8.0000e-04\n",
      "Epoch 203/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4542\n",
      "Epoch 203: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0352 - accuracy: 0.4542 - val_loss: 1.0360 - val_accuracy: 0.4501 - lr: 8.0000e-04\n",
      "Epoch 204/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4552\n",
      "Epoch 204: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4552 - val_loss: 1.0302 - val_accuracy: 0.4591 - lr: 8.0000e-04\n",
      "Epoch 205/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4540\n",
      "Epoch 205: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0352 - accuracy: 0.4541 - val_loss: 1.0308 - val_accuracy: 0.4612 - lr: 8.0000e-04\n",
      "Epoch 206/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4545\n",
      "Epoch 206: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4545 - val_loss: 1.0330 - val_accuracy: 0.4590 - lr: 8.0000e-04\n",
      "Epoch 207/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4554\n",
      "Epoch 207: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4555 - val_loss: 1.0326 - val_accuracy: 0.4591 - lr: 8.0000e-04\n",
      "Epoch 208/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4560\n",
      "Epoch 208: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4560 - val_loss: 1.0323 - val_accuracy: 0.4582 - lr: 8.0000e-04\n",
      "Epoch 209/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0347 - accuracy: 0.4554\n",
      "Epoch 209: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4554 - val_loss: 1.0361 - val_accuracy: 0.4523 - lr: 8.0000e-04\n",
      "Epoch 210/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4570\n",
      "Epoch 210: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0339 - accuracy: 0.4569 - val_loss: 1.0420 - val_accuracy: 0.4421 - lr: 8.0000e-04\n",
      "Epoch 211/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4555\n",
      "Epoch 211: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4555 - val_loss: 1.0387 - val_accuracy: 0.4538 - lr: 8.0000e-04\n",
      "Epoch 212/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4568\n",
      "Epoch 212: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0347 - accuracy: 0.4569 - val_loss: 1.0382 - val_accuracy: 0.4511 - lr: 8.0000e-04\n",
      "Epoch 213/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4545\n",
      "Epoch 213: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 18ms/step - loss: 1.0352 - accuracy: 0.4546 - val_loss: 1.0376 - val_accuracy: 0.4570 - lr: 8.0000e-04\n",
      "Epoch 214/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4543\n",
      "Epoch 214: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4543 - val_loss: 1.0322 - val_accuracy: 0.4576 - lr: 8.0000e-04\n",
      "Epoch 215/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4550\n",
      "Epoch 215: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4549 - val_loss: 1.0338 - val_accuracy: 0.4519 - lr: 8.0000e-04\n",
      "Epoch 216/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4544\n",
      "Epoch 216: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4544 - val_loss: 1.0351 - val_accuracy: 0.4537 - lr: 8.0000e-04\n",
      "Epoch 217/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4564\n",
      "Epoch 217: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0349 - accuracy: 0.4565 - val_loss: 1.0354 - val_accuracy: 0.4574 - lr: 8.0000e-04\n",
      "Epoch 218/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4551\n",
      "Epoch 218: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0346 - accuracy: 0.4551 - val_loss: 1.0327 - val_accuracy: 0.4566 - lr: 8.0000e-04\n",
      "Epoch 219/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4552\n",
      "Epoch 219: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4552 - val_loss: 1.0356 - val_accuracy: 0.4546 - lr: 8.0000e-04\n",
      "Epoch 220/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4551\n",
      "Epoch 220: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0346 - accuracy: 0.4552 - val_loss: 1.0326 - val_accuracy: 0.4595 - lr: 8.0000e-04\n",
      "Epoch 221/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4566\n",
      "Epoch 221: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4566 - val_loss: 1.0385 - val_accuracy: 0.4557 - lr: 8.0000e-04\n",
      "Epoch 222/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4556\n",
      "Epoch 222: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0347 - accuracy: 0.4556 - val_loss: 1.0340 - val_accuracy: 0.4568 - lr: 8.0000e-04\n",
      "Epoch 223/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4556\n",
      "Epoch 223: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0343 - accuracy: 0.4556 - val_loss: 1.0328 - val_accuracy: 0.4564 - lr: 8.0000e-04\n",
      "Epoch 224/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4566\n",
      "Epoch 224: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0342 - accuracy: 0.4566 - val_loss: 1.0320 - val_accuracy: 0.4569 - lr: 8.0000e-04\n",
      "Epoch 225/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4556\n",
      "Epoch 225: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4555 - val_loss: 1.0345 - val_accuracy: 0.4571 - lr: 8.0000e-04\n",
      "Epoch 226/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4546\n",
      "Epoch 226: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4546 - val_loss: 1.0317 - val_accuracy: 0.4612 - lr: 8.0000e-04\n",
      "Epoch 227/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4565\n",
      "Epoch 227: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0347 - accuracy: 0.4565 - val_loss: 1.0362 - val_accuracy: 0.4551 - lr: 8.0000e-04\n",
      "Epoch 228/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4563\n",
      "Epoch 228: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0338 - accuracy: 0.4563 - val_loss: 1.0335 - val_accuracy: 0.4629 - lr: 6.4000e-04\n",
      "Epoch 229/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4563\n",
      "Epoch 229: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0336 - accuracy: 0.4563 - val_loss: 1.0365 - val_accuracy: 0.4556 - lr: 6.4000e-04\n",
      "Epoch 230/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0334 - accuracy: 0.4575\n",
      "Epoch 230: val_loss did not improve from 1.03019\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0334 - accuracy: 0.4575 - val_loss: 1.0345 - val_accuracy: 0.4544 - lr: 6.4000e-04\n",
      "Epoch 231/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4552\n",
      "Epoch 231: val_loss improved from 1.03019 to 1.02978, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0347 - accuracy: 0.4553 - val_loss: 1.0298 - val_accuracy: 0.4617 - lr: 6.4000e-04\n",
      "Epoch 232/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4568\n",
      "Epoch 232: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0338 - accuracy: 0.4567 - val_loss: 1.0329 - val_accuracy: 0.4584 - lr: 6.4000e-04\n",
      "Epoch 233/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4557\n",
      "Epoch 233: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0336 - accuracy: 0.4557 - val_loss: 1.0301 - val_accuracy: 0.4599 - lr: 6.4000e-04\n",
      "Epoch 234/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4558\n",
      "Epoch 234: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0342 - accuracy: 0.4558 - val_loss: 1.0347 - val_accuracy: 0.4592 - lr: 6.4000e-04\n",
      "Epoch 235/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4568\n",
      "Epoch 235: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4568 - val_loss: 1.0298 - val_accuracy: 0.4609 - lr: 6.4000e-04\n",
      "Epoch 236/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0340 - accuracy: 0.4565\n",
      "Epoch 236: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0340 - accuracy: 0.4565 - val_loss: 1.0323 - val_accuracy: 0.4574 - lr: 6.4000e-04\n",
      "Epoch 237/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4562\n",
      "Epoch 237: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0346 - accuracy: 0.4562 - val_loss: 1.0362 - val_accuracy: 0.4550 - lr: 6.4000e-04\n",
      "Epoch 238/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.4549\n",
      "Epoch 238: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4549 - val_loss: 1.0332 - val_accuracy: 0.4579 - lr: 6.4000e-04\n",
      "Epoch 239/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0351 - accuracy: 0.4548\n",
      "Epoch 239: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0351 - accuracy: 0.4548 - val_loss: 1.0334 - val_accuracy: 0.4574 - lr: 6.4000e-04\n",
      "Epoch 240/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4558\n",
      "Epoch 240: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 18s 16ms/step - loss: 1.0345 - accuracy: 0.4557 - val_loss: 1.0336 - val_accuracy: 0.4567 - lr: 6.4000e-04\n",
      "Epoch 241/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4551\n",
      "Epoch 241: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0350 - accuracy: 0.4551 - val_loss: 1.0379 - val_accuracy: 0.4508 - lr: 6.4000e-04\n",
      "Epoch 242/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4556\n",
      "Epoch 242: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0347 - accuracy: 0.4555 - val_loss: 1.0302 - val_accuracy: 0.4572 - lr: 6.4000e-04\n",
      "Epoch 243/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4563\n",
      "Epoch 243: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0341 - accuracy: 0.4563 - val_loss: 1.0304 - val_accuracy: 0.4596 - lr: 6.4000e-04\n",
      "Epoch 244/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4561\n",
      "Epoch 244: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0345 - accuracy: 0.4562 - val_loss: 1.0315 - val_accuracy: 0.4607 - lr: 6.4000e-04\n",
      "Epoch 245/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4552\n",
      "Epoch 245: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4552 - val_loss: 1.0352 - val_accuracy: 0.4596 - lr: 6.4000e-04\n",
      "Epoch 246/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4560\n",
      "Epoch 246: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0345 - accuracy: 0.4560 - val_loss: 1.0323 - val_accuracy: 0.4576 - lr: 6.4000e-04\n",
      "Epoch 247/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4548\n",
      "Epoch 247: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0343 - accuracy: 0.4548 - val_loss: 1.0359 - val_accuracy: 0.4565 - lr: 6.4000e-04\n",
      "Epoch 248/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4569\n",
      "Epoch 248: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0346 - accuracy: 0.4568 - val_loss: 1.0334 - val_accuracy: 0.4568 - lr: 6.4000e-04\n",
      "Epoch 249/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4559\n",
      "Epoch 249: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0344 - accuracy: 0.4559 - val_loss: 1.0388 - val_accuracy: 0.4523 - lr: 6.4000e-04\n",
      "Epoch 250/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4558\n",
      "Epoch 250: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0345 - accuracy: 0.4558 - val_loss: 1.0318 - val_accuracy: 0.4592 - lr: 6.4000e-04\n",
      "Epoch 251/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4562\n",
      "Epoch 251: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0348 - accuracy: 0.4562 - val_loss: 1.0355 - val_accuracy: 0.4578 - lr: 6.4000e-04\n",
      "Epoch 252/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4558\n",
      "Epoch 252: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0347 - accuracy: 0.4558 - val_loss: 1.0311 - val_accuracy: 0.4610 - lr: 6.4000e-04\n",
      "Epoch 253/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4559\n",
      "Epoch 253: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0347 - accuracy: 0.4559 - val_loss: 1.0361 - val_accuracy: 0.4557 - lr: 6.4000e-04\n",
      "Epoch 254/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0352 - accuracy: 0.4558\n",
      "Epoch 254: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0352 - accuracy: 0.4558 - val_loss: 1.0349 - val_accuracy: 0.4565 - lr: 6.4000e-04\n",
      "Epoch 255/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4548\n",
      "Epoch 255: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4548 - val_loss: 1.0304 - val_accuracy: 0.4629 - lr: 6.4000e-04\n",
      "Epoch 256/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4552\n",
      "Epoch 256: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0347 - accuracy: 0.4551 - val_loss: 1.0376 - val_accuracy: 0.4531 - lr: 6.4000e-04\n",
      "Epoch 257/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4570\n",
      "Epoch 257: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0345 - accuracy: 0.4570 - val_loss: 1.0362 - val_accuracy: 0.4599 - lr: 6.4000e-04\n",
      "Epoch 258/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.4562\n",
      "Epoch 258: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0344 - accuracy: 0.4562 - val_loss: 1.0343 - val_accuracy: 0.4578 - lr: 6.4000e-04\n",
      "Epoch 259/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.4566\n",
      "Epoch 259: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4566 - val_loss: 1.0336 - val_accuracy: 0.4564 - lr: 6.4000e-04\n",
      "Epoch 260/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4566\n",
      "Epoch 260: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4566 - val_loss: 1.0389 - val_accuracy: 0.4547 - lr: 6.4000e-04\n",
      "Epoch 261/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4568\n",
      "Epoch 261: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4568 - val_loss: 1.0343 - val_accuracy: 0.4606 - lr: 6.4000e-04\n",
      "Epoch 262/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4560\n",
      "Epoch 262: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4561 - val_loss: 1.0323 - val_accuracy: 0.4550 - lr: 6.4000e-04\n",
      "Epoch 263/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4565\n",
      "Epoch 263: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4565 - val_loss: 1.0378 - val_accuracy: 0.4502 - lr: 6.4000e-04\n",
      "Epoch 264/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4574\n",
      "Epoch 264: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0339 - accuracy: 0.4574 - val_loss: 1.0317 - val_accuracy: 0.4594 - lr: 6.4000e-04\n",
      "Epoch 265/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4565\n",
      "Epoch 265: val_loss did not improve from 1.02978\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4565 - val_loss: 1.0385 - val_accuracy: 0.4472 - lr: 6.4000e-04\n",
      "Epoch 266/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0343 - accuracy: 0.4552\n",
      "Epoch 266: val_loss improved from 1.02978 to 1.02900, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4552 - val_loss: 1.0290 - val_accuracy: 0.4605 - lr: 6.4000e-04\n",
      "Epoch 267/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0341 - accuracy: 0.4567\n",
      "Epoch 267: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4567 - val_loss: 1.0353 - val_accuracy: 0.4537 - lr: 6.4000e-04\n",
      "Epoch 268/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4563\n",
      "Epoch 268: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4564 - val_loss: 1.0343 - val_accuracy: 0.4585 - lr: 6.4000e-04\n",
      "Epoch 269/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4558\n",
      "Epoch 269: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4558 - val_loss: 1.0355 - val_accuracy: 0.4539 - lr: 6.4000e-04\n",
      "Epoch 270/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0346 - accuracy: 0.4559\n",
      "Epoch 270: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4559 - val_loss: 1.0300 - val_accuracy: 0.4595 - lr: 6.4000e-04\n",
      "Epoch 271/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4563\n",
      "Epoch 271: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4564 - val_loss: 1.0441 - val_accuracy: 0.4379 - lr: 6.4000e-04\n",
      "Epoch 272/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4558\n",
      "Epoch 272: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 19s 17ms/step - loss: 1.0347 - accuracy: 0.4558 - val_loss: 1.0318 - val_accuracy: 0.4583 - lr: 6.4000e-04\n",
      "Epoch 273/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4560\n",
      "Epoch 273: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4560 - val_loss: 1.0328 - val_accuracy: 0.4570 - lr: 6.4000e-04\n",
      "Epoch 274/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4556\n",
      "Epoch 274: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4555 - val_loss: 1.0319 - val_accuracy: 0.4579 - lr: 6.4000e-04\n",
      "Epoch 275/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0343 - accuracy: 0.4554\n",
      "Epoch 275: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4554 - val_loss: 1.0324 - val_accuracy: 0.4610 - lr: 6.4000e-04\n",
      "Epoch 276/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4573\n",
      "Epoch 276: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4573 - val_loss: 1.0321 - val_accuracy: 0.4596 - lr: 6.4000e-04\n",
      "Epoch 277/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4553\n",
      "Epoch 277: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4553 - val_loss: 1.0357 - val_accuracy: 0.4528 - lr: 6.4000e-04\n",
      "Epoch 278/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0346 - accuracy: 0.4553\n",
      "Epoch 278: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4553 - val_loss: 1.0306 - val_accuracy: 0.4623 - lr: 6.4000e-04\n",
      "Epoch 279/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4564\n",
      "Epoch 279: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4564 - val_loss: 1.0298 - val_accuracy: 0.4631 - lr: 6.4000e-04\n",
      "Epoch 280/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4563\n",
      "Epoch 280: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4563 - val_loss: 1.0315 - val_accuracy: 0.4606 - lr: 6.4000e-04\n",
      "Epoch 281/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4546\n",
      "Epoch 281: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4546 - val_loss: 1.0432 - val_accuracy: 0.4562 - lr: 6.4000e-04\n",
      "Epoch 282/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4570\n",
      "Epoch 282: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0339 - accuracy: 0.4569 - val_loss: 1.0348 - val_accuracy: 0.4516 - lr: 6.4000e-04\n",
      "Epoch 283/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4544\n",
      "Epoch 283: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4544 - val_loss: 1.0363 - val_accuracy: 0.4526 - lr: 6.4000e-04\n",
      "Epoch 284/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4564\n",
      "Epoch 284: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4564 - val_loss: 1.0344 - val_accuracy: 0.4579 - lr: 6.4000e-04\n",
      "Epoch 285/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4561\n",
      "Epoch 285: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4560 - val_loss: 1.0357 - val_accuracy: 0.4540 - lr: 6.4000e-04\n",
      "Epoch 286/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0343 - accuracy: 0.4560\n",
      "Epoch 286: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4560 - val_loss: 1.0346 - val_accuracy: 0.4532 - lr: 6.4000e-04\n",
      "Epoch 287/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4566\n",
      "Epoch 287: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4566 - val_loss: 1.0323 - val_accuracy: 0.4604 - lr: 6.4000e-04\n",
      "Epoch 288/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4564\n",
      "Epoch 288: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4564 - val_loss: 1.0329 - val_accuracy: 0.4558 - lr: 6.4000e-04\n",
      "Epoch 289/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4567\n",
      "Epoch 289: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4566 - val_loss: 1.0313 - val_accuracy: 0.4622 - lr: 6.4000e-04\n",
      "Epoch 290/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0341 - accuracy: 0.4570\n",
      "Epoch 290: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4570 - val_loss: 1.0334 - val_accuracy: 0.4516 - lr: 6.4000e-04\n",
      "Epoch 291/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4550\n",
      "Epoch 291: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4549 - val_loss: 1.0290 - val_accuracy: 0.4623 - lr: 6.4000e-04\n",
      "Epoch 292/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0341 - accuracy: 0.4564\n",
      "Epoch 292: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4564 - val_loss: 1.0344 - val_accuracy: 0.4558 - lr: 6.4000e-04\n",
      "Epoch 293/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4563\n",
      "Epoch 293: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4563 - val_loss: 1.0347 - val_accuracy: 0.4589 - lr: 6.4000e-04\n",
      "Epoch 294/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0347 - accuracy: 0.4552\n",
      "Epoch 294: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4552 - val_loss: 1.0316 - val_accuracy: 0.4603 - lr: 6.4000e-04\n",
      "Epoch 295/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4554\n",
      "Epoch 295: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4552 - val_loss: 1.0371 - val_accuracy: 0.4535 - lr: 6.4000e-04\n",
      "Epoch 296/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4565\n",
      "Epoch 296: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4564 - val_loss: 1.0350 - val_accuracy: 0.4537 - lr: 6.4000e-04\n",
      "Epoch 297/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0346 - accuracy: 0.4550\n",
      "Epoch 297: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4550 - val_loss: 1.0365 - val_accuracy: 0.4553 - lr: 6.4000e-04\n",
      "Epoch 298/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4553\n",
      "Epoch 298: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4553 - val_loss: 1.0320 - val_accuracy: 0.4618 - lr: 6.4000e-04\n",
      "Epoch 299/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4556\n",
      "Epoch 299: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4556 - val_loss: 1.0411 - val_accuracy: 0.4537 - lr: 6.4000e-04\n",
      "Epoch 300/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4561\n",
      "Epoch 300: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4561 - val_loss: 1.0331 - val_accuracy: 0.4600 - lr: 6.4000e-04\n",
      "Epoch 301/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4562\n",
      "Epoch 301: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4562 - val_loss: 1.0304 - val_accuracy: 0.4622 - lr: 6.4000e-04\n",
      "Epoch 302/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4560\n",
      "Epoch 302: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4561 - val_loss: 1.0318 - val_accuracy: 0.4568 - lr: 6.4000e-04\n",
      "Epoch 303/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4558\n",
      "Epoch 303: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4558 - val_loss: 1.0305 - val_accuracy: 0.4601 - lr: 6.4000e-04\n",
      "Epoch 304/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4555\n",
      "Epoch 304: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4555 - val_loss: 1.0329 - val_accuracy: 0.4566 - lr: 6.4000e-04\n",
      "Epoch 305/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4549\n",
      "Epoch 305: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4549 - val_loss: 1.0303 - val_accuracy: 0.4590 - lr: 6.4000e-04\n",
      "Epoch 306/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4566\n",
      "Epoch 306: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4567 - val_loss: 1.0311 - val_accuracy: 0.4592 - lr: 6.4000e-04\n",
      "Epoch 307/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4557\n",
      "Epoch 307: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4558 - val_loss: 1.0374 - val_accuracy: 0.4608 - lr: 6.4000e-04\n",
      "Epoch 308/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0342 - accuracy: 0.4549\n",
      "Epoch 308: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4549 - val_loss: 1.0404 - val_accuracy: 0.4471 - lr: 6.4000e-04\n",
      "Epoch 309/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4550\n",
      "Epoch 309: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4549 - val_loss: 1.0326 - val_accuracy: 0.4584 - lr: 6.4000e-04\n",
      "Epoch 310/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4553\n",
      "Epoch 310: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4553 - val_loss: 1.0345 - val_accuracy: 0.4588 - lr: 6.4000e-04\n",
      "Epoch 311/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4558\n",
      "Epoch 311: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4558 - val_loss: 1.0383 - val_accuracy: 0.4555 - lr: 6.4000e-04\n",
      "Epoch 312/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4553\n",
      "Epoch 312: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4554 - val_loss: 1.0341 - val_accuracy: 0.4594 - lr: 6.4000e-04\n",
      "Epoch 313/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4564\n",
      "Epoch 313: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4564 - val_loss: 1.0332 - val_accuracy: 0.4542 - lr: 6.4000e-04\n",
      "Epoch 314/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4563\n",
      "Epoch 314: val_loss did not improve from 1.02900\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4563 - val_loss: 1.0294 - val_accuracy: 0.4592 - lr: 6.4000e-04\n",
      "Epoch 315/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0337 - accuracy: 0.4560\n",
      "Epoch 315: val_loss improved from 1.02900 to 1.02844, saving model to best_model_5.h5\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0337 - accuracy: 0.4560 - val_loss: 1.0284 - val_accuracy: 0.4633 - lr: 6.4000e-04\n",
      "Epoch 316/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4555\n",
      "Epoch 316: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4555 - val_loss: 1.0346 - val_accuracy: 0.4599 - lr: 6.4000e-04\n",
      "Epoch 317/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4565\n",
      "Epoch 317: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4564 - val_loss: 1.0338 - val_accuracy: 0.4592 - lr: 6.4000e-04\n",
      "Epoch 318/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4559\n",
      "Epoch 318: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4560 - val_loss: 1.0344 - val_accuracy: 0.4519 - lr: 6.4000e-04\n",
      "Epoch 319/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4559\n",
      "Epoch 319: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4559 - val_loss: 1.0343 - val_accuracy: 0.4562 - lr: 6.4000e-04\n",
      "Epoch 320/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4548\n",
      "Epoch 320: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0349 - accuracy: 0.4548 - val_loss: 1.0323 - val_accuracy: 0.4601 - lr: 6.4000e-04\n",
      "Epoch 321/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4564\n",
      "Epoch 321: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4564 - val_loss: 1.0327 - val_accuracy: 0.4595 - lr: 6.4000e-04\n",
      "Epoch 322/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4559\n",
      "Epoch 322: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4559 - val_loss: 1.0318 - val_accuracy: 0.4545 - lr: 6.4000e-04\n",
      "Epoch 323/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4562\n",
      "Epoch 323: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4562 - val_loss: 1.0334 - val_accuracy: 0.4535 - lr: 6.4000e-04\n",
      "Epoch 324/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4575\n",
      "Epoch 324: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4575 - val_loss: 1.0409 - val_accuracy: 0.4475 - lr: 6.4000e-04\n",
      "Epoch 325/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4563\n",
      "Epoch 325: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4563 - val_loss: 1.0360 - val_accuracy: 0.4507 - lr: 6.4000e-04\n",
      "Epoch 326/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4552\n",
      "Epoch 326: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0353 - accuracy: 0.4552 - val_loss: 1.0320 - val_accuracy: 0.4592 - lr: 6.4000e-04\n",
      "Epoch 327/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4551\n",
      "Epoch 327: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4552 - val_loss: 1.0366 - val_accuracy: 0.4565 - lr: 6.4000e-04\n",
      "Epoch 328/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0346 - accuracy: 0.4562\n",
      "Epoch 328: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4562 - val_loss: 1.0346 - val_accuracy: 0.4534 - lr: 6.4000e-04\n",
      "Epoch 329/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4545\n",
      "Epoch 329: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4545 - val_loss: 1.0335 - val_accuracy: 0.4599 - lr: 6.4000e-04\n",
      "Epoch 330/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4571\n",
      "Epoch 330: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4571 - val_loss: 1.0388 - val_accuracy: 0.4559 - lr: 6.4000e-04\n",
      "Epoch 331/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4554\n",
      "Epoch 331: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4554 - val_loss: 1.0369 - val_accuracy: 0.4568 - lr: 6.4000e-04\n",
      "Epoch 332/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4557\n",
      "Epoch 332: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4557 - val_loss: 1.0440 - val_accuracy: 0.4454 - lr: 6.4000e-04\n",
      "Epoch 333/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4554\n",
      "Epoch 333: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0349 - accuracy: 0.4554 - val_loss: 1.0342 - val_accuracy: 0.4586 - lr: 6.4000e-04\n",
      "Epoch 334/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0343 - accuracy: 0.4564\n",
      "Epoch 334: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0343 - accuracy: 0.4564 - val_loss: 1.0408 - val_accuracy: 0.4452 - lr: 6.4000e-04\n",
      "Epoch 335/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4559\n",
      "Epoch 335: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4559 - val_loss: 1.0321 - val_accuracy: 0.4589 - lr: 6.4000e-04\n",
      "Epoch 336/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0348 - accuracy: 0.4561\n",
      "Epoch 336: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4561 - val_loss: 1.0334 - val_accuracy: 0.4555 - lr: 6.4000e-04\n",
      "Epoch 337/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0348 - accuracy: 0.4562\n",
      "Epoch 337: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4562 - val_loss: 1.0350 - val_accuracy: 0.4573 - lr: 6.4000e-04\n",
      "Epoch 338/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0347 - accuracy: 0.4544\n",
      "Epoch 338: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4544 - val_loss: 1.0440 - val_accuracy: 0.4506 - lr: 6.4000e-04\n",
      "Epoch 339/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4552\n",
      "Epoch 339: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4552 - val_loss: 1.0331 - val_accuracy: 0.4563 - lr: 6.4000e-04\n",
      "Epoch 340/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4550\n",
      "Epoch 340: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4551 - val_loss: 1.0365 - val_accuracy: 0.4576 - lr: 6.4000e-04\n",
      "Epoch 341/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4563\n",
      "Epoch 341: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4562 - val_loss: 1.0331 - val_accuracy: 0.4570 - lr: 6.4000e-04\n",
      "Epoch 342/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4563\n",
      "Epoch 342: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4563 - val_loss: 1.0332 - val_accuracy: 0.4575 - lr: 6.4000e-04\n",
      "Epoch 343/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4558\n",
      "Epoch 343: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4558 - val_loss: 1.0314 - val_accuracy: 0.4595 - lr: 6.4000e-04\n",
      "Epoch 344/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4570\n",
      "Epoch 344: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4570 - val_loss: 1.0407 - val_accuracy: 0.4534 - lr: 6.4000e-04\n",
      "Epoch 345/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4567\n",
      "Epoch 345: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4567 - val_loss: 1.0334 - val_accuracy: 0.4537 - lr: 6.4000e-04\n",
      "Epoch 346/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4567\n",
      "Epoch 346: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4567 - val_loss: 1.0359 - val_accuracy: 0.4584 - lr: 6.4000e-04\n",
      "Epoch 347/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4559\n",
      "Epoch 347: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4559 - val_loss: 1.0318 - val_accuracy: 0.4602 - lr: 6.4000e-04\n",
      "Epoch 348/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0346 - accuracy: 0.4555\n",
      "Epoch 348: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4555 - val_loss: 1.0326 - val_accuracy: 0.4596 - lr: 6.4000e-04\n",
      "Epoch 349/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4564\n",
      "Epoch 349: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0338 - accuracy: 0.4564 - val_loss: 1.0360 - val_accuracy: 0.4597 - lr: 6.4000e-04\n",
      "Epoch 350/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4570\n",
      "Epoch 350: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4570 - val_loss: 1.0358 - val_accuracy: 0.4580 - lr: 6.4000e-04\n",
      "Epoch 351/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4561\n",
      "Epoch 351: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4562 - val_loss: 1.0327 - val_accuracy: 0.4605 - lr: 6.4000e-04\n",
      "Epoch 352/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0342 - accuracy: 0.4562\n",
      "Epoch 352: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4562 - val_loss: 1.0343 - val_accuracy: 0.4547 - lr: 6.4000e-04\n",
      "Epoch 353/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4564\n",
      "Epoch 353: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4564 - val_loss: 1.0339 - val_accuracy: 0.4583 - lr: 6.4000e-04\n",
      "Epoch 354/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4557\n",
      "Epoch 354: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4558 - val_loss: 1.0330 - val_accuracy: 0.4611 - lr: 6.4000e-04\n",
      "Epoch 355/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0346 - accuracy: 0.4549\n",
      "Epoch 355: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4549 - val_loss: 1.0318 - val_accuracy: 0.4573 - lr: 6.4000e-04\n",
      "Epoch 356/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4549\n",
      "Epoch 356: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0351 - accuracy: 0.4548 - val_loss: 1.0389 - val_accuracy: 0.4459 - lr: 6.4000e-04\n",
      "Epoch 357/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0343 - accuracy: 0.4565\n",
      "Epoch 357: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4565 - val_loss: 1.0332 - val_accuracy: 0.4596 - lr: 6.4000e-04\n",
      "Epoch 358/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4564\n",
      "Epoch 358: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4564 - val_loss: 1.0331 - val_accuracy: 0.4569 - lr: 6.4000e-04\n",
      "Epoch 359/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4553\n",
      "Epoch 359: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0352 - accuracy: 0.4553 - val_loss: 1.0323 - val_accuracy: 0.4563 - lr: 6.4000e-04\n",
      "Epoch 360/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4555\n",
      "Epoch 360: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0341 - accuracy: 0.4555 - val_loss: 1.0383 - val_accuracy: 0.4521 - lr: 6.4000e-04\n",
      "Epoch 361/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4563\n",
      "Epoch 361: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4563 - val_loss: 1.0387 - val_accuracy: 0.4558 - lr: 6.4000e-04\n",
      "Epoch 362/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0342 - accuracy: 0.4564\n",
      "Epoch 362: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4564 - val_loss: 1.0380 - val_accuracy: 0.4498 - lr: 6.4000e-04\n",
      "Epoch 363/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4558\n",
      "Epoch 363: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4558 - val_loss: 1.0322 - val_accuracy: 0.4607 - lr: 6.4000e-04\n",
      "Epoch 364/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4562\n",
      "Epoch 364: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0350 - accuracy: 0.4562 - val_loss: 1.0359 - val_accuracy: 0.4548 - lr: 6.4000e-04\n",
      "Epoch 365/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4569\n",
      "Epoch 365: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4568 - val_loss: 1.0385 - val_accuracy: 0.4498 - lr: 6.4000e-04\n",
      "Epoch 366/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.4571\n",
      "Epoch 366: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0332 - accuracy: 0.4572 - val_loss: 1.0381 - val_accuracy: 0.4529 - lr: 5.1200e-04\n",
      "Epoch 367/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0337 - accuracy: 0.4573\n",
      "Epoch 367: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0337 - accuracy: 0.4572 - val_loss: 1.0348 - val_accuracy: 0.4527 - lr: 5.1200e-04\n",
      "Epoch 368/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.4565\n",
      "Epoch 368: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0332 - accuracy: 0.4564 - val_loss: 1.0341 - val_accuracy: 0.4536 - lr: 5.1200e-04\n",
      "Epoch 369/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4569\n",
      "Epoch 369: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0338 - accuracy: 0.4569 - val_loss: 1.0295 - val_accuracy: 0.4603 - lr: 5.1200e-04\n",
      "Epoch 370/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0334 - accuracy: 0.4572\n",
      "Epoch 370: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0334 - accuracy: 0.4572 - val_loss: 1.0290 - val_accuracy: 0.4615 - lr: 5.1200e-04\n",
      "Epoch 371/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4561\n",
      "Epoch 371: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4561 - val_loss: 1.0321 - val_accuracy: 0.4568 - lr: 5.1200e-04\n",
      "Epoch 372/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0340 - accuracy: 0.4573\n",
      "Epoch 372: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4573 - val_loss: 1.0346 - val_accuracy: 0.4607 - lr: 5.1200e-04\n",
      "Epoch 373/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4555\n",
      "Epoch 373: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0340 - accuracy: 0.4554 - val_loss: 1.0350 - val_accuracy: 0.4561 - lr: 5.1200e-04\n",
      "Epoch 374/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4562\n",
      "Epoch 374: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4562 - val_loss: 1.0346 - val_accuracy: 0.4580 - lr: 5.1200e-04\n",
      "Epoch 375/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4568\n",
      "Epoch 375: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4567 - val_loss: 1.0298 - val_accuracy: 0.4584 - lr: 5.1200e-04\n",
      "Epoch 376/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4562\n",
      "Epoch 376: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4562 - val_loss: 1.0355 - val_accuracy: 0.4576 - lr: 5.1200e-04\n",
      "Epoch 377/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.4554\n",
      "Epoch 377: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4554 - val_loss: 1.0372 - val_accuracy: 0.4503 - lr: 5.1200e-04\n",
      "Epoch 378/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4542\n",
      "Epoch 378: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0347 - accuracy: 0.4542 - val_loss: 1.0329 - val_accuracy: 0.4603 - lr: 5.1200e-04\n",
      "Epoch 379/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4566\n",
      "Epoch 379: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0342 - accuracy: 0.4566 - val_loss: 1.0352 - val_accuracy: 0.4546 - lr: 5.1200e-04\n",
      "Epoch 380/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4566\n",
      "Epoch 380: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4566 - val_loss: 1.0323 - val_accuracy: 0.4589 - lr: 5.1200e-04\n",
      "Epoch 381/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4557\n",
      "Epoch 381: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0343 - accuracy: 0.4557 - val_loss: 1.0333 - val_accuracy: 0.4547 - lr: 5.1200e-04\n",
      "Epoch 382/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4547\n",
      "Epoch 382: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0345 - accuracy: 0.4547 - val_loss: 1.0357 - val_accuracy: 0.4520 - lr: 5.1200e-04\n",
      "Epoch 383/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0349 - accuracy: 0.4548\n",
      "Epoch 383: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0349 - accuracy: 0.4548 - val_loss: 1.0339 - val_accuracy: 0.4555 - lr: 5.1200e-04\n",
      "Epoch 384/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4562\n",
      "Epoch 384: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0349 - accuracy: 0.4562 - val_loss: 1.0394 - val_accuracy: 0.4505 - lr: 5.1200e-04\n",
      "Epoch 385/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.4549\n",
      "Epoch 385: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0344 - accuracy: 0.4549 - val_loss: 1.0306 - val_accuracy: 0.4621 - lr: 5.1200e-04\n",
      "Epoch 386/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4551\n",
      "Epoch 386: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0354 - accuracy: 0.4551 - val_loss: 1.0362 - val_accuracy: 0.4568 - lr: 5.1200e-04\n",
      "Epoch 387/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4549\n",
      "Epoch 387: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0349 - accuracy: 0.4549 - val_loss: 1.0438 - val_accuracy: 0.4463 - lr: 5.1200e-04\n",
      "Epoch 388/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4549\n",
      "Epoch 388: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4550 - val_loss: 1.0341 - val_accuracy: 0.4557 - lr: 5.1200e-04\n",
      "Epoch 389/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4566\n",
      "Epoch 389: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4566 - val_loss: 1.0317 - val_accuracy: 0.4575 - lr: 5.1200e-04\n",
      "Epoch 390/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4555\n",
      "Epoch 390: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0348 - accuracy: 0.4555 - val_loss: 1.0365 - val_accuracy: 0.4551 - lr: 5.1200e-04\n",
      "Epoch 391/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.4566\n",
      "Epoch 391: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0344 - accuracy: 0.4566 - val_loss: 1.0355 - val_accuracy: 0.4593 - lr: 5.1200e-04\n",
      "Epoch 392/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.4549\n",
      "Epoch 392: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0344 - accuracy: 0.4549 - val_loss: 1.0310 - val_accuracy: 0.4571 - lr: 5.1200e-04\n",
      "Epoch 393/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4559\n",
      "Epoch 393: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0350 - accuracy: 0.4559 - val_loss: 1.0352 - val_accuracy: 0.4590 - lr: 5.1200e-04\n",
      "Epoch 394/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4551\n",
      "Epoch 394: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0346 - accuracy: 0.4551 - val_loss: 1.0316 - val_accuracy: 0.4589 - lr: 5.1200e-04\n",
      "Epoch 395/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.4565\n",
      "Epoch 395: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 19s 16ms/step - loss: 1.0345 - accuracy: 0.4565 - val_loss: 1.0334 - val_accuracy: 0.4608 - lr: 5.1200e-04\n",
      "Epoch 396/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4553\n",
      "Epoch 396: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 20s 17ms/step - loss: 1.0348 - accuracy: 0.4554 - val_loss: 1.0347 - val_accuracy: 0.4535 - lr: 5.1200e-04\n",
      "Epoch 397/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4563\n",
      "Epoch 397: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0346 - accuracy: 0.4562 - val_loss: 1.0368 - val_accuracy: 0.4575 - lr: 5.1200e-04\n",
      "Epoch 398/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4562\n",
      "Epoch 398: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0347 - accuracy: 0.4562 - val_loss: 1.0339 - val_accuracy: 0.4590 - lr: 5.1200e-04\n",
      "Epoch 399/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.4568\n",
      "Epoch 399: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0344 - accuracy: 0.4568 - val_loss: 1.0357 - val_accuracy: 0.4520 - lr: 5.1200e-04\n",
      "Epoch 400/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4548\n",
      "Epoch 400: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0342 - accuracy: 0.4548 - val_loss: 1.0350 - val_accuracy: 0.4524 - lr: 5.1200e-04\n",
      "Epoch 401/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4562\n",
      "Epoch 401: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0340 - accuracy: 0.4561 - val_loss: 1.0439 - val_accuracy: 0.4396 - lr: 5.1200e-04\n",
      "Epoch 402/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0349 - accuracy: 0.4557\n",
      "Epoch 402: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0349 - accuracy: 0.4557 - val_loss: 1.0375 - val_accuracy: 0.4559 - lr: 5.1200e-04\n",
      "Epoch 403/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0346 - accuracy: 0.4560\n",
      "Epoch 403: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0346 - accuracy: 0.4560 - val_loss: 1.0332 - val_accuracy: 0.4579 - lr: 5.1200e-04\n",
      "Epoch 404/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4556\n",
      "Epoch 404: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0348 - accuracy: 0.4556 - val_loss: 1.0375 - val_accuracy: 0.4557 - lr: 5.1200e-04\n",
      "Epoch 405/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0349 - accuracy: 0.4564\n",
      "Epoch 405: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0349 - accuracy: 0.4564 - val_loss: 1.0347 - val_accuracy: 0.4554 - lr: 5.1200e-04\n",
      "Epoch 406/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0349 - accuracy: 0.4547\n",
      "Epoch 406: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0349 - accuracy: 0.4547 - val_loss: 1.0313 - val_accuracy: 0.4606 - lr: 5.1200e-04\n",
      "Epoch 407/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4545\n",
      "Epoch 407: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0351 - accuracy: 0.4545 - val_loss: 1.0384 - val_accuracy: 0.4479 - lr: 5.1200e-04\n",
      "Epoch 408/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4543\n",
      "Epoch 408: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0351 - accuracy: 0.4543 - val_loss: 1.0416 - val_accuracy: 0.4454 - lr: 5.1200e-04\n",
      "Epoch 409/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4570\n",
      "Epoch 409: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0345 - accuracy: 0.4570 - val_loss: 1.0317 - val_accuracy: 0.4599 - lr: 5.1200e-04\n",
      "Epoch 410/1000\n",
      "1166/1168 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4556\n",
      "Epoch 410: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0347 - accuracy: 0.4556 - val_loss: 1.0374 - val_accuracy: 0.4547 - lr: 5.1200e-04\n",
      "Epoch 411/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4559\n",
      "Epoch 411: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0349 - accuracy: 0.4559 - val_loss: 1.0334 - val_accuracy: 0.4608 - lr: 5.1200e-04\n",
      "Epoch 412/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4550\n",
      "Epoch 412: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0349 - accuracy: 0.4550 - val_loss: 1.0445 - val_accuracy: 0.4452 - lr: 5.1200e-04\n",
      "Epoch 413/1000\n",
      "1165/1168 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4549\n",
      "Epoch 413: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0351 - accuracy: 0.4548 - val_loss: 1.0332 - val_accuracy: 0.4572 - lr: 5.1200e-04\n",
      "Epoch 414/1000\n",
      "1167/1168 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4553\n",
      "Epoch 414: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0348 - accuracy: 0.4553 - val_loss: 1.0451 - val_accuracy: 0.4520 - lr: 5.1200e-04\n",
      "Epoch 415/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1.0348 - accuracy: 0.4562Restoring model weights from the end of the best epoch: 315.\n",
      "\n",
      "Epoch 415: val_loss did not improve from 1.02844\n",
      "1168/1168 [==============================] - 21s 18ms/step - loss: 1.0348 - accuracy: 0.4562 - val_loss: 1.0354 - val_accuracy: 0.4518 - lr: 5.1200e-04\n",
      "Epoch 415: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 5. 모델 생성 및 학습\n",
    "input_shape = X.shape[1:]  # 10분 동안의 데이터 \n",
    "model = create_transformer_model(input_shape, num_heads=4, ff_dim=128, d_model=128, num_classes=3)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=1000, \n",
    "                    batch_size=128, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    class_weight=class_weights_dict,\n",
    "                    callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 6s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.52      0.50     13173\n",
      "           1       0.43      0.32      0.37     11299\n",
      "           2       0.46      0.53      0.50     12892\n",
      "\n",
      "    accuracy                           0.46     37364\n",
      "   macro avg       0.46      0.46      0.45     37364\n",
      "weighted avg       0.46      0.46      0.46     37364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 성능 평가 지표\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 시각화\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 정확도 시각화\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

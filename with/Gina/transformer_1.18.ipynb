{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 변동률 데이터로 확인\n",
    "#### 얕은 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149328, 60, 11)\n",
      "(149328,)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 전처리\n",
    "def create_classification_targets(data, columns, window_size=60, pred_offset=1):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window_size - pred_offset + 1):\n",
    "        X.append(data.iloc[i:i+window_size][columns])\n",
    "        # 3분 뒤의 Close 가격 변화에 따른 클래스 설정\n",
    "        future_close = data.iloc[i+window_size+pred_offset-1]['Close']\n",
    "        current_close = data.iloc[i+window_size-1]['Close']\n",
    "        \n",
    "        # 가격 변화에 따라 클래스 설정: 상승(2), 보합(1), 하락(0)\n",
    "        if future_close > current_close:\n",
    "            y.append(2)  # 상승\n",
    "        elif future_close < current_close:\n",
    "            y.append(0)  # 하락\n",
    "        else:\n",
    "            y.append(1)  # 보합\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 데이터 불러오기 및 전처리\n",
    "data = pd.read_csv('./000660.csv', parse_dates=['Unnamed: 0'])\n",
    "data = data.rename(columns={'Unnamed: 0': 'Time', \n",
    "                            '매수량': 'BuyVolume', \n",
    "                            '매도량': 'SellVolume', \n",
    "                            '종가': 'Close', \n",
    "                            '저가': 'Low',\n",
    "                            '시가': 'Open',\n",
    "                            '고가': 'High'})\n",
    "data['Power'] = data['BuyVolume'] - data['SellVolume']\n",
    "columns = ['Power', 'Close', 'Low', 'Open', 'High', 'MA5', 'MA20', 'MA60', 'MA120', 'Upper_Band', 'Lower_Band']\n",
    "data = data[columns].dropna()\n",
    "columns_to_change = columns[1:]\n",
    "changed_columns = ['Changed_' + col for col in columns_to_change]\n",
    "\n",
    "# 변화율 데이터 추가\n",
    "data[changed_columns] = data[columns_to_change].pct_change() * 100\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "columns_to_scale = columns[:1] + changed_columns\n",
    "\n",
    "scaled_columns = ['Scaled_' + col for col in columns]\n",
    "\n",
    "# 스케일\n",
    "robust_scaler = RobustScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "data[scaled_columns[:1]] = robust_scaler.fit_transform(data[columns[:1]])\n",
    "data[scaled_columns[1:]] = standard_scaler.fit_transform(data[changed_columns])\n",
    "\n",
    "# 사용할 데이터만 만들기\n",
    "using_columns = ['Close'] + scaled_columns\n",
    "data = data[using_columns]\n",
    "\n",
    "X, y = create_classification_targets(data, using_columns[1:])\n",
    "\n",
    "# 학습/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)  # (samples, 10, 3)\n",
    "print(y_train.shape)  # (samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44190 61593 43545]\n"
     ]
    }
   ],
   "source": [
    "# y 클래스 데이터 분포 확인\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 가중치 계산\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Positional Encoding 구현\n",
    "def positional_encoding(max_len, d_model):\n",
    "    pos = np.arange(max_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "\n",
    "    # 짝수 인덱스에 대해 sin 적용\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 홀수 인덱스에 대해 cos 적용\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    return pos_encoding\n",
    "\n",
    "def add_positional_encoding(inputs, max_len, d_model):\n",
    "    pos_encoding = positional_encoding(max_len, d_model)\n",
    "    pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    pos_encoded_inputs = inputs + pos_encoding[:tf.shape(inputs)[1], :]\n",
    "    return pos_encoded_inputs\n",
    "\n",
    "# 3. Transformer 블록 구현\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"gelu\", kernel_initializer='he_normal'), layers.Dense(d_model)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output1 = self.att1(inputs, inputs)\n",
    "        attn_output1 = self.dropout1(attn_output1)\n",
    "        out1 = self.layernorm1(inputs + attn_output1)\n",
    "        attn_output2 = self.att2(out1, out1)\n",
    "        attn_output2 = self.dropout2(attn_output2)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout1(ffn_output)\n",
    "        return self.layernorm2(out2 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# 4. Transformer 모델 구성\n",
    "def create_transformer_model(input_shape, num_heads, ff_dim, d_model, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # 임베딩 및 위치 인코딩\n",
    "    embedding_layer = layers.Dense(d_model, kernel_initializer='he_normal')(inputs)\n",
    "    pos_encoded_inputs = add_positional_encoding(embedding_layer, max_len=input_shape[0], d_model=d_model)\n",
    "\n",
    "    # Transformer 블록 적용\n",
    "    transformer_block = TransformerBlock(d_model, num_heads, ff_dim)\n",
    "    x = transformer_block(pos_encoded_inputs)\n",
    "    \n",
    "    # 출력 레이어\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation=\"gelu\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model_18.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.4, \n",
    "    patience=5, \n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "callbacks = [model_checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 60, 11)]     0           []                               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 60, 128)      1536        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_2 (TFOpLamb  (3,)                0           ['dense_10[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  ()                  0           ['tf.compat.v1.shape_2[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.stack_2 (TFOpLambda)        (2,)                 0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.strided_slice_2 (TFOpLambda  (60, 128)           0           ['tf.stack_2[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 60, 128)     0           ['dense_10[0][0]',               \n",
      " mbda)                                                            'tf.strided_slice_2[0][0]']     \n",
      "                                                                                                  \n",
      " transformer_block_2 (Transform  (None, 60, 128)     561152      ['tf.__operators__.add_2[0][0]'] \n",
      " erBlock)                                                                                         \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 128)         0           ['transformer_block_2[0][0]']    \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 128)          16512       ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 128)          0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 3)            387         ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 579,587\n",
      "Trainable params: 579,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.1247 - accuracy: 0.3507\n",
      "Epoch 1: val_loss improved from inf to 1.10231, saving model to best_model_18.h5\n",
      "292/292 [==============================] - 45s 147ms/step - loss: 1.1247 - accuracy: 0.3507 - val_loss: 1.1023 - val_accuracy: 0.3005 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0972 - accuracy: 0.3691\n",
      "Epoch 2: val_loss improved from 1.10231 to 1.09340, saving model to best_model_18.h5\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0972 - accuracy: 0.3691 - val_loss: 1.0934 - val_accuracy: 0.3865 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0956 - accuracy: 0.3865\n",
      "Epoch 3: val_loss did not improve from 1.09340\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0956 - accuracy: 0.3865 - val_loss: 1.0943 - val_accuracy: 0.4048 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0950 - accuracy: 0.3916\n",
      "Epoch 4: val_loss did not improve from 1.09340\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0950 - accuracy: 0.3916 - val_loss: 1.0973 - val_accuracy: 0.3986 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0943 - accuracy: 0.3922\n",
      "Epoch 5: val_loss did not improve from 1.09340\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0943 - accuracy: 0.3922 - val_loss: 1.0966 - val_accuracy: 0.3916 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0948 - accuracy: 0.3917\n",
      "Epoch 6: val_loss did not improve from 1.09340\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0948 - accuracy: 0.3917 - val_loss: 1.0936 - val_accuracy: 0.3959 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0948 - accuracy: 0.3911\n",
      "Epoch 7: val_loss did not improve from 1.09340\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0948 - accuracy: 0.3911 - val_loss: 1.0937 - val_accuracy: 0.3939 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0936 - accuracy: 0.3982\n",
      "Epoch 8: val_loss did not improve from 1.09340\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0936 - accuracy: 0.3982 - val_loss: 1.0939 - val_accuracy: 0.3887 - lr: 0.0040\n",
      "Epoch 9/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0931 - accuracy: 0.3946\n",
      "Epoch 9: val_loss improved from 1.09340 to 1.09133, saving model to best_model_18.h5\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0931 - accuracy: 0.3946 - val_loss: 1.0913 - val_accuracy: 0.4010 - lr: 0.0040\n",
      "Epoch 10/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0925 - accuracy: 0.3980\n",
      "Epoch 10: val_loss improved from 1.09133 to 1.08620, saving model to best_model_18.h5\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0925 - accuracy: 0.3980 - val_loss: 1.0862 - val_accuracy: 0.4150 - lr: 0.0040\n",
      "Epoch 11/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0914 - accuracy: 0.3990\n",
      "Epoch 11: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0914 - accuracy: 0.3990 - val_loss: 1.1027 - val_accuracy: 0.3593 - lr: 0.0040\n",
      "Epoch 12/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0917 - accuracy: 0.3995\n",
      "Epoch 12: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0917 - accuracy: 0.3995 - val_loss: 1.0900 - val_accuracy: 0.3998 - lr: 0.0040\n",
      "Epoch 13/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0916 - accuracy: 0.3993\n",
      "Epoch 13: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0916 - accuracy: 0.3993 - val_loss: 1.0920 - val_accuracy: 0.3998 - lr: 0.0040\n",
      "Epoch 14/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0918 - accuracy: 0.4002\n",
      "Epoch 14: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0918 - accuracy: 0.4002 - val_loss: 1.0989 - val_accuracy: 0.3843 - lr: 0.0040\n",
      "Epoch 15/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0917 - accuracy: 0.4034\n",
      "Epoch 15: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0917 - accuracy: 0.4034 - val_loss: 1.0920 - val_accuracy: 0.3911 - lr: 0.0040\n",
      "Epoch 16/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0910 - accuracy: 0.4036\n",
      "Epoch 16: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 43s 147ms/step - loss: 1.0910 - accuracy: 0.4036 - val_loss: 1.0967 - val_accuracy: 0.3824 - lr: 0.0016\n",
      "Epoch 17/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0912 - accuracy: 0.4023\n",
      "Epoch 17: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0912 - accuracy: 0.4023 - val_loss: 1.0873 - val_accuracy: 0.4109 - lr: 0.0016\n",
      "Epoch 18/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0905 - accuracy: 0.4051\n",
      "Epoch 18: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 42s 145ms/step - loss: 1.0905 - accuracy: 0.4051 - val_loss: 1.0899 - val_accuracy: 0.4058 - lr: 0.0016\n",
      "Epoch 19/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0907 - accuracy: 0.4030\n",
      "Epoch 19: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0907 - accuracy: 0.4030 - val_loss: 1.0910 - val_accuracy: 0.4061 - lr: 0.0016\n",
      "Epoch 20/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0905 - accuracy: 0.4059\n",
      "Epoch 20: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 42s 144ms/step - loss: 1.0905 - accuracy: 0.4059 - val_loss: 1.0964 - val_accuracy: 0.3908 - lr: 0.0016\n",
      "Epoch 21/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0899 - accuracy: 0.4064\n",
      "Epoch 21: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0899 - accuracy: 0.4064 - val_loss: 1.0887 - val_accuracy: 0.4015 - lr: 6.4000e-04\n",
      "Epoch 22/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0897 - accuracy: 0.4065\n",
      "Epoch 22: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0897 - accuracy: 0.4065 - val_loss: 1.0909 - val_accuracy: 0.4041 - lr: 6.4000e-04\n",
      "Epoch 23/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0893 - accuracy: 0.4078\n",
      "Epoch 23: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0893 - accuracy: 0.4078 - val_loss: 1.0891 - val_accuracy: 0.4088 - lr: 6.4000e-04\n",
      "Epoch 24/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0896 - accuracy: 0.4060\n",
      "Epoch 24: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 40s 138ms/step - loss: 1.0896 - accuracy: 0.4060 - val_loss: 1.0889 - val_accuracy: 0.4036 - lr: 6.4000e-04\n",
      "Epoch 25/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0893 - accuracy: 0.4071\n",
      "Epoch 25: val_loss did not improve from 1.08620\n",
      "292/292 [==============================] - 41s 141ms/step - loss: 1.0893 - accuracy: 0.4071 - val_loss: 1.0884 - val_accuracy: 0.4097 - lr: 6.4000e-04\n",
      "Epoch 26/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0886 - accuracy: 0.4090\n",
      "Epoch 26: val_loss improved from 1.08620 to 1.08292, saving model to best_model_18.h5\n",
      "292/292 [==============================] - 41s 141ms/step - loss: 1.0886 - accuracy: 0.4090 - val_loss: 1.0829 - val_accuracy: 0.4174 - lr: 2.5600e-04\n",
      "Epoch 27/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0886 - accuracy: 0.4070\n",
      "Epoch 27: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0886 - accuracy: 0.4070 - val_loss: 1.0868 - val_accuracy: 0.4055 - lr: 2.5600e-04\n",
      "Epoch 28/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0885 - accuracy: 0.4077\n",
      "Epoch 28: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0885 - accuracy: 0.4077 - val_loss: 1.0898 - val_accuracy: 0.4061 - lr: 2.5600e-04\n",
      "Epoch 29/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0884 - accuracy: 0.4082\n",
      "Epoch 29: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 41s 139ms/step - loss: 1.0884 - accuracy: 0.4082 - val_loss: 1.0879 - val_accuracy: 0.4100 - lr: 2.5600e-04\n",
      "Epoch 30/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0882 - accuracy: 0.4093\n",
      "Epoch 30: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0882 - accuracy: 0.4093 - val_loss: 1.0833 - val_accuracy: 0.4096 - lr: 2.5600e-04\n",
      "Epoch 31/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0883 - accuracy: 0.4072\n",
      "Epoch 31: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 40s 138ms/step - loss: 1.0883 - accuracy: 0.4072 - val_loss: 1.0860 - val_accuracy: 0.4118 - lr: 2.5600e-04\n",
      "Epoch 32/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0880 - accuracy: 0.4090\n",
      "Epoch 32: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 40s 138ms/step - loss: 1.0880 - accuracy: 0.4090 - val_loss: 1.0870 - val_accuracy: 0.4039 - lr: 1.0240e-04\n",
      "Epoch 33/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.4071\n",
      "Epoch 33: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0878 - accuracy: 0.4071 - val_loss: 1.0849 - val_accuracy: 0.4097 - lr: 1.0240e-04\n",
      "Epoch 34/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.4086\n",
      "Epoch 34: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0878 - accuracy: 0.4086 - val_loss: 1.0865 - val_accuracy: 0.4053 - lr: 1.0240e-04\n",
      "Epoch 35/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.4082\n",
      "Epoch 35: val_loss did not improve from 1.08292\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0878 - accuracy: 0.4082 - val_loss: 1.0856 - val_accuracy: 0.4083 - lr: 1.0240e-04\n",
      "Epoch 36/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.4093\n",
      "Epoch 36: val_loss improved from 1.08292 to 1.08254, saving model to best_model_18.h5\n",
      "292/292 [==============================] - 40s 137ms/step - loss: 1.0878 - accuracy: 0.4093 - val_loss: 1.0825 - val_accuracy: 0.4104 - lr: 1.0240e-04\n",
      "Epoch 37/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.4085\n",
      "Epoch 37: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 40s 139ms/step - loss: 1.0878 - accuracy: 0.4085 - val_loss: 1.0859 - val_accuracy: 0.4075 - lr: 1.0240e-04\n",
      "Epoch 38/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0879 - accuracy: 0.4078\n",
      "Epoch 38: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 43s 147ms/step - loss: 1.0879 - accuracy: 0.4078 - val_loss: 1.0893 - val_accuracy: 0.3991 - lr: 1.0240e-04\n",
      "Epoch 39/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.4078\n",
      "Epoch 39: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 43s 147ms/step - loss: 1.0878 - accuracy: 0.4078 - val_loss: 1.0833 - val_accuracy: 0.4118 - lr: 1.0240e-04\n",
      "Epoch 40/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0877 - accuracy: 0.4101\n",
      "Epoch 40: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 42s 145ms/step - loss: 1.0877 - accuracy: 0.4101 - val_loss: 1.0849 - val_accuracy: 0.4084 - lr: 1.0240e-04\n",
      "Epoch 41/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0877 - accuracy: 0.4088\n",
      "Epoch 41: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 40s 138ms/step - loss: 1.0877 - accuracy: 0.4088 - val_loss: 1.0839 - val_accuracy: 0.4126 - lr: 1.0240e-04\n",
      "Epoch 42/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0877 - accuracy: 0.4096\n",
      "Epoch 42: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 41s 140ms/step - loss: 1.0877 - accuracy: 0.4096 - val_loss: 1.0839 - val_accuracy: 0.4118 - lr: 4.0960e-05\n",
      "Epoch 43/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.4096\n",
      "Epoch 43: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 42s 143ms/step - loss: 1.0878 - accuracy: 0.4096 - val_loss: 1.0859 - val_accuracy: 0.4089 - lr: 4.0960e-05\n",
      "Epoch 44/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0880 - accuracy: 0.4091\n",
      "Epoch 44: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 42s 145ms/step - loss: 1.0880 - accuracy: 0.4091 - val_loss: 1.0850 - val_accuracy: 0.4105 - lr: 4.0960e-05\n",
      "Epoch 45/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0881 - accuracy: 0.4081\n",
      "Epoch 45: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 43s 147ms/step - loss: 1.0881 - accuracy: 0.4081 - val_loss: 1.0865 - val_accuracy: 0.4064 - lr: 4.0960e-05\n",
      "Epoch 46/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0882 - accuracy: 0.4085\n",
      "Epoch 46: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 43s 147ms/step - loss: 1.0882 - accuracy: 0.4085 - val_loss: 1.0840 - val_accuracy: 0.4102 - lr: 4.0960e-05\n",
      "Epoch 47/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0884 - accuracy: 0.4084\n",
      "Epoch 47: val_loss did not improve from 1.08254\n",
      "292/292 [==============================] - 43s 146ms/step - loss: 1.0884 - accuracy: 0.4084 - val_loss: 1.0860 - val_accuracy: 0.4090 - lr: 1.6384e-05\n",
      "Epoch 48/100\n",
      "292/292 [==============================] - ETA: 0s - loss: 1.0887 - accuracy: 0.4076"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\final_tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5. 모델 생성 및 학습\n",
    "input_shape = X.shape[1:]  # 10분 동안의 데이터 \n",
    "model = create_transformer_model(input_shape, num_heads=4, ff_dim=128, d_model=128, num_classes=3)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=tfa.optimizers.AdamW(learning_rate=0.01, weight_decay=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=512, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    class_weight=class_weights_dict,\n",
    "                    callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 9s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.18      0.24     11220\n",
      "           1       0.45      0.72      0.55     15278\n",
      "           2       0.32      0.22      0.26     10834\n",
      "\n",
      "    accuracy                           0.41     37332\n",
      "   macro avg       0.38      0.37      0.35     37332\n",
      "weighted avg       0.39      0.41      0.37     37332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 성능 평가 지표\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 손실 시각화\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 시각화\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 정확도 시각화\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 혼동 행렬 계산 및 시각화\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

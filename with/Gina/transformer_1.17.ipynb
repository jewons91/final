{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다시 얕은 신경망 120분으로 묶고, 사용 데이터 늘리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149279, 120, 11)\n",
      "(149279,)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 전처리\n",
    "def create_classification_targets(data, window_size=120, pred_offset=3):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - window_size - pred_offset + 1):\n",
    "        sequence = data.iloc[i:i+window_size]\n",
    "        scaled_sequence = scale_within_sequence(sequence)\n",
    "        X.append(scaled_sequence)\n",
    "        # 3분 뒤의 Close 가격 변화에 따른 클래스 설정\n",
    "        future_close = data.iloc[i+window_size+pred_offset-1]['Close']\n",
    "        current_close = data.iloc[i+window_size-1]['Close']\n",
    "        \n",
    "        # 가격 변화에 따라 클래스 설정: 상승(2), 보합(1), 하락(0)\n",
    "        if future_close > current_close:\n",
    "            y.append(2)  # 상승\n",
    "        elif future_close < current_close:\n",
    "            y.append(0)  # 하락\n",
    "        else:\n",
    "            y.append(1)  # 보합\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 데이터 불러오기 및 전처리\n",
    "data = pd.read_csv('./000660.csv', parse_dates=['Unnamed: 0'])\n",
    "data = data.rename(columns={'Unnamed: 0': 'Time', \n",
    "                            '매수량': 'BuyVolume', \n",
    "                            '매도량': 'SellVolume', \n",
    "                            '종가': 'Close', \n",
    "                            '저가': 'Low',\n",
    "                            '시가': 'Open',\n",
    "                            '고가': 'High'})\n",
    "data['Power'] = data['BuyVolume'] - data['SellVolume']\n",
    "columns = ['Close', 'Power', 'Low', 'Open', 'High', 'MA5', 'MA20', 'MA60', 'MA120', 'Upper_Band', 'Lower_Band']\n",
    "data = data[columns].dropna()\n",
    "\n",
    "# 데이터 정규화\n",
    "# Power 전체 스케일\n",
    "volume_scaler = RobustScaler()\n",
    "data['Scaled_Power'] = volume_scaler.fit_transform(data[['Power']])\n",
    "\n",
    "# 시퀀스내 스케일 종가, 저가\n",
    "def scale_within_sequence(sequence):\n",
    "    # 종가 스케일 시퀀스내\n",
    "    close_scaler = RobustScaler()\n",
    "    close_scaled = close_scaler.fit_transform(sequence[['Close']].values)\n",
    "    \n",
    "    # 나머지 스케일 시퀀스내\n",
    "    low_scaled = close_scaler.transform(sequence[['Low']].values)\n",
    "    open_scaled = close_scaler.transform(sequence[['Open']].values)\n",
    "    high_scaled = close_scaler.transform(sequence[['High']].values)\n",
    "    ma5_scaled = close_scaler.transform(sequence[['MA5']].values)\n",
    "    ma20_scaled = close_scaler.transform(sequence[['MA20']].values)\n",
    "    ma60_scaled = close_scaler.transform(sequence[['MA60']].values)\n",
    "    ma120_scaled = close_scaler.transform(sequence[['MA120']].values)\n",
    "    upperBand_scaled = close_scaler.transform(sequence[['Upper_Band']].values)\n",
    "    lowerBand_scaled = close_scaler.transform(sequence[['Lower_Band']].values)\n",
    "    \n",
    "    # 거래량 미리 스케일값 사용\n",
    "    power_scaled = sequence[['Scaled_Power']].values\n",
    "    \n",
    "    return np.hstack([open_scaled, high_scaled, low_scaled, close_scaled, power_scaled, \n",
    "                      ma5_scaled, ma20_scaled, ma60_scaled, ma120_scaled, upperBand_scaled, lowerBand_scaled])\n",
    "\n",
    "\n",
    "X, y = create_classification_targets(data)\n",
    "\n",
    "# 학습/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)  # (samples, 10, 3)\n",
    "print(y_train.shape)  # (samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52500 45668 51111]\n"
     ]
    }
   ],
   "source": [
    "# y 클래스 데이터 분포 확인\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 가중치 계산\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Positional Encoding 구현\n",
    "def positional_encoding(max_len, d_model):\n",
    "    pos = np.arange(max_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "\n",
    "    # 짝수 인덱스에 대해 sin 적용\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 홀수 인덱스에 대해 cos 적용\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    return pos_encoding\n",
    "\n",
    "def add_positional_encoding(inputs, max_len, d_model):\n",
    "    pos_encoding = positional_encoding(max_len, d_model)\n",
    "    pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    pos_encoded_inputs = inputs + pos_encoding[:tf.shape(inputs)[1], :]\n",
    "    return pos_encoded_inputs\n",
    "\n",
    "# 3. Transformer 블록 구현\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"gelu\", kernel_initializer='he_normal'), layers.Dense(d_model)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output1 = self.att1(inputs, inputs)\n",
    "        attn_output1 = self.dropout1(attn_output1)\n",
    "        out1 = self.layernorm1(inputs + attn_output1)\n",
    "        attn_output2 = self.att2(out1, out1)\n",
    "        attn_output2 = self.dropout2(attn_output2)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout1(ffn_output)\n",
    "        return self.layernorm2(out2 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# 4. Transformer 모델 구성\n",
    "def create_transformer_model(input_shape, num_heads, ff_dim, d_model, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # 임베딩 및 위치 인코딩\n",
    "    embedding_layer = layers.Dense(d_model, kernel_initializer='he_normal')(inputs)\n",
    "    pos_encoded_inputs = add_positional_encoding(embedding_layer, max_len=input_shape[0], d_model=d_model)\n",
    "\n",
    "    # Transformer 블록 적용\n",
    "    transformer_block = TransformerBlock(d_model, num_heads, ff_dim)\n",
    "    x = transformer_block(pos_encoded_inputs)\n",
    "    \n",
    "    # 출력 레이어\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation=\"gelu\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=100,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model_17.2.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.8, \n",
    "    patience=50, \n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 120, 11)]    0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 120, 128)     1536        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOpLamb  (3,)                0           ['dense_5[0][0]']                \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  ()                  0           ['tf.compat.v1.shape_1[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.stack_1 (TFOpLambda)        (2,)                 0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.strided_slice_1 (TFOpLambda  (120, 128)          0           ['tf.stack_1[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 120, 128)    0           ['dense_5[0][0]',                \n",
      " mbda)                                                            'tf.strided_slice_1[0][0]']     \n",
      "                                                                                                  \n",
      " transformer_block_1 (Transform  (None, 120, 128)    561152      ['tf.__operators__.add_1[0][0]'] \n",
      " erBlock)                                                                                         \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['transformer_block_1[0][0]']    \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          16512       ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 3)            387         ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 579,587\n",
      "Trainable params: 579,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0946 - accuracy: 0.3652\n",
      "Epoch 1: val_loss improved from inf to 1.08472, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 107s 90ms/step - loss: 1.0946 - accuracy: 0.3652 - val_loss: 1.0847 - val_accuracy: 0.3780 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0873 - accuracy: 0.3736\n",
      "Epoch 2: val_loss did not improve from 1.08472\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0873 - accuracy: 0.3736 - val_loss: 1.0848 - val_accuracy: 0.3743 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0865 - accuracy: 0.3738\n",
      "Epoch 3: val_loss did not improve from 1.08472\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0865 - accuracy: 0.3738 - val_loss: 1.0942 - val_accuracy: 0.3573 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0890 - accuracy: 0.3683\n",
      "Epoch 4: val_loss did not improve from 1.08472\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0890 - accuracy: 0.3683 - val_loss: 1.0918 - val_accuracy: 0.3572 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0899 - accuracy: 0.3636\n",
      "Epoch 5: val_loss did not improve from 1.08472\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0899 - accuracy: 0.3636 - val_loss: 1.0894 - val_accuracy: 0.3563 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0903 - accuracy: 0.3636\n",
      "Epoch 6: val_loss did not improve from 1.08472\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0902 - accuracy: 0.3636 - val_loss: 1.0864 - val_accuracy: 0.3712 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0865 - accuracy: 0.3719\n",
      "Epoch 7: val_loss did not improve from 1.08472\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0865 - accuracy: 0.3719 - val_loss: 1.0847 - val_accuracy: 0.3788 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0855 - accuracy: 0.3748\n",
      "Epoch 8: val_loss did not improve from 1.08472\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0855 - accuracy: 0.3748 - val_loss: 1.0879 - val_accuracy: 0.3661 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0837 - accuracy: 0.3780\n",
      "Epoch 9: val_loss improved from 1.08472 to 1.08366, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0837 - accuracy: 0.3780 - val_loss: 1.0837 - val_accuracy: 0.3787 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0823 - accuracy: 0.3784\n",
      "Epoch 10: val_loss improved from 1.08366 to 1.07973, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0823 - accuracy: 0.3784 - val_loss: 1.0797 - val_accuracy: 0.3756 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0817 - accuracy: 0.3790\n",
      "Epoch 11: val_loss did not improve from 1.07973\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0817 - accuracy: 0.3790 - val_loss: 1.0811 - val_accuracy: 0.3842 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0811 - accuracy: 0.3797\n",
      "Epoch 12: val_loss did not improve from 1.07973\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0811 - accuracy: 0.3797 - val_loss: 1.0850 - val_accuracy: 0.3781 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0805 - accuracy: 0.3827\n",
      "Epoch 13: val_loss did not improve from 1.07973\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0805 - accuracy: 0.3827 - val_loss: 1.0855 - val_accuracy: 0.3717 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0804 - accuracy: 0.3821\n",
      "Epoch 14: val_loss did not improve from 1.07973\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0804 - accuracy: 0.3821 - val_loss: 1.0807 - val_accuracy: 0.3776 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0803 - accuracy: 0.3823\n",
      "Epoch 15: val_loss did not improve from 1.07973\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0803 - accuracy: 0.3823 - val_loss: 1.0800 - val_accuracy: 0.3846 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0803 - accuracy: 0.3813\n",
      "Epoch 16: val_loss improved from 1.07973 to 1.07831, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0803 - accuracy: 0.3813 - val_loss: 1.0783 - val_accuracy: 0.3872 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0795 - accuracy: 0.3838\n",
      "Epoch 17: val_loss improved from 1.07831 to 1.07694, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0795 - accuracy: 0.3838 - val_loss: 1.0769 - val_accuracy: 0.3890 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0795 - accuracy: 0.3820\n",
      "Epoch 18: val_loss did not improve from 1.07694\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0795 - accuracy: 0.3820 - val_loss: 1.0870 - val_accuracy: 0.3702 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0793 - accuracy: 0.3838\n",
      "Epoch 19: val_loss did not improve from 1.07694\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0793 - accuracy: 0.3838 - val_loss: 1.0822 - val_accuracy: 0.3797 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0789 - accuracy: 0.3856\n",
      "Epoch 20: val_loss did not improve from 1.07694\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0789 - accuracy: 0.3856 - val_loss: 1.0810 - val_accuracy: 0.3822 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0785 - accuracy: 0.3847\n",
      "Epoch 21: val_loss did not improve from 1.07694\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0785 - accuracy: 0.3847 - val_loss: 1.0802 - val_accuracy: 0.3823 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0786 - accuracy: 0.3862\n",
      "Epoch 22: val_loss did not improve from 1.07694\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0786 - accuracy: 0.3862 - val_loss: 1.0795 - val_accuracy: 0.3851 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0785 - accuracy: 0.3849\n",
      "Epoch 23: val_loss did not improve from 1.07694\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0785 - accuracy: 0.3849 - val_loss: 1.0775 - val_accuracy: 0.3888 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0781 - accuracy: 0.3861\n",
      "Epoch 24: val_loss improved from 1.07694 to 1.07632, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0781 - accuracy: 0.3861 - val_loss: 1.0763 - val_accuracy: 0.3905 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0779 - accuracy: 0.3862\n",
      "Epoch 25: val_loss did not improve from 1.07632\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0779 - accuracy: 0.3862 - val_loss: 1.0791 - val_accuracy: 0.3838 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0776 - accuracy: 0.3868\n",
      "Epoch 26: val_loss did not improve from 1.07632\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0776 - accuracy: 0.3868 - val_loss: 1.0857 - val_accuracy: 0.3762 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0773 - accuracy: 0.3874\n",
      "Epoch 27: val_loss did not improve from 1.07632\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0773 - accuracy: 0.3874 - val_loss: 1.0774 - val_accuracy: 0.3871 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0773 - accuracy: 0.3877\n",
      "Epoch 28: val_loss did not improve from 1.07632\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0773 - accuracy: 0.3876 - val_loss: 1.0766 - val_accuracy: 0.3880 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0557 - accuracy: 0.4211\n",
      "Epoch 29: val_loss improved from 1.07632 to 1.04031, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0557 - accuracy: 0.4211 - val_loss: 1.0403 - val_accuracy: 0.4468 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0418 - accuracy: 0.4425\n",
      "Epoch 30: val_loss improved from 1.04031 to 1.03872, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0418 - accuracy: 0.4424 - val_loss: 1.0387 - val_accuracy: 0.4459 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0408 - accuracy: 0.4427\n",
      "Epoch 31: val_loss improved from 1.03872 to 1.03301, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0408 - accuracy: 0.4427 - val_loss: 1.0330 - val_accuracy: 0.4537 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0389 - accuracy: 0.4452\n",
      "Epoch 32: val_loss did not improve from 1.03301\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0389 - accuracy: 0.4452 - val_loss: 1.0398 - val_accuracy: 0.4485 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0388 - accuracy: 0.4462\n",
      "Epoch 33: val_loss improved from 1.03301 to 1.03217, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0388 - accuracy: 0.4462 - val_loss: 1.0322 - val_accuracy: 0.4542 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0377 - accuracy: 0.4458\n",
      "Epoch 34: val_loss did not improve from 1.03217\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0377 - accuracy: 0.4459 - val_loss: 1.0387 - val_accuracy: 0.4408 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0376 - accuracy: 0.4459\n",
      "Epoch 35: val_loss did not improve from 1.03217\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0376 - accuracy: 0.4459 - val_loss: 1.0341 - val_accuracy: 0.4530 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0371 - accuracy: 0.4472\n",
      "Epoch 36: val_loss improved from 1.03217 to 1.03212, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0371 - accuracy: 0.4472 - val_loss: 1.0321 - val_accuracy: 0.4569 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.4505\n",
      "Epoch 37: val_loss did not improve from 1.03212\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0355 - accuracy: 0.4505 - val_loss: 1.0346 - val_accuracy: 0.4456 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0365 - accuracy: 0.4484\n",
      "Epoch 38: val_loss did not improve from 1.03212\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0365 - accuracy: 0.4484 - val_loss: 1.0350 - val_accuracy: 0.4489 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0360 - accuracy: 0.4496\n",
      "Epoch 39: val_loss did not improve from 1.03212\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0360 - accuracy: 0.4496 - val_loss: 1.0335 - val_accuracy: 0.4512 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0363 - accuracy: 0.4512\n",
      "Epoch 40: val_loss improved from 1.03212 to 1.03065, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0363 - accuracy: 0.4513 - val_loss: 1.0307 - val_accuracy: 0.4606 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4501\n",
      "Epoch 41: val_loss improved from 1.03065 to 1.02972, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0351 - accuracy: 0.4501 - val_loss: 1.0297 - val_accuracy: 0.4567 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4513\n",
      "Epoch 42: val_loss did not improve from 1.02972\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0352 - accuracy: 0.4513 - val_loss: 1.0371 - val_accuracy: 0.4416 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4511\n",
      "Epoch 43: val_loss did not improve from 1.02972\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0345 - accuracy: 0.4510 - val_loss: 1.0398 - val_accuracy: 0.4386 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4508\n",
      "Epoch 44: val_loss did not improve from 1.02972\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0343 - accuracy: 0.4508 - val_loss: 1.0320 - val_accuracy: 0.4550 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4526\n",
      "Epoch 45: val_loss did not improve from 1.02972\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0339 - accuracy: 0.4525 - val_loss: 1.0347 - val_accuracy: 0.4508 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4543\n",
      "Epoch 46: val_loss did not improve from 1.02972\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0336 - accuracy: 0.4543 - val_loss: 1.0309 - val_accuracy: 0.4623 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0333 - accuracy: 0.4551\n",
      "Epoch 47: val_loss did not improve from 1.02972\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0333 - accuracy: 0.4551 - val_loss: 1.0319 - val_accuracy: 0.4545 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0322 - accuracy: 0.4555\n",
      "Epoch 48: val_loss did not improve from 1.02972\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0322 - accuracy: 0.4555 - val_loss: 1.0333 - val_accuracy: 0.4506 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0323 - accuracy: 0.4541\n",
      "Epoch 49: val_loss improved from 1.02972 to 1.02739, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0323 - accuracy: 0.4541 - val_loss: 1.0274 - val_accuracy: 0.4618 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0331 - accuracy: 0.4529\n",
      "Epoch 50: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0331 - accuracy: 0.4529 - val_loss: 1.0353 - val_accuracy: 0.4537 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.4547\n",
      "Epoch 51: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0332 - accuracy: 0.4547 - val_loss: 1.0295 - val_accuracy: 0.4593 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0321 - accuracy: 0.4556\n",
      "Epoch 52: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0321 - accuracy: 0.4556 - val_loss: 1.0316 - val_accuracy: 0.4533 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4565\n",
      "Epoch 53: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0312 - accuracy: 0.4566 - val_loss: 1.0359 - val_accuracy: 0.4488 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0316 - accuracy: 0.4571\n",
      "Epoch 54: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0316 - accuracy: 0.4571 - val_loss: 1.0334 - val_accuracy: 0.4474 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0308 - accuracy: 0.4564\n",
      "Epoch 55: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0308 - accuracy: 0.4564 - val_loss: 1.0304 - val_accuracy: 0.4594 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0323 - accuracy: 0.4550\n",
      "Epoch 56: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0323 - accuracy: 0.4549 - val_loss: 1.0477 - val_accuracy: 0.4498 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0321 - accuracy: 0.4546\n",
      "Epoch 57: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0321 - accuracy: 0.4546 - val_loss: 1.0279 - val_accuracy: 0.4618 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4563\n",
      "Epoch 58: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0313 - accuracy: 0.4562 - val_loss: 1.0359 - val_accuracy: 0.4536 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0309 - accuracy: 0.4573\n",
      "Epoch 59: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0309 - accuracy: 0.4573 - val_loss: 1.0309 - val_accuracy: 0.4554 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0300 - accuracy: 0.4574\n",
      "Epoch 60: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0300 - accuracy: 0.4573 - val_loss: 1.0331 - val_accuracy: 0.4543 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0301 - accuracy: 0.4570\n",
      "Epoch 61: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0301 - accuracy: 0.4570 - val_loss: 1.0378 - val_accuracy: 0.4447 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0309 - accuracy: 0.4572\n",
      "Epoch 62: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0308 - accuracy: 0.4573 - val_loss: 1.0297 - val_accuracy: 0.4573 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0300 - accuracy: 0.4577\n",
      "Epoch 63: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0300 - accuracy: 0.4577 - val_loss: 1.0289 - val_accuracy: 0.4617 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.4583\n",
      "Epoch 64: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0297 - accuracy: 0.4583 - val_loss: 1.0290 - val_accuracy: 0.4588 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0307 - accuracy: 0.4570\n",
      "Epoch 65: val_loss did not improve from 1.02739\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0307 - accuracy: 0.4570 - val_loss: 1.0291 - val_accuracy: 0.4626 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.4564\n",
      "Epoch 66: val_loss improved from 1.02739 to 1.02611, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0296 - accuracy: 0.4564 - val_loss: 1.0261 - val_accuracy: 0.4646 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0293 - accuracy: 0.4568\n",
      "Epoch 67: val_loss did not improve from 1.02611\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0294 - accuracy: 0.4568 - val_loss: 1.0274 - val_accuracy: 0.4599 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.4578\n",
      "Epoch 68: val_loss did not improve from 1.02611\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0298 - accuracy: 0.4578 - val_loss: 1.0297 - val_accuracy: 0.4557 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4592\n",
      "Epoch 69: val_loss did not improve from 1.02611\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0287 - accuracy: 0.4592 - val_loss: 1.0268 - val_accuracy: 0.4647 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0315 - accuracy: 0.4555\n",
      "Epoch 70: val_loss did not improve from 1.02611\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0315 - accuracy: 0.4555 - val_loss: 1.0314 - val_accuracy: 0.4610 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0294 - accuracy: 0.4591\n",
      "Epoch 71: val_loss improved from 1.02611 to 1.02406, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0294 - accuracy: 0.4591 - val_loss: 1.0241 - val_accuracy: 0.4644 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.4571\n",
      "Epoch 72: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0296 - accuracy: 0.4571 - val_loss: 1.0300 - val_accuracy: 0.4563 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0299 - accuracy: 0.4565\n",
      "Epoch 73: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0299 - accuracy: 0.4565 - val_loss: 1.0286 - val_accuracy: 0.4607 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.4556\n",
      "Epoch 74: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0297 - accuracy: 0.4556 - val_loss: 1.0241 - val_accuracy: 0.4678 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0294 - accuracy: 0.4591\n",
      "Epoch 75: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0294 - accuracy: 0.4591 - val_loss: 1.0434 - val_accuracy: 0.4470 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0305 - accuracy: 0.4560\n",
      "Epoch 76: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0305 - accuracy: 0.4560 - val_loss: 1.0308 - val_accuracy: 0.4557 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0290 - accuracy: 0.4579\n",
      "Epoch 77: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0290 - accuracy: 0.4579 - val_loss: 1.0333 - val_accuracy: 0.4519 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.4590\n",
      "Epoch 78: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0291 - accuracy: 0.4590 - val_loss: 1.0272 - val_accuracy: 0.4593 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.4579\n",
      "Epoch 79: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0298 - accuracy: 0.4578 - val_loss: 1.0256 - val_accuracy: 0.4631 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0292 - accuracy: 0.4582\n",
      "Epoch 80: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0293 - accuracy: 0.4582 - val_loss: 1.0300 - val_accuracy: 0.4517 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0286 - accuracy: 0.4585\n",
      "Epoch 81: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0285 - accuracy: 0.4585 - val_loss: 1.0330 - val_accuracy: 0.4568 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0289 - accuracy: 0.4568\n",
      "Epoch 82: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0288 - accuracy: 0.4568 - val_loss: 1.0270 - val_accuracy: 0.4605 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0289 - accuracy: 0.4591\n",
      "Epoch 83: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0289 - accuracy: 0.4590 - val_loss: 1.0271 - val_accuracy: 0.4597 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0285 - accuracy: 0.4576\n",
      "Epoch 84: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0285 - accuracy: 0.4576 - val_loss: 1.0348 - val_accuracy: 0.4544 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4582\n",
      "Epoch 85: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0282 - accuracy: 0.4582 - val_loss: 1.0349 - val_accuracy: 0.4506 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4585\n",
      "Epoch 86: val_loss did not improve from 1.02406\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0281 - accuracy: 0.4586 - val_loss: 1.0373 - val_accuracy: 0.4518 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0281 - accuracy: 0.4589\n",
      "Epoch 87: val_loss improved from 1.02406 to 1.02238, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0281 - accuracy: 0.4590 - val_loss: 1.0224 - val_accuracy: 0.4659 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4584\n",
      "Epoch 88: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0287 - accuracy: 0.4584 - val_loss: 1.0326 - val_accuracy: 0.4509 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4581\n",
      "Epoch 89: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0287 - accuracy: 0.4581 - val_loss: 1.0230 - val_accuracy: 0.4650 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0281 - accuracy: 0.4592\n",
      "Epoch 90: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0281 - accuracy: 0.4592 - val_loss: 1.0334 - val_accuracy: 0.4549 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0284 - accuracy: 0.4580\n",
      "Epoch 91: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0283 - accuracy: 0.4581 - val_loss: 1.0280 - val_accuracy: 0.4613 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0294 - accuracy: 0.4584\n",
      "Epoch 92: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0294 - accuracy: 0.4584 - val_loss: 1.0269 - val_accuracy: 0.4645 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0304 - accuracy: 0.4562\n",
      "Epoch 93: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0304 - accuracy: 0.4561 - val_loss: 1.0286 - val_accuracy: 0.4583 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4582\n",
      "Epoch 94: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0287 - accuracy: 0.4581 - val_loss: 1.0250 - val_accuracy: 0.4642 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4580\n",
      "Epoch 95: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0282 - accuracy: 0.4580 - val_loss: 1.0293 - val_accuracy: 0.4578 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4591\n",
      "Epoch 96: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0282 - accuracy: 0.4591 - val_loss: 1.0343 - val_accuracy: 0.4546 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0283 - accuracy: 0.4570\n",
      "Epoch 97: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0283 - accuracy: 0.4570 - val_loss: 1.0324 - val_accuracy: 0.4613 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0290 - accuracy: 0.4582\n",
      "Epoch 98: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0290 - accuracy: 0.4582 - val_loss: 1.0280 - val_accuracy: 0.4599 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0289 - accuracy: 0.4574\n",
      "Epoch 99: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0289 - accuracy: 0.4574 - val_loss: 1.0371 - val_accuracy: 0.4524 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0285 - accuracy: 0.4573\n",
      "Epoch 100: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0285 - accuracy: 0.4573 - val_loss: 1.0321 - val_accuracy: 0.4561 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0284 - accuracy: 0.4568\n",
      "Epoch 101: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0284 - accuracy: 0.4568 - val_loss: 1.0262 - val_accuracy: 0.4663 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4581\n",
      "Epoch 102: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0282 - accuracy: 0.4581 - val_loss: 1.0292 - val_accuracy: 0.4583 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0285 - accuracy: 0.4577\n",
      "Epoch 103: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0285 - accuracy: 0.4577 - val_loss: 1.0341 - val_accuracy: 0.4481 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0285 - accuracy: 0.4593\n",
      "Epoch 104: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0285 - accuracy: 0.4593 - val_loss: 1.0309 - val_accuracy: 0.4573 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0278 - accuracy: 0.4580\n",
      "Epoch 105: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0278 - accuracy: 0.4580 - val_loss: 1.0271 - val_accuracy: 0.4643 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0281 - accuracy: 0.4592\n",
      "Epoch 106: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0281 - accuracy: 0.4592 - val_loss: 1.0295 - val_accuracy: 0.4612 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0281 - accuracy: 0.4593\n",
      "Epoch 107: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0281 - accuracy: 0.4593 - val_loss: 1.0306 - val_accuracy: 0.4610 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0289 - accuracy: 0.4584\n",
      "Epoch 108: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0289 - accuracy: 0.4584 - val_loss: 1.0264 - val_accuracy: 0.4622 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0278 - accuracy: 0.4589\n",
      "Epoch 109: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0278 - accuracy: 0.4589 - val_loss: 1.0281 - val_accuracy: 0.4611 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4576\n",
      "Epoch 110: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0282 - accuracy: 0.4576 - val_loss: 1.0313 - val_accuracy: 0.4565 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4609\n",
      "Epoch 111: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0270 - accuracy: 0.4609 - val_loss: 1.0308 - val_accuracy: 0.4596 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.4576\n",
      "Epoch 112: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0298 - accuracy: 0.4576 - val_loss: 1.0305 - val_accuracy: 0.4572 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0326 - accuracy: 0.4550\n",
      "Epoch 113: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0325 - accuracy: 0.4550 - val_loss: 1.0486 - val_accuracy: 0.4291 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0299 - accuracy: 0.4581\n",
      "Epoch 114: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0299 - accuracy: 0.4581 - val_loss: 1.0278 - val_accuracy: 0.4603 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4583\n",
      "Epoch 115: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0287 - accuracy: 0.4583 - val_loss: 1.0393 - val_accuracy: 0.4415 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4590\n",
      "Epoch 116: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0282 - accuracy: 0.4590 - val_loss: 1.0277 - val_accuracy: 0.4604 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0285 - accuracy: 0.4593\n",
      "Epoch 117: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0284 - accuracy: 0.4594 - val_loss: 1.0256 - val_accuracy: 0.4598 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0278 - accuracy: 0.4601\n",
      "Epoch 118: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0278 - accuracy: 0.4601 - val_loss: 1.0236 - val_accuracy: 0.4639 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4580\n",
      "Epoch 119: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0282 - accuracy: 0.4580 - val_loss: 1.0245 - val_accuracy: 0.4644 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0285 - accuracy: 0.4583\n",
      "Epoch 120: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0285 - accuracy: 0.4583 - val_loss: 1.0341 - val_accuracy: 0.4596 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0290 - accuracy: 0.4572\n",
      "Epoch 121: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0290 - accuracy: 0.4572 - val_loss: 1.0329 - val_accuracy: 0.4557 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4593\n",
      "Epoch 122: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0287 - accuracy: 0.4593 - val_loss: 1.0330 - val_accuracy: 0.4547 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0283 - accuracy: 0.4599\n",
      "Epoch 123: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0283 - accuracy: 0.4599 - val_loss: 1.0263 - val_accuracy: 0.4590 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0281 - accuracy: 0.4587\n",
      "Epoch 124: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0281 - accuracy: 0.4587 - val_loss: 1.0293 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0279 - accuracy: 0.4584\n",
      "Epoch 125: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0279 - accuracy: 0.4584 - val_loss: 1.0260 - val_accuracy: 0.4658 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0280 - accuracy: 0.4583\n",
      "Epoch 126: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0280 - accuracy: 0.4582 - val_loss: 1.0266 - val_accuracy: 0.4590 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0272 - accuracy: 0.4599\n",
      "Epoch 127: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0272 - accuracy: 0.4600 - val_loss: 1.0414 - val_accuracy: 0.4464 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0268 - accuracy: 0.4610\n",
      "Epoch 128: val_loss did not improve from 1.02238\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0268 - accuracy: 0.4610 - val_loss: 1.0562 - val_accuracy: 0.4421 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0274 - accuracy: 0.4593\n",
      "Epoch 129: val_loss improved from 1.02238 to 1.02191, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0273 - accuracy: 0.4593 - val_loss: 1.0219 - val_accuracy: 0.4658 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4600\n",
      "Epoch 130: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 107s 92ms/step - loss: 1.0271 - accuracy: 0.4599 - val_loss: 1.0263 - val_accuracy: 0.4623 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0268 - accuracy: 0.4609\n",
      "Epoch 131: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0268 - accuracy: 0.4609 - val_loss: 1.0333 - val_accuracy: 0.4522 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4601\n",
      "Epoch 132: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0270 - accuracy: 0.4601 - val_loss: 1.0243 - val_accuracy: 0.4634 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0273 - accuracy: 0.4600\n",
      "Epoch 133: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0273 - accuracy: 0.4600 - val_loss: 1.0245 - val_accuracy: 0.4648 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0274 - accuracy: 0.4601\n",
      "Epoch 134: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0274 - accuracy: 0.4600 - val_loss: 1.0411 - val_accuracy: 0.4484 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0276 - accuracy: 0.4606\n",
      "Epoch 135: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0276 - accuracy: 0.4606 - val_loss: 1.0283 - val_accuracy: 0.4630 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4590\n",
      "Epoch 136: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0270 - accuracy: 0.4590 - val_loss: 1.0313 - val_accuracy: 0.4543 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0274 - accuracy: 0.4582\n",
      "Epoch 137: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0274 - accuracy: 0.4582 - val_loss: 1.0436 - val_accuracy: 0.4421 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0271 - accuracy: 0.4605\n",
      "Epoch 138: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0271 - accuracy: 0.4605 - val_loss: 1.0257 - val_accuracy: 0.4642 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0275 - accuracy: 0.4578\n",
      "Epoch 139: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0275 - accuracy: 0.4579 - val_loss: 1.0291 - val_accuracy: 0.4546 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0273 - accuracy: 0.4611\n",
      "Epoch 140: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0273 - accuracy: 0.4612 - val_loss: 1.0384 - val_accuracy: 0.4443 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0277 - accuracy: 0.4590\n",
      "Epoch 141: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0277 - accuracy: 0.4590 - val_loss: 1.0341 - val_accuracy: 0.4563 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0269 - accuracy: 0.4595\n",
      "Epoch 142: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0269 - accuracy: 0.4595 - val_loss: 1.0239 - val_accuracy: 0.4629 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0275 - accuracy: 0.4591\n",
      "Epoch 143: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0275 - accuracy: 0.4591 - val_loss: 1.0268 - val_accuracy: 0.4644 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4595\n",
      "Epoch 144: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0270 - accuracy: 0.4595 - val_loss: 1.0435 - val_accuracy: 0.4537 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0271 - accuracy: 0.4593\n",
      "Epoch 145: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0271 - accuracy: 0.4593 - val_loss: 1.0319 - val_accuracy: 0.4519 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0267 - accuracy: 0.4616\n",
      "Epoch 146: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0267 - accuracy: 0.4616 - val_loss: 1.0328 - val_accuracy: 0.4555 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.4603\n",
      "Epoch 147: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0264 - accuracy: 0.4603 - val_loss: 1.0283 - val_accuracy: 0.4563 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.4607\n",
      "Epoch 148: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 104s 89ms/step - loss: 1.0264 - accuracy: 0.4607 - val_loss: 1.0267 - val_accuracy: 0.4589 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0265 - accuracy: 0.4603\n",
      "Epoch 149: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0265 - accuracy: 0.4603 - val_loss: 1.0316 - val_accuracy: 0.4503 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0269 - accuracy: 0.4604\n",
      "Epoch 150: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0269 - accuracy: 0.4604 - val_loss: 1.0264 - val_accuracy: 0.4625 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4599\n",
      "Epoch 151: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0263 - accuracy: 0.4599 - val_loss: 1.0252 - val_accuracy: 0.4644 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4569\n",
      "Epoch 152: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0287 - accuracy: 0.4569 - val_loss: 1.0302 - val_accuracy: 0.4568 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0275 - accuracy: 0.4592\n",
      "Epoch 153: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0275 - accuracy: 0.4592 - val_loss: 1.0295 - val_accuracy: 0.4567 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0279 - accuracy: 0.4578\n",
      "Epoch 154: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0279 - accuracy: 0.4578 - val_loss: 1.0251 - val_accuracy: 0.4647 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0267 - accuracy: 0.4590\n",
      "Epoch 155: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0267 - accuracy: 0.4590 - val_loss: 1.0278 - val_accuracy: 0.4554 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.4602\n",
      "Epoch 156: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0264 - accuracy: 0.4602 - val_loss: 1.0273 - val_accuracy: 0.4621 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.4592\n",
      "Epoch 157: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0262 - accuracy: 0.4592 - val_loss: 1.0237 - val_accuracy: 0.4628 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0271 - accuracy: 0.4614\n",
      "Epoch 158: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0271 - accuracy: 0.4614 - val_loss: 1.0320 - val_accuracy: 0.4535 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0280 - accuracy: 0.4600\n",
      "Epoch 159: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0280 - accuracy: 0.4600 - val_loss: 1.0222 - val_accuracy: 0.4712 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.4606\n",
      "Epoch 160: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0262 - accuracy: 0.4606 - val_loss: 1.0295 - val_accuracy: 0.4573 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0269 - accuracy: 0.4620\n",
      "Epoch 161: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0269 - accuracy: 0.4620 - val_loss: 1.0273 - val_accuracy: 0.4624 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4615\n",
      "Epoch 162: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0263 - accuracy: 0.4615 - val_loss: 1.0244 - val_accuracy: 0.4652 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4600\n",
      "Epoch 163: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0263 - accuracy: 0.4600 - val_loss: 1.0372 - val_accuracy: 0.4459 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4612\n",
      "Epoch 164: val_loss did not improve from 1.02191\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0263 - accuracy: 0.4612 - val_loss: 1.0345 - val_accuracy: 0.4564 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.4617\n",
      "Epoch 165: val_loss improved from 1.02191 to 1.02147, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0260 - accuracy: 0.4617 - val_loss: 1.0215 - val_accuracy: 0.4679 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0268 - accuracy: 0.4599\n",
      "Epoch 166: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0268 - accuracy: 0.4599 - val_loss: 1.0275 - val_accuracy: 0.4566 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.4607\n",
      "Epoch 167: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0262 - accuracy: 0.4607 - val_loss: 1.0248 - val_accuracy: 0.4625 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0268 - accuracy: 0.4609\n",
      "Epoch 168: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0268 - accuracy: 0.4609 - val_loss: 1.0359 - val_accuracy: 0.4457 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0266 - accuracy: 0.4621\n",
      "Epoch 169: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0266 - accuracy: 0.4621 - val_loss: 1.0292 - val_accuracy: 0.4561 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0268 - accuracy: 0.4615\n",
      "Epoch 170: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0268 - accuracy: 0.4615 - val_loss: 1.0249 - val_accuracy: 0.4635 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.4611\n",
      "Epoch 171: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0260 - accuracy: 0.4611 - val_loss: 1.0308 - val_accuracy: 0.4588 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4615\n",
      "Epoch 172: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0259 - accuracy: 0.4615 - val_loss: 1.0278 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4623\n",
      "Epoch 173: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0259 - accuracy: 0.4623 - val_loss: 1.0254 - val_accuracy: 0.4618 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.4609\n",
      "Epoch 174: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0256 - accuracy: 0.4609 - val_loss: 1.0233 - val_accuracy: 0.4638 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4608\n",
      "Epoch 175: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0259 - accuracy: 0.4608 - val_loss: 1.0255 - val_accuracy: 0.4586 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.4605\n",
      "Epoch 176: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0262 - accuracy: 0.4605 - val_loss: 1.0431 - val_accuracy: 0.4510 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4632\n",
      "Epoch 177: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0259 - accuracy: 0.4632 - val_loss: 1.0313 - val_accuracy: 0.4587 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0267 - accuracy: 0.4602\n",
      "Epoch 178: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0267 - accuracy: 0.4602 - val_loss: 1.0278 - val_accuracy: 0.4558 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.4628\n",
      "Epoch 179: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0261 - accuracy: 0.4628 - val_loss: 1.0301 - val_accuracy: 0.4522 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.4616\n",
      "Epoch 180: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0264 - accuracy: 0.4615 - val_loss: 1.0294 - val_accuracy: 0.4594 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4623\n",
      "Epoch 181: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0263 - accuracy: 0.4623 - val_loss: 1.0267 - val_accuracy: 0.4646 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.4617\n",
      "Epoch 182: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0260 - accuracy: 0.4616 - val_loss: 1.0304 - val_accuracy: 0.4536 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0257 - accuracy: 0.4621\n",
      "Epoch 183: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0257 - accuracy: 0.4621 - val_loss: 1.0271 - val_accuracy: 0.4609 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4618\n",
      "Epoch 184: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0253 - accuracy: 0.4618 - val_loss: 1.0286 - val_accuracy: 0.4581 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0257 - accuracy: 0.4629\n",
      "Epoch 185: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0257 - accuracy: 0.4629 - val_loss: 1.0303 - val_accuracy: 0.4567 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0257 - accuracy: 0.4611\n",
      "Epoch 186: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0257 - accuracy: 0.4610 - val_loss: 1.0288 - val_accuracy: 0.4602 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.4610\n",
      "Epoch 187: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0256 - accuracy: 0.4610 - val_loss: 1.0263 - val_accuracy: 0.4586 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4626\n",
      "Epoch 188: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0270 - accuracy: 0.4626 - val_loss: 1.0275 - val_accuracy: 0.4581 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.4619\n",
      "Epoch 189: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0256 - accuracy: 0.4618 - val_loss: 1.0286 - val_accuracy: 0.4598 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.4610\n",
      "Epoch 190: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0256 - accuracy: 0.4610 - val_loss: 1.0262 - val_accuracy: 0.4617 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0261 - accuracy: 0.4620\n",
      "Epoch 191: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0261 - accuracy: 0.4620 - val_loss: 1.0294 - val_accuracy: 0.4625 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0254 - accuracy: 0.4629\n",
      "Epoch 192: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0254 - accuracy: 0.4629 - val_loss: 1.0306 - val_accuracy: 0.4535 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.4626\n",
      "Epoch 193: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0260 - accuracy: 0.4626 - val_loss: 1.0346 - val_accuracy: 0.4498 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.4618\n",
      "Epoch 194: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0260 - accuracy: 0.4618 - val_loss: 1.0316 - val_accuracy: 0.4499 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0257 - accuracy: 0.4641\n",
      "Epoch 195: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0257 - accuracy: 0.4642 - val_loss: 1.0260 - val_accuracy: 0.4636 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0254 - accuracy: 0.4629\n",
      "Epoch 196: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0254 - accuracy: 0.4629 - val_loss: 1.0319 - val_accuracy: 0.4641 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4633\n",
      "Epoch 197: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0251 - accuracy: 0.4633 - val_loss: 1.0241 - val_accuracy: 0.4616 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4627\n",
      "Epoch 198: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0251 - accuracy: 0.4627 - val_loss: 1.0282 - val_accuracy: 0.4592 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0252 - accuracy: 0.4637\n",
      "Epoch 199: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0252 - accuracy: 0.4637 - val_loss: 1.0240 - val_accuracy: 0.4628 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4627\n",
      "Epoch 200: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0253 - accuracy: 0.4627 - val_loss: 1.0270 - val_accuracy: 0.4607 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.4625\n",
      "Epoch 201: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0256 - accuracy: 0.4625 - val_loss: 1.0223 - val_accuracy: 0.4652 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4626\n",
      "Epoch 202: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0253 - accuracy: 0.4627 - val_loss: 1.0242 - val_accuracy: 0.4657 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4594\n",
      "Epoch 203: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0282 - accuracy: 0.4594 - val_loss: 1.0229 - val_accuracy: 0.4659 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0275 - accuracy: 0.4608\n",
      "Epoch 204: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0275 - accuracy: 0.4607 - val_loss: 1.0265 - val_accuracy: 0.4632 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.4619\n",
      "Epoch 205: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0264 - accuracy: 0.4619 - val_loss: 1.0294 - val_accuracy: 0.4607 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0258 - accuracy: 0.4605\n",
      "Epoch 206: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0258 - accuracy: 0.4605 - val_loss: 1.0294 - val_accuracy: 0.4562 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.4628\n",
      "Epoch 207: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0256 - accuracy: 0.4628 - val_loss: 1.0268 - val_accuracy: 0.4580 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0252 - accuracy: 0.4622\n",
      "Epoch 208: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0252 - accuracy: 0.4622 - val_loss: 1.0244 - val_accuracy: 0.4663 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.4620\n",
      "Epoch 209: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0264 - accuracy: 0.4620 - val_loss: 1.0226 - val_accuracy: 0.4668 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0261 - accuracy: 0.4627\n",
      "Epoch 210: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0261 - accuracy: 0.4627 - val_loss: 1.0265 - val_accuracy: 0.4631 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.4623\n",
      "Epoch 211: val_loss did not improve from 1.02147\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0256 - accuracy: 0.4624 - val_loss: 1.0253 - val_accuracy: 0.4663 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4628\n",
      "Epoch 212: val_loss improved from 1.02147 to 1.02131, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0253 - accuracy: 0.4628 - val_loss: 1.0213 - val_accuracy: 0.4678 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0254 - accuracy: 0.4615\n",
      "Epoch 213: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0254 - accuracy: 0.4616 - val_loss: 1.0265 - val_accuracy: 0.4638 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4631\n",
      "Epoch 214: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0253 - accuracy: 0.4631 - val_loss: 1.0343 - val_accuracy: 0.4575 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4566\n",
      "Epoch 215: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0313 - accuracy: 0.4566 - val_loss: 1.0258 - val_accuracy: 0.4640 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4614\n",
      "Epoch 216: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0270 - accuracy: 0.4614 - val_loss: 1.0312 - val_accuracy: 0.4526 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.4608\n",
      "Epoch 217: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0264 - accuracy: 0.4608 - val_loss: 1.0248 - val_accuracy: 0.4651 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0258 - accuracy: 0.4619\n",
      "Epoch 218: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0257 - accuracy: 0.4619 - val_loss: 1.0378 - val_accuracy: 0.4450 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.4636\n",
      "Epoch 219: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0261 - accuracy: 0.4635 - val_loss: 1.0261 - val_accuracy: 0.4639 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0264 - accuracy: 0.4628\n",
      "Epoch 220: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0264 - accuracy: 0.4628 - val_loss: 1.0260 - val_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4626\n",
      "Epoch 221: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0263 - accuracy: 0.4626 - val_loss: 1.0242 - val_accuracy: 0.4622 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4607\n",
      "Epoch 222: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0270 - accuracy: 0.4607 - val_loss: 1.0233 - val_accuracy: 0.4639 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0258 - accuracy: 0.4633\n",
      "Epoch 223: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0257 - accuracy: 0.4633 - val_loss: 1.0441 - val_accuracy: 0.4468 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.4633\n",
      "Epoch 224: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0262 - accuracy: 0.4633 - val_loss: 1.0248 - val_accuracy: 0.4614 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.4620\n",
      "Epoch 225: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0262 - accuracy: 0.4620 - val_loss: 1.0221 - val_accuracy: 0.4685 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0257 - accuracy: 0.4630\n",
      "Epoch 226: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0257 - accuracy: 0.4630 - val_loss: 1.0300 - val_accuracy: 0.4529 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0249 - accuracy: 0.4631\n",
      "Epoch 227: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0249 - accuracy: 0.4631 - val_loss: 1.0321 - val_accuracy: 0.4562 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4639\n",
      "Epoch 228: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0253 - accuracy: 0.4639 - val_loss: 1.0305 - val_accuracy: 0.4524 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4632\n",
      "Epoch 229: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0259 - accuracy: 0.4632 - val_loss: 1.0306 - val_accuracy: 0.4594 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4630\n",
      "Epoch 230: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0251 - accuracy: 0.4630 - val_loss: 1.0220 - val_accuracy: 0.4640 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0244 - accuracy: 0.4636\n",
      "Epoch 231: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0244 - accuracy: 0.4636 - val_loss: 1.0253 - val_accuracy: 0.4619 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4636\n",
      "Epoch 232: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0253 - accuracy: 0.4636 - val_loss: 1.0236 - val_accuracy: 0.4663 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0248 - accuracy: 0.4635\n",
      "Epoch 233: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0249 - accuracy: 0.4635 - val_loss: 1.0235 - val_accuracy: 0.4660 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4627\n",
      "Epoch 234: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0259 - accuracy: 0.4627 - val_loss: 1.0243 - val_accuracy: 0.4665 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4630\n",
      "Epoch 235: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0250 - accuracy: 0.4630 - val_loss: 1.0257 - val_accuracy: 0.4630 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0252 - accuracy: 0.4634\n",
      "Epoch 236: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0252 - accuracy: 0.4634 - val_loss: 1.0301 - val_accuracy: 0.4589 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0255 - accuracy: 0.4624\n",
      "Epoch 237: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0255 - accuracy: 0.4624 - val_loss: 1.0216 - val_accuracy: 0.4684 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4649\n",
      "Epoch 238: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0246 - accuracy: 0.4649 - val_loss: 1.0422 - val_accuracy: 0.4447 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0243 - accuracy: 0.4651\n",
      "Epoch 239: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0243 - accuracy: 0.4651 - val_loss: 1.0265 - val_accuracy: 0.4605 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4644\n",
      "Epoch 240: val_loss did not improve from 1.02131\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0251 - accuracy: 0.4644 - val_loss: 1.0286 - val_accuracy: 0.4621 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4626\n",
      "Epoch 241: val_loss improved from 1.02131 to 1.02039, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0251 - accuracy: 0.4626 - val_loss: 1.0204 - val_accuracy: 0.4693 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0248 - accuracy: 0.4634\n",
      "Epoch 242: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0247 - accuracy: 0.4635 - val_loss: 1.0229 - val_accuracy: 0.4633 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0254 - accuracy: 0.4636\n",
      "Epoch 243: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0254 - accuracy: 0.4636 - val_loss: 1.0259 - val_accuracy: 0.4641 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.4640\n",
      "Epoch 244: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0245 - accuracy: 0.4640 - val_loss: 1.0315 - val_accuracy: 0.4649 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4634\n",
      "Epoch 245: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0246 - accuracy: 0.4634 - val_loss: 1.0365 - val_accuracy: 0.4553 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0249 - accuracy: 0.4637\n",
      "Epoch 246: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0249 - accuracy: 0.4637 - val_loss: 1.0251 - val_accuracy: 0.4635 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4646\n",
      "Epoch 247: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0251 - accuracy: 0.4647 - val_loss: 1.0247 - val_accuracy: 0.4596 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4645\n",
      "Epoch 248: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0246 - accuracy: 0.4645 - val_loss: 1.0242 - val_accuracy: 0.4627 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0244 - accuracy: 0.4634\n",
      "Epoch 249: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0244 - accuracy: 0.4633 - val_loss: 1.0338 - val_accuracy: 0.4574 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0258 - accuracy: 0.4629\n",
      "Epoch 250: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0258 - accuracy: 0.4629 - val_loss: 1.0310 - val_accuracy: 0.4556 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0247 - accuracy: 0.4624\n",
      "Epoch 251: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0247 - accuracy: 0.4624 - val_loss: 1.0271 - val_accuracy: 0.4564 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4621\n",
      "Epoch 252: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0253 - accuracy: 0.4620 - val_loss: 1.0248 - val_accuracy: 0.4622 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4640\n",
      "Epoch 253: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0246 - accuracy: 0.4639 - val_loss: 1.0236 - val_accuracy: 0.4648 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0244 - accuracy: 0.4626\n",
      "Epoch 254: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0244 - accuracy: 0.4626 - val_loss: 1.0249 - val_accuracy: 0.4689 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4628\n",
      "Epoch 255: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0246 - accuracy: 0.4628 - val_loss: 1.0302 - val_accuracy: 0.4570 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.4635\n",
      "Epoch 256: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0245 - accuracy: 0.4635 - val_loss: 1.0222 - val_accuracy: 0.4681 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0243 - accuracy: 0.4639\n",
      "Epoch 257: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0243 - accuracy: 0.4639 - val_loss: 1.0218 - val_accuracy: 0.4694 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4631\n",
      "Epoch 258: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0250 - accuracy: 0.4631 - val_loss: 1.0240 - val_accuracy: 0.4630 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0255 - accuracy: 0.4630\n",
      "Epoch 259: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0255 - accuracy: 0.4630 - val_loss: 1.0219 - val_accuracy: 0.4675 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4643\n",
      "Epoch 260: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0251 - accuracy: 0.4643 - val_loss: 1.0326 - val_accuracy: 0.4486 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.4646\n",
      "Epoch 261: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0245 - accuracy: 0.4646 - val_loss: 1.0360 - val_accuracy: 0.4421 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4640\n",
      "Epoch 262: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0246 - accuracy: 0.4641 - val_loss: 1.0301 - val_accuracy: 0.4636 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4624\n",
      "Epoch 263: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0251 - accuracy: 0.4625 - val_loss: 1.0283 - val_accuracy: 0.4558 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4633\n",
      "Epoch 264: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0246 - accuracy: 0.4633 - val_loss: 1.0228 - val_accuracy: 0.4646 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4631\n",
      "Epoch 265: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0246 - accuracy: 0.4631 - val_loss: 1.0231 - val_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0248 - accuracy: 0.4655\n",
      "Epoch 266: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0247 - accuracy: 0.4655 - val_loss: 1.0302 - val_accuracy: 0.4562 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0252 - accuracy: 0.4631\n",
      "Epoch 267: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0252 - accuracy: 0.4632 - val_loss: 1.0332 - val_accuracy: 0.4513 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0248 - accuracy: 0.4632\n",
      "Epoch 268: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0248 - accuracy: 0.4632 - val_loss: 1.0221 - val_accuracy: 0.4684 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0247 - accuracy: 0.4645\n",
      "Epoch 269: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0247 - accuracy: 0.4645 - val_loss: 1.0381 - val_accuracy: 0.4585 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4641\n",
      "Epoch 270: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0250 - accuracy: 0.4641 - val_loss: 1.0288 - val_accuracy: 0.4663 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4622\n",
      "Epoch 271: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0250 - accuracy: 0.4622 - val_loss: 1.0235 - val_accuracy: 0.4654 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0243 - accuracy: 0.4636\n",
      "Epoch 272: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0243 - accuracy: 0.4636 - val_loss: 1.0265 - val_accuracy: 0.4680 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0249 - accuracy: 0.4629\n",
      "Epoch 273: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0248 - accuracy: 0.4629 - val_loss: 1.0338 - val_accuracy: 0.4536 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4630\n",
      "Epoch 274: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0253 - accuracy: 0.4630 - val_loss: 1.0287 - val_accuracy: 0.4512 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4631\n",
      "Epoch 275: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0250 - accuracy: 0.4632 - val_loss: 1.0260 - val_accuracy: 0.4603 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4644\n",
      "Epoch 276: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0250 - accuracy: 0.4644 - val_loss: 1.0353 - val_accuracy: 0.4566 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0248 - accuracy: 0.4635\n",
      "Epoch 277: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0248 - accuracy: 0.4635 - val_loss: 1.0338 - val_accuracy: 0.4515 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0242 - accuracy: 0.4648\n",
      "Epoch 278: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0242 - accuracy: 0.4648 - val_loss: 1.0238 - val_accuracy: 0.4649 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4642\n",
      "Epoch 279: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0252 - accuracy: 0.4642 - val_loss: 1.0307 - val_accuracy: 0.4579 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0243 - accuracy: 0.4651\n",
      "Epoch 280: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0243 - accuracy: 0.4651 - val_loss: 1.0443 - val_accuracy: 0.4552 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0255 - accuracy: 0.4626\n",
      "Epoch 281: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0255 - accuracy: 0.4626 - val_loss: 1.0270 - val_accuracy: 0.4612 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0243 - accuracy: 0.4646\n",
      "Epoch 282: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0243 - accuracy: 0.4646 - val_loss: 1.0252 - val_accuracy: 0.4676 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0248 - accuracy: 0.4643\n",
      "Epoch 283: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0249 - accuracy: 0.4643 - val_loss: 1.0291 - val_accuracy: 0.4592 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0261 - accuracy: 0.4621\n",
      "Epoch 284: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0261 - accuracy: 0.4621 - val_loss: 1.0205 - val_accuracy: 0.4671 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4659\n",
      "Epoch 285: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0246 - accuracy: 0.4659 - val_loss: 1.0348 - val_accuracy: 0.4634 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0247 - accuracy: 0.4634\n",
      "Epoch 286: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0247 - accuracy: 0.4634 - val_loss: 1.0239 - val_accuracy: 0.4683 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0240 - accuracy: 0.4642\n",
      "Epoch 287: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0240 - accuracy: 0.4642 - val_loss: 1.0269 - val_accuracy: 0.4613 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4624\n",
      "Epoch 288: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0253 - accuracy: 0.4624 - val_loss: 1.0319 - val_accuracy: 0.4551 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4630\n",
      "Epoch 289: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0249 - accuracy: 0.4630 - val_loss: 1.0272 - val_accuracy: 0.4640 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4636\n",
      "Epoch 290: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0250 - accuracy: 0.4636 - val_loss: 1.0273 - val_accuracy: 0.4599 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.4627\n",
      "Epoch 291: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0245 - accuracy: 0.4627 - val_loss: 1.0395 - val_accuracy: 0.4489 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.4666\n",
      "Epoch 292: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0222 - accuracy: 0.4666 - val_loss: 1.0274 - val_accuracy: 0.4644 - lr: 8.0000e-04\n",
      "Epoch 293/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0217 - accuracy: 0.4680\n",
      "Epoch 293: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0217 - accuracy: 0.4680 - val_loss: 1.0299 - val_accuracy: 0.4555 - lr: 8.0000e-04\n",
      "Epoch 294/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0217 - accuracy: 0.4676\n",
      "Epoch 294: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0217 - accuracy: 0.4676 - val_loss: 1.0221 - val_accuracy: 0.4691 - lr: 8.0000e-04\n",
      "Epoch 295/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0221 - accuracy: 0.4675\n",
      "Epoch 295: val_loss did not improve from 1.02039\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0221 - accuracy: 0.4675 - val_loss: 1.0236 - val_accuracy: 0.4594 - lr: 8.0000e-04\n",
      "Epoch 296/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0218 - accuracy: 0.4666\n",
      "Epoch 296: val_loss improved from 1.02039 to 1.02024, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0218 - accuracy: 0.4666 - val_loss: 1.0202 - val_accuracy: 0.4695 - lr: 8.0000e-04\n",
      "Epoch 297/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4685\n",
      "Epoch 297: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0219 - accuracy: 0.4685 - val_loss: 1.0226 - val_accuracy: 0.4679 - lr: 8.0000e-04\n",
      "Epoch 298/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4661\n",
      "Epoch 298: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0219 - accuracy: 0.4661 - val_loss: 1.0254 - val_accuracy: 0.4615 - lr: 8.0000e-04\n",
      "Epoch 299/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.4656\n",
      "Epoch 299: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0220 - accuracy: 0.4656 - val_loss: 1.0310 - val_accuracy: 0.4588 - lr: 8.0000e-04\n",
      "Epoch 300/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0225 - accuracy: 0.4659\n",
      "Epoch 300: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0225 - accuracy: 0.4659 - val_loss: 1.0280 - val_accuracy: 0.4578 - lr: 8.0000e-04\n",
      "Epoch 301/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0229 - accuracy: 0.4663\n",
      "Epoch 301: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0229 - accuracy: 0.4663 - val_loss: 1.0244 - val_accuracy: 0.4653 - lr: 8.0000e-04\n",
      "Epoch 302/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.4653\n",
      "Epoch 302: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0222 - accuracy: 0.4653 - val_loss: 1.0205 - val_accuracy: 0.4687 - lr: 8.0000e-04\n",
      "Epoch 303/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.4672\n",
      "Epoch 303: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0222 - accuracy: 0.4672 - val_loss: 1.0232 - val_accuracy: 0.4688 - lr: 8.0000e-04\n",
      "Epoch 304/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4659\n",
      "Epoch 304: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0227 - accuracy: 0.4659 - val_loss: 1.0203 - val_accuracy: 0.4702 - lr: 8.0000e-04\n",
      "Epoch 305/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0234 - accuracy: 0.4667\n",
      "Epoch 305: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0234 - accuracy: 0.4667 - val_loss: 1.0236 - val_accuracy: 0.4645 - lr: 8.0000e-04\n",
      "Epoch 306/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0228 - accuracy: 0.4659\n",
      "Epoch 306: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0228 - accuracy: 0.4659 - val_loss: 1.0318 - val_accuracy: 0.4494 - lr: 8.0000e-04\n",
      "Epoch 307/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.4667\n",
      "Epoch 307: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0223 - accuracy: 0.4667 - val_loss: 1.0249 - val_accuracy: 0.4650 - lr: 8.0000e-04\n",
      "Epoch 308/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.4678\n",
      "Epoch 308: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0221 - accuracy: 0.4677 - val_loss: 1.0260 - val_accuracy: 0.4622 - lr: 8.0000e-04\n",
      "Epoch 309/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0224 - accuracy: 0.4670\n",
      "Epoch 309: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 104s 89ms/step - loss: 1.0224 - accuracy: 0.4670 - val_loss: 1.0252 - val_accuracy: 0.4592 - lr: 8.0000e-04\n",
      "Epoch 310/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0224 - accuracy: 0.4659\n",
      "Epoch 310: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 104s 89ms/step - loss: 1.0224 - accuracy: 0.4659 - val_loss: 1.0268 - val_accuracy: 0.4659 - lr: 8.0000e-04\n",
      "Epoch 311/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0221 - accuracy: 0.4676\n",
      "Epoch 311: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0221 - accuracy: 0.4676 - val_loss: 1.0298 - val_accuracy: 0.4665 - lr: 8.0000e-04\n",
      "Epoch 312/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0229 - accuracy: 0.4659\n",
      "Epoch 312: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0229 - accuracy: 0.4659 - val_loss: 1.0289 - val_accuracy: 0.4576 - lr: 8.0000e-04\n",
      "Epoch 313/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4657\n",
      "Epoch 313: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0227 - accuracy: 0.4657 - val_loss: 1.0228 - val_accuracy: 0.4640 - lr: 8.0000e-04\n",
      "Epoch 314/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0240 - accuracy: 0.4653\n",
      "Epoch 314: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0240 - accuracy: 0.4653 - val_loss: 1.0269 - val_accuracy: 0.4630 - lr: 8.0000e-04\n",
      "Epoch 315/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0228 - accuracy: 0.4661\n",
      "Epoch 315: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0228 - accuracy: 0.4661 - val_loss: 1.0323 - val_accuracy: 0.4572 - lr: 8.0000e-04\n",
      "Epoch 316/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.4672\n",
      "Epoch 316: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0223 - accuracy: 0.4672 - val_loss: 1.0290 - val_accuracy: 0.4589 - lr: 8.0000e-04\n",
      "Epoch 317/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4663\n",
      "Epoch 317: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0227 - accuracy: 0.4663 - val_loss: 1.0554 - val_accuracy: 0.4370 - lr: 8.0000e-04\n",
      "Epoch 318/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4662\n",
      "Epoch 318: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0227 - accuracy: 0.4662 - val_loss: 1.0391 - val_accuracy: 0.4507 - lr: 8.0000e-04\n",
      "Epoch 319/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0238 - accuracy: 0.4643\n",
      "Epoch 319: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0238 - accuracy: 0.4643 - val_loss: 1.0251 - val_accuracy: 0.4632 - lr: 8.0000e-04\n",
      "Epoch 320/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4656\n",
      "Epoch 320: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0227 - accuracy: 0.4655 - val_loss: 1.0295 - val_accuracy: 0.4586 - lr: 8.0000e-04\n",
      "Epoch 321/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0233 - accuracy: 0.4651\n",
      "Epoch 321: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0233 - accuracy: 0.4650 - val_loss: 1.0262 - val_accuracy: 0.4630 - lr: 8.0000e-04\n",
      "Epoch 322/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0225 - accuracy: 0.4676\n",
      "Epoch 322: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0225 - accuracy: 0.4676 - val_loss: 1.0317 - val_accuracy: 0.4572 - lr: 8.0000e-04\n",
      "Epoch 323/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0228 - accuracy: 0.4664\n",
      "Epoch 323: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0228 - accuracy: 0.4664 - val_loss: 1.0296 - val_accuracy: 0.4646 - lr: 8.0000e-04\n",
      "Epoch 324/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0229 - accuracy: 0.4667\n",
      "Epoch 324: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 90ms/step - loss: 1.0229 - accuracy: 0.4667 - val_loss: 1.0504 - val_accuracy: 0.4395 - lr: 8.0000e-04\n",
      "Epoch 325/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4659\n",
      "Epoch 325: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0219 - accuracy: 0.4659 - val_loss: 1.0223 - val_accuracy: 0.4678 - lr: 8.0000e-04\n",
      "Epoch 326/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0216 - accuracy: 0.4674\n",
      "Epoch 326: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0216 - accuracy: 0.4674 - val_loss: 1.0256 - val_accuracy: 0.4630 - lr: 8.0000e-04\n",
      "Epoch 327/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4670\n",
      "Epoch 327: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0219 - accuracy: 0.4671 - val_loss: 1.0231 - val_accuracy: 0.4674 - lr: 8.0000e-04\n",
      "Epoch 328/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.4660\n",
      "Epoch 328: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0223 - accuracy: 0.4660 - val_loss: 1.0292 - val_accuracy: 0.4558 - lr: 8.0000e-04\n",
      "Epoch 329/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4668\n",
      "Epoch 329: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0218 - accuracy: 0.4668 - val_loss: 1.0320 - val_accuracy: 0.4558 - lr: 8.0000e-04\n",
      "Epoch 330/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0217 - accuracy: 0.4655\n",
      "Epoch 330: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0218 - accuracy: 0.4655 - val_loss: 1.0272 - val_accuracy: 0.4616 - lr: 8.0000e-04\n",
      "Epoch 331/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0228 - accuracy: 0.4658\n",
      "Epoch 331: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0228 - accuracy: 0.4658 - val_loss: 1.0431 - val_accuracy: 0.4398 - lr: 8.0000e-04\n",
      "Epoch 332/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.4660\n",
      "Epoch 332: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 105s 90ms/step - loss: 1.0222 - accuracy: 0.4660 - val_loss: 1.0256 - val_accuracy: 0.4662 - lr: 8.0000e-04\n",
      "Epoch 333/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.4653\n",
      "Epoch 333: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0223 - accuracy: 0.4654 - val_loss: 1.0205 - val_accuracy: 0.4696 - lr: 8.0000e-04\n",
      "Epoch 334/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0217 - accuracy: 0.4680\n",
      "Epoch 334: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0217 - accuracy: 0.4680 - val_loss: 1.0289 - val_accuracy: 0.4555 - lr: 8.0000e-04\n",
      "Epoch 335/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0225 - accuracy: 0.4658\n",
      "Epoch 335: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 68ms/step - loss: 1.0225 - accuracy: 0.4658 - val_loss: 1.0246 - val_accuracy: 0.4655 - lr: 8.0000e-04\n",
      "Epoch 336/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0225 - accuracy: 0.4675\n",
      "Epoch 336: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0225 - accuracy: 0.4675 - val_loss: 1.0208 - val_accuracy: 0.4665 - lr: 8.0000e-04\n",
      "Epoch 337/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.4672\n",
      "Epoch 337: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0224 - accuracy: 0.4672 - val_loss: 1.0209 - val_accuracy: 0.4704 - lr: 8.0000e-04\n",
      "Epoch 338/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0214 - accuracy: 0.4690\n",
      "Epoch 338: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 79s 67ms/step - loss: 1.0214 - accuracy: 0.4690 - val_loss: 1.0231 - val_accuracy: 0.4683 - lr: 8.0000e-04\n",
      "Epoch 339/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0230 - accuracy: 0.4660\n",
      "Epoch 339: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0230 - accuracy: 0.4660 - val_loss: 1.0249 - val_accuracy: 0.4610 - lr: 8.0000e-04\n",
      "Epoch 340/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.4670\n",
      "Epoch 340: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 81s 69ms/step - loss: 1.0223 - accuracy: 0.4670 - val_loss: 1.0359 - val_accuracy: 0.4470 - lr: 8.0000e-04\n",
      "Epoch 341/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0226 - accuracy: 0.4658\n",
      "Epoch 341: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 81s 69ms/step - loss: 1.0226 - accuracy: 0.4659 - val_loss: 1.0312 - val_accuracy: 0.4574 - lr: 8.0000e-04\n",
      "Epoch 342/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4661\n",
      "Epoch 342: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 81s 69ms/step - loss: 1.0219 - accuracy: 0.4661 - val_loss: 1.0273 - val_accuracy: 0.4576 - lr: 8.0000e-04\n",
      "Epoch 343/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0224 - accuracy: 0.4655\n",
      "Epoch 343: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0224 - accuracy: 0.4655 - val_loss: 1.0373 - val_accuracy: 0.4501 - lr: 8.0000e-04\n",
      "Epoch 344/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0221 - accuracy: 0.4665\n",
      "Epoch 344: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0221 - accuracy: 0.4665 - val_loss: 1.0224 - val_accuracy: 0.4661 - lr: 8.0000e-04\n",
      "Epoch 345/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.4660\n",
      "Epoch 345: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0220 - accuracy: 0.4660 - val_loss: 1.0227 - val_accuracy: 0.4689 - lr: 8.0000e-04\n",
      "Epoch 346/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0221 - accuracy: 0.4676\n",
      "Epoch 346: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0221 - accuracy: 0.4676 - val_loss: 1.0246 - val_accuracy: 0.4637 - lr: 8.0000e-04\n",
      "Epoch 347/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0201 - accuracy: 0.4682\n",
      "Epoch 347: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0201 - accuracy: 0.4682 - val_loss: 1.0205 - val_accuracy: 0.4681 - lr: 6.4000e-04\n",
      "Epoch 348/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4704\n",
      "Epoch 348: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0199 - accuracy: 0.4704 - val_loss: 1.0208 - val_accuracy: 0.4653 - lr: 6.4000e-04\n",
      "Epoch 349/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.4695\n",
      "Epoch 349: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0196 - accuracy: 0.4695 - val_loss: 1.0363 - val_accuracy: 0.4561 - lr: 6.4000e-04\n",
      "Epoch 350/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0192 - accuracy: 0.4687\n",
      "Epoch 350: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0192 - accuracy: 0.4687 - val_loss: 1.0220 - val_accuracy: 0.4715 - lr: 6.4000e-04\n",
      "Epoch 351/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4688\n",
      "Epoch 351: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 81s 70ms/step - loss: 1.0199 - accuracy: 0.4687 - val_loss: 1.0293 - val_accuracy: 0.4587 - lr: 6.4000e-04\n",
      "Epoch 352/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0192 - accuracy: 0.4712\n",
      "Epoch 352: val_loss did not improve from 1.02024\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0192 - accuracy: 0.4712 - val_loss: 1.0364 - val_accuracy: 0.4494 - lr: 6.4000e-04\n",
      "Epoch 353/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.4698\n",
      "Epoch 353: val_loss improved from 1.02024 to 1.01934, saving model to best_model_17.2.h5\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0196 - accuracy: 0.4699 - val_loss: 1.0193 - val_accuracy: 0.4706 - lr: 6.4000e-04\n",
      "Epoch 354/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4697\n",
      "Epoch 354: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0195 - accuracy: 0.4697 - val_loss: 1.0329 - val_accuracy: 0.4619 - lr: 6.4000e-04\n",
      "Epoch 355/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.4696\n",
      "Epoch 355: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0202 - accuracy: 0.4696 - val_loss: 1.0265 - val_accuracy: 0.4674 - lr: 6.4000e-04\n",
      "Epoch 356/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4703\n",
      "Epoch 356: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0198 - accuracy: 0.4703 - val_loss: 1.0329 - val_accuracy: 0.4588 - lr: 6.4000e-04\n",
      "Epoch 357/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4703\n",
      "Epoch 357: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 80s 68ms/step - loss: 1.0199 - accuracy: 0.4703 - val_loss: 1.0230 - val_accuracy: 0.4657 - lr: 6.4000e-04\n",
      "Epoch 358/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4705\n",
      "Epoch 358: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0199 - accuracy: 0.4704 - val_loss: 1.0249 - val_accuracy: 0.4607 - lr: 6.4000e-04\n",
      "Epoch 359/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0204 - accuracy: 0.4696\n",
      "Epoch 359: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0204 - accuracy: 0.4696 - val_loss: 1.0244 - val_accuracy: 0.4681 - lr: 6.4000e-04\n",
      "Epoch 360/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.4702\n",
      "Epoch 360: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0202 - accuracy: 0.4702 - val_loss: 1.0411 - val_accuracy: 0.4570 - lr: 6.4000e-04\n",
      "Epoch 361/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4706\n",
      "Epoch 361: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0199 - accuracy: 0.4706 - val_loss: 1.0303 - val_accuracy: 0.4606 - lr: 6.4000e-04\n",
      "Epoch 362/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4701\n",
      "Epoch 362: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0195 - accuracy: 0.4701 - val_loss: 1.0249 - val_accuracy: 0.4666 - lr: 6.4000e-04\n",
      "Epoch 363/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0197 - accuracy: 0.4706\n",
      "Epoch 363: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0197 - accuracy: 0.4706 - val_loss: 1.0221 - val_accuracy: 0.4661 - lr: 6.4000e-04\n",
      "Epoch 364/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4693\n",
      "Epoch 364: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0198 - accuracy: 0.4693 - val_loss: 1.0264 - val_accuracy: 0.4600 - lr: 6.4000e-04\n",
      "Epoch 365/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4703\n",
      "Epoch 365: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0199 - accuracy: 0.4704 - val_loss: 1.0244 - val_accuracy: 0.4627 - lr: 6.4000e-04\n",
      "Epoch 366/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4697\n",
      "Epoch 366: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 67ms/step - loss: 1.0195 - accuracy: 0.4697 - val_loss: 1.0211 - val_accuracy: 0.4685 - lr: 6.4000e-04\n",
      "Epoch 367/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4696\n",
      "Epoch 367: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0195 - accuracy: 0.4696 - val_loss: 1.0251 - val_accuracy: 0.4644 - lr: 6.4000e-04\n",
      "Epoch 368/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.4705\n",
      "Epoch 368: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0196 - accuracy: 0.4705 - val_loss: 1.0241 - val_accuracy: 0.4656 - lr: 6.4000e-04\n",
      "Epoch 369/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4688\n",
      "Epoch 369: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 67ms/step - loss: 1.0198 - accuracy: 0.4688 - val_loss: 1.0242 - val_accuracy: 0.4692 - lr: 6.4000e-04\n",
      "Epoch 370/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4706\n",
      "Epoch 370: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0195 - accuracy: 0.4705 - val_loss: 1.0217 - val_accuracy: 0.4692 - lr: 6.4000e-04\n",
      "Epoch 371/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0205 - accuracy: 0.4682\n",
      "Epoch 371: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0205 - accuracy: 0.4683 - val_loss: 1.0499 - val_accuracy: 0.4546 - lr: 6.4000e-04\n",
      "Epoch 372/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0204 - accuracy: 0.4696\n",
      "Epoch 372: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0204 - accuracy: 0.4697 - val_loss: 1.0244 - val_accuracy: 0.4699 - lr: 6.4000e-04\n",
      "Epoch 373/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.4697\n",
      "Epoch 373: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0196 - accuracy: 0.4697 - val_loss: 1.0230 - val_accuracy: 0.4663 - lr: 6.4000e-04\n",
      "Epoch 374/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4697\n",
      "Epoch 374: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0197 - accuracy: 0.4698 - val_loss: 1.0214 - val_accuracy: 0.4696 - lr: 6.4000e-04\n",
      "Epoch 375/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.4686\n",
      "Epoch 375: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0202 - accuracy: 0.4686 - val_loss: 1.0223 - val_accuracy: 0.4709 - lr: 6.4000e-04\n",
      "Epoch 376/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4689\n",
      "Epoch 376: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0198 - accuracy: 0.4688 - val_loss: 1.0268 - val_accuracy: 0.4650 - lr: 6.4000e-04\n",
      "Epoch 377/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.4692\n",
      "Epoch 377: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0196 - accuracy: 0.4692 - val_loss: 1.0227 - val_accuracy: 0.4656 - lr: 6.4000e-04\n",
      "Epoch 378/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0193 - accuracy: 0.4707\n",
      "Epoch 378: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0193 - accuracy: 0.4707 - val_loss: 1.0327 - val_accuracy: 0.4531 - lr: 6.4000e-04\n",
      "Epoch 379/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4694\n",
      "Epoch 379: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0195 - accuracy: 0.4694 - val_loss: 1.0207 - val_accuracy: 0.4671 - lr: 6.4000e-04\n",
      "Epoch 380/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0203 - accuracy: 0.4679\n",
      "Epoch 380: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0203 - accuracy: 0.4679 - val_loss: 1.0234 - val_accuracy: 0.4646 - lr: 6.4000e-04\n",
      "Epoch 381/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4695\n",
      "Epoch 381: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0199 - accuracy: 0.4695 - val_loss: 1.0194 - val_accuracy: 0.4694 - lr: 6.4000e-04\n",
      "Epoch 382/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0204 - accuracy: 0.4691\n",
      "Epoch 382: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0204 - accuracy: 0.4691 - val_loss: 1.0270 - val_accuracy: 0.4575 - lr: 6.4000e-04\n",
      "Epoch 383/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4688\n",
      "Epoch 383: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0195 - accuracy: 0.4687 - val_loss: 1.0221 - val_accuracy: 0.4702 - lr: 6.4000e-04\n",
      "Epoch 384/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.4679\n",
      "Epoch 384: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0201 - accuracy: 0.4679 - val_loss: 1.0258 - val_accuracy: 0.4654 - lr: 6.4000e-04\n",
      "Epoch 385/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.4709\n",
      "Epoch 385: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0200 - accuracy: 0.4709 - val_loss: 1.0217 - val_accuracy: 0.4696 - lr: 6.4000e-04\n",
      "Epoch 386/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0197 - accuracy: 0.4703\n",
      "Epoch 386: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0197 - accuracy: 0.4703 - val_loss: 1.0241 - val_accuracy: 0.4677 - lr: 6.4000e-04\n",
      "Epoch 387/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4692\n",
      "Epoch 387: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0199 - accuracy: 0.4692 - val_loss: 1.0290 - val_accuracy: 0.4630 - lr: 6.4000e-04\n",
      "Epoch 388/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0206 - accuracy: 0.4693\n",
      "Epoch 388: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0206 - accuracy: 0.4693 - val_loss: 1.0215 - val_accuracy: 0.4659 - lr: 6.4000e-04\n",
      "Epoch 389/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4691\n",
      "Epoch 389: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0189 - accuracy: 0.4690 - val_loss: 1.0269 - val_accuracy: 0.4626 - lr: 6.4000e-04\n",
      "Epoch 390/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0197 - accuracy: 0.4682\n",
      "Epoch 390: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0197 - accuracy: 0.4682 - val_loss: 1.0317 - val_accuracy: 0.4553 - lr: 6.4000e-04\n",
      "Epoch 391/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4688\n",
      "Epoch 391: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0198 - accuracy: 0.4688 - val_loss: 1.0306 - val_accuracy: 0.4628 - lr: 6.4000e-04\n",
      "Epoch 392/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4686\n",
      "Epoch 392: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0198 - accuracy: 0.4686 - val_loss: 1.0287 - val_accuracy: 0.4679 - lr: 6.4000e-04\n",
      "Epoch 393/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4689\n",
      "Epoch 393: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0195 - accuracy: 0.4689 - val_loss: 1.0236 - val_accuracy: 0.4680 - lr: 6.4000e-04\n",
      "Epoch 394/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0192 - accuracy: 0.4712\n",
      "Epoch 394: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0192 - accuracy: 0.4712 - val_loss: 1.0440 - val_accuracy: 0.4534 - lr: 6.4000e-04\n",
      "Epoch 395/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4699\n",
      "Epoch 395: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0195 - accuracy: 0.4699 - val_loss: 1.0271 - val_accuracy: 0.4645 - lr: 6.4000e-04\n",
      "Epoch 396/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0192 - accuracy: 0.4695\n",
      "Epoch 396: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0192 - accuracy: 0.4694 - val_loss: 1.0260 - val_accuracy: 0.4644 - lr: 6.4000e-04\n",
      "Epoch 397/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0197 - accuracy: 0.4689\n",
      "Epoch 397: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0197 - accuracy: 0.4689 - val_loss: 1.0348 - val_accuracy: 0.4541 - lr: 6.4000e-04\n",
      "Epoch 398/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.4692\n",
      "Epoch 398: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0200 - accuracy: 0.4692 - val_loss: 1.0242 - val_accuracy: 0.4689 - lr: 6.4000e-04\n",
      "Epoch 399/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0194 - accuracy: 0.4706\n",
      "Epoch 399: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0194 - accuracy: 0.4706 - val_loss: 1.0251 - val_accuracy: 0.4644 - lr: 6.4000e-04\n",
      "Epoch 400/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.4694\n",
      "Epoch 400: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0200 - accuracy: 0.4694 - val_loss: 1.0267 - val_accuracy: 0.4656 - lr: 6.4000e-04\n",
      "Epoch 401/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0207 - accuracy: 0.4687\n",
      "Epoch 401: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0208 - accuracy: 0.4687 - val_loss: 1.0485 - val_accuracy: 0.4484 - lr: 6.4000e-04\n",
      "Epoch 402/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4700\n",
      "Epoch 402: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 67ms/step - loss: 1.0198 - accuracy: 0.4700 - val_loss: 1.0331 - val_accuracy: 0.4562 - lr: 6.4000e-04\n",
      "Epoch 403/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0193 - accuracy: 0.4694\n",
      "Epoch 403: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0193 - accuracy: 0.4694 - val_loss: 1.0323 - val_accuracy: 0.4565 - lr: 6.4000e-04\n",
      "Epoch 404/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0174 - accuracy: 0.4721\n",
      "Epoch 404: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0174 - accuracy: 0.4722 - val_loss: 1.0235 - val_accuracy: 0.4659 - lr: 5.1200e-04\n",
      "Epoch 405/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0170 - accuracy: 0.4718\n",
      "Epoch 405: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0170 - accuracy: 0.4718 - val_loss: 1.0288 - val_accuracy: 0.4595 - lr: 5.1200e-04\n",
      "Epoch 406/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0178 - accuracy: 0.4713\n",
      "Epoch 406: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0178 - accuracy: 0.4713 - val_loss: 1.0253 - val_accuracy: 0.4663 - lr: 5.1200e-04\n",
      "Epoch 407/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0169 - accuracy: 0.4725\n",
      "Epoch 407: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 66ms/step - loss: 1.0169 - accuracy: 0.4726 - val_loss: 1.0312 - val_accuracy: 0.4696 - lr: 5.1200e-04\n",
      "Epoch 408/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0174 - accuracy: 0.4729\n",
      "Epoch 408: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0174 - accuracy: 0.4729 - val_loss: 1.0211 - val_accuracy: 0.4672 - lr: 5.1200e-04\n",
      "Epoch 409/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0171 - accuracy: 0.4719\n",
      "Epoch 409: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0171 - accuracy: 0.4719 - val_loss: 1.0364 - val_accuracy: 0.4647 - lr: 5.1200e-04\n",
      "Epoch 410/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0173 - accuracy: 0.4717\n",
      "Epoch 410: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0173 - accuracy: 0.4717 - val_loss: 1.0459 - val_accuracy: 0.4494 - lr: 5.1200e-04\n",
      "Epoch 411/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0185 - accuracy: 0.4685\n",
      "Epoch 411: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0185 - accuracy: 0.4685 - val_loss: 1.0218 - val_accuracy: 0.4671 - lr: 5.1200e-04\n",
      "Epoch 412/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0174 - accuracy: 0.4723\n",
      "Epoch 412: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0174 - accuracy: 0.4723 - val_loss: 1.0242 - val_accuracy: 0.4645 - lr: 5.1200e-04\n",
      "Epoch 413/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0177 - accuracy: 0.4721\n",
      "Epoch 413: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0177 - accuracy: 0.4720 - val_loss: 1.0278 - val_accuracy: 0.4661 - lr: 5.1200e-04\n",
      "Epoch 414/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0179 - accuracy: 0.4715\n",
      "Epoch 414: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0179 - accuracy: 0.4715 - val_loss: 1.0264 - val_accuracy: 0.4650 - lr: 5.1200e-04\n",
      "Epoch 415/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0173 - accuracy: 0.4711\n",
      "Epoch 415: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0173 - accuracy: 0.4711 - val_loss: 1.0277 - val_accuracy: 0.4652 - lr: 5.1200e-04\n",
      "Epoch 416/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0182 - accuracy: 0.4719\n",
      "Epoch 416: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0182 - accuracy: 0.4718 - val_loss: 1.0252 - val_accuracy: 0.4637 - lr: 5.1200e-04\n",
      "Epoch 417/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0181 - accuracy: 0.4718\n",
      "Epoch 417: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0181 - accuracy: 0.4718 - val_loss: 1.0271 - val_accuracy: 0.4677 - lr: 5.1200e-04\n",
      "Epoch 418/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0176 - accuracy: 0.4720\n",
      "Epoch 418: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0175 - accuracy: 0.4720 - val_loss: 1.0378 - val_accuracy: 0.4570 - lr: 5.1200e-04\n",
      "Epoch 419/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0175 - accuracy: 0.4720\n",
      "Epoch 419: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0175 - accuracy: 0.4720 - val_loss: 1.0284 - val_accuracy: 0.4585 - lr: 5.1200e-04\n",
      "Epoch 420/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0177 - accuracy: 0.4717\n",
      "Epoch 420: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 82s 70ms/step - loss: 1.0176 - accuracy: 0.4717 - val_loss: 1.0200 - val_accuracy: 0.4711 - lr: 5.1200e-04\n",
      "Epoch 421/1000\n",
      "1167/1167 [==============================] - ETA: 0s - loss: 1.0171 - accuracy: 0.4729\n",
      "Epoch 421: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 231s 198ms/step - loss: 1.0171 - accuracy: 0.4729 - val_loss: 1.0212 - val_accuracy: 0.4667 - lr: 5.1200e-04\n",
      "Epoch 422/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0178 - accuracy: 0.4727\n",
      "Epoch 422: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 422s 362ms/step - loss: 1.0178 - accuracy: 0.4727 - val_loss: 1.0397 - val_accuracy: 0.4553 - lr: 5.1200e-04\n",
      "Epoch 423/1000\n",
      "1167/1167 [==============================] - ETA: 0s - loss: 1.0182 - accuracy: 0.4705\n",
      "Epoch 423: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 337s 289ms/step - loss: 1.0182 - accuracy: 0.4705 - val_loss: 1.0248 - val_accuracy: 0.4679 - lr: 5.1200e-04\n",
      "Epoch 424/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0182 - accuracy: 0.4713\n",
      "Epoch 424: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 360s 309ms/step - loss: 1.0182 - accuracy: 0.4713 - val_loss: 1.0221 - val_accuracy: 0.4676 - lr: 5.1200e-04\n",
      "Epoch 425/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0184 - accuracy: 0.4721\n",
      "Epoch 425: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 271s 233ms/step - loss: 1.0184 - accuracy: 0.4721 - val_loss: 1.0386 - val_accuracy: 0.4528 - lr: 5.1200e-04\n",
      "Epoch 426/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0177 - accuracy: 0.4723\n",
      "Epoch 426: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 269s 231ms/step - loss: 1.0176 - accuracy: 0.4723 - val_loss: 1.0285 - val_accuracy: 0.4581 - lr: 5.1200e-04\n",
      "Epoch 427/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0186 - accuracy: 0.4708\n",
      "Epoch 427: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 267s 229ms/step - loss: 1.0186 - accuracy: 0.4707 - val_loss: 1.0293 - val_accuracy: 0.4632 - lr: 5.1200e-04\n",
      "Epoch 428/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0179 - accuracy: 0.4726\n",
      "Epoch 428: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 267s 229ms/step - loss: 1.0179 - accuracy: 0.4726 - val_loss: 1.0350 - val_accuracy: 0.4623 - lr: 5.1200e-04\n",
      "Epoch 429/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0175 - accuracy: 0.4720\n",
      "Epoch 429: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 267s 229ms/step - loss: 1.0175 - accuracy: 0.4720 - val_loss: 1.0246 - val_accuracy: 0.4619 - lr: 5.1200e-04\n",
      "Epoch 430/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0181 - accuracy: 0.4719\n",
      "Epoch 430: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 267s 229ms/step - loss: 1.0181 - accuracy: 0.4719 - val_loss: 1.0251 - val_accuracy: 0.4699 - lr: 5.1200e-04\n",
      "Epoch 431/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0183 - accuracy: 0.4713\n",
      "Epoch 431: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 270s 231ms/step - loss: 1.0183 - accuracy: 0.4713 - val_loss: 1.0390 - val_accuracy: 0.4610 - lr: 5.1200e-04\n",
      "Epoch 432/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0176 - accuracy: 0.4721\n",
      "Epoch 432: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 169s 144ms/step - loss: 1.0176 - accuracy: 0.4721 - val_loss: 1.0293 - val_accuracy: 0.4666 - lr: 5.1200e-04\n",
      "Epoch 433/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0177 - accuracy: 0.4727\n",
      "Epoch 433: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0176 - accuracy: 0.4727 - val_loss: 1.0246 - val_accuracy: 0.4639 - lr: 5.1200e-04\n",
      "Epoch 434/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0177 - accuracy: 0.4713\n",
      "Epoch 434: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0177 - accuracy: 0.4713 - val_loss: 1.0355 - val_accuracy: 0.4592 - lr: 5.1200e-04\n",
      "Epoch 435/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0187 - accuracy: 0.4707\n",
      "Epoch 435: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0187 - accuracy: 0.4707 - val_loss: 1.0267 - val_accuracy: 0.4662 - lr: 5.1200e-04\n",
      "Epoch 436/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0178 - accuracy: 0.4712\n",
      "Epoch 436: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0178 - accuracy: 0.4712 - val_loss: 1.0240 - val_accuracy: 0.4672 - lr: 5.1200e-04\n",
      "Epoch 437/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0181 - accuracy: 0.4694\n",
      "Epoch 437: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0181 - accuracy: 0.4694 - val_loss: 1.0217 - val_accuracy: 0.4670 - lr: 5.1200e-04\n",
      "Epoch 438/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0181 - accuracy: 0.4721\n",
      "Epoch 438: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0181 - accuracy: 0.4721 - val_loss: 1.0317 - val_accuracy: 0.4539 - lr: 5.1200e-04\n",
      "Epoch 439/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0185 - accuracy: 0.4705\n",
      "Epoch 439: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0185 - accuracy: 0.4705 - val_loss: 1.0314 - val_accuracy: 0.4629 - lr: 5.1200e-04\n",
      "Epoch 440/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0193 - accuracy: 0.4692\n",
      "Epoch 440: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0193 - accuracy: 0.4692 - val_loss: 1.0223 - val_accuracy: 0.4690 - lr: 5.1200e-04\n",
      "Epoch 441/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0186 - accuracy: 0.4724\n",
      "Epoch 441: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0186 - accuracy: 0.4724 - val_loss: 1.0224 - val_accuracy: 0.4659 - lr: 5.1200e-04\n",
      "Epoch 442/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4697\n",
      "Epoch 442: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0189 - accuracy: 0.4698 - val_loss: 1.0300 - val_accuracy: 0.4625 - lr: 5.1200e-04\n",
      "Epoch 443/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0184 - accuracy: 0.4715\n",
      "Epoch 443: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0184 - accuracy: 0.4715 - val_loss: 1.0264 - val_accuracy: 0.4661 - lr: 5.1200e-04\n",
      "Epoch 444/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0184 - accuracy: 0.4709\n",
      "Epoch 444: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0184 - accuracy: 0.4709 - val_loss: 1.0277 - val_accuracy: 0.4627 - lr: 5.1200e-04\n",
      "Epoch 445/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0185 - accuracy: 0.4710\n",
      "Epoch 445: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0185 - accuracy: 0.4710 - val_loss: 1.0239 - val_accuracy: 0.4696 - lr: 5.1200e-04\n",
      "Epoch 446/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0190 - accuracy: 0.4700\n",
      "Epoch 446: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 98s 84ms/step - loss: 1.0190 - accuracy: 0.4700 - val_loss: 1.0229 - val_accuracy: 0.4691 - lr: 5.1200e-04\n",
      "Epoch 447/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0183 - accuracy: 0.4698\n",
      "Epoch 447: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0183 - accuracy: 0.4698 - val_loss: 1.0237 - val_accuracy: 0.4683 - lr: 5.1200e-04\n",
      "Epoch 448/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4716\n",
      "Epoch 448: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0189 - accuracy: 0.4716 - val_loss: 1.0230 - val_accuracy: 0.4706 - lr: 5.1200e-04\n",
      "Epoch 449/1000\n",
      "1167/1167 [==============================] - ETA: 0s - loss: 1.0180 - accuracy: 0.4715\n",
      "Epoch 449: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 82s 70ms/step - loss: 1.0180 - accuracy: 0.4715 - val_loss: 1.0256 - val_accuracy: 0.4693 - lr: 5.1200e-04\n",
      "Epoch 450/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0187 - accuracy: 0.4722\n",
      "Epoch 450: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 119s 102ms/step - loss: 1.0187 - accuracy: 0.4721 - val_loss: 1.0216 - val_accuracy: 0.4690 - lr: 5.1200e-04\n",
      "Epoch 451/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0185 - accuracy: 0.4706\n",
      "Epoch 451: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0185 - accuracy: 0.4706 - val_loss: 1.0247 - val_accuracy: 0.4681 - lr: 5.1200e-04\n",
      "Epoch 452/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0190 - accuracy: 0.4697\n",
      "Epoch 452: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0190 - accuracy: 0.4697 - val_loss: 1.0326 - val_accuracy: 0.4640 - lr: 5.1200e-04\n",
      "Epoch 453/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4694\n",
      "Epoch 453: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0195 - accuracy: 0.4694 - val_loss: 1.0231 - val_accuracy: 0.4650 - lr: 5.1200e-04\n",
      "Epoch 454/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0163 - accuracy: 0.4728\n",
      "Epoch 454: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0164 - accuracy: 0.4728 - val_loss: 1.0233 - val_accuracy: 0.4674 - lr: 4.0960e-04\n",
      "Epoch 455/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0170 - accuracy: 0.4725\n",
      "Epoch 455: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0170 - accuracy: 0.4725 - val_loss: 1.0254 - val_accuracy: 0.4630 - lr: 4.0960e-04\n",
      "Epoch 456/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0170 - accuracy: 0.4720\n",
      "Epoch 456: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 80s 68ms/step - loss: 1.0171 - accuracy: 0.4720 - val_loss: 1.0237 - val_accuracy: 0.4670 - lr: 4.0960e-04\n",
      "Epoch 457/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0169 - accuracy: 0.4729\n",
      "Epoch 457: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 131s 113ms/step - loss: 1.0169 - accuracy: 0.4729 - val_loss: 1.0233 - val_accuracy: 0.4683 - lr: 4.0960e-04\n",
      "Epoch 458/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0166 - accuracy: 0.4729\n",
      "Epoch 458: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 145s 125ms/step - loss: 1.0166 - accuracy: 0.4729 - val_loss: 1.0230 - val_accuracy: 0.4649 - lr: 4.0960e-04\n",
      "Epoch 459/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0172 - accuracy: 0.4723\n",
      "Epoch 459: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 139s 119ms/step - loss: 1.0172 - accuracy: 0.4724 - val_loss: 1.0225 - val_accuracy: 0.4667 - lr: 4.0960e-04\n",
      "Epoch 460/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0170 - accuracy: 0.4729\n",
      "Epoch 460: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 100s 85ms/step - loss: 1.0170 - accuracy: 0.4729 - val_loss: 1.0245 - val_accuracy: 0.4676 - lr: 4.0960e-04\n",
      "Epoch 461/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0169 - accuracy: 0.4720\n",
      "Epoch 461: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0170 - accuracy: 0.4720 - val_loss: 1.0306 - val_accuracy: 0.4625 - lr: 4.0960e-04\n",
      "Epoch 462/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0183 - accuracy: 0.4711\n",
      "Epoch 462: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0183 - accuracy: 0.4711 - val_loss: 1.0372 - val_accuracy: 0.4540 - lr: 4.0960e-04\n",
      "Epoch 463/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0175 - accuracy: 0.4717\n",
      "Epoch 463: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0175 - accuracy: 0.4717 - val_loss: 1.0224 - val_accuracy: 0.4676 - lr: 4.0960e-04\n",
      "Epoch 464/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0180 - accuracy: 0.4716\n",
      "Epoch 464: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0180 - accuracy: 0.4716 - val_loss: 1.0243 - val_accuracy: 0.4687 - lr: 4.0960e-04\n",
      "Epoch 465/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0174 - accuracy: 0.4726\n",
      "Epoch 465: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0175 - accuracy: 0.4726 - val_loss: 1.0310 - val_accuracy: 0.4637 - lr: 4.0960e-04\n",
      "Epoch 466/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0182 - accuracy: 0.4714\n",
      "Epoch 466: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0182 - accuracy: 0.4714 - val_loss: 1.0216 - val_accuracy: 0.4697 - lr: 4.0960e-04\n",
      "Epoch 467/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0177 - accuracy: 0.4710\n",
      "Epoch 467: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0178 - accuracy: 0.4710 - val_loss: 1.0227 - val_accuracy: 0.4691 - lr: 4.0960e-04\n",
      "Epoch 468/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0187 - accuracy: 0.4723\n",
      "Epoch 468: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0187 - accuracy: 0.4723 - val_loss: 1.0356 - val_accuracy: 0.4557 - lr: 4.0960e-04\n",
      "Epoch 469/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0182 - accuracy: 0.4703\n",
      "Epoch 469: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 67ms/step - loss: 1.0182 - accuracy: 0.4703 - val_loss: 1.0300 - val_accuracy: 0.4569 - lr: 4.0960e-04\n",
      "Epoch 470/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0180 - accuracy: 0.4719\n",
      "Epoch 470: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0180 - accuracy: 0.4719 - val_loss: 1.0319 - val_accuracy: 0.4635 - lr: 4.0960e-04\n",
      "Epoch 471/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0182 - accuracy: 0.4715\n",
      "Epoch 471: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0182 - accuracy: 0.4715 - val_loss: 1.0285 - val_accuracy: 0.4649 - lr: 4.0960e-04\n",
      "Epoch 472/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0178 - accuracy: 0.4712\n",
      "Epoch 472: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0178 - accuracy: 0.4712 - val_loss: 1.0286 - val_accuracy: 0.4660 - lr: 4.0960e-04\n",
      "Epoch 473/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4700\n",
      "Epoch 473: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0189 - accuracy: 0.4701 - val_loss: 1.0227 - val_accuracy: 0.4672 - lr: 4.0960e-04\n",
      "Epoch 474/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0184 - accuracy: 0.4712\n",
      "Epoch 474: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 80s 68ms/step - loss: 1.0184 - accuracy: 0.4712 - val_loss: 1.0317 - val_accuracy: 0.4590 - lr: 4.0960e-04\n",
      "Epoch 475/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0188 - accuracy: 0.4706\n",
      "Epoch 475: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0188 - accuracy: 0.4706 - val_loss: 1.0252 - val_accuracy: 0.4640 - lr: 4.0960e-04\n",
      "Epoch 476/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4707\n",
      "Epoch 476: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 67ms/step - loss: 1.0189 - accuracy: 0.4706 - val_loss: 1.0235 - val_accuracy: 0.4637 - lr: 4.0960e-04\n",
      "Epoch 477/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0185 - accuracy: 0.4705\n",
      "Epoch 477: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0185 - accuracy: 0.4705 - val_loss: 1.0280 - val_accuracy: 0.4618 - lr: 4.0960e-04\n",
      "Epoch 478/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0188 - accuracy: 0.4696\n",
      "Epoch 478: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0187 - accuracy: 0.4696 - val_loss: 1.0416 - val_accuracy: 0.4549 - lr: 4.0960e-04\n",
      "Epoch 479/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0190 - accuracy: 0.4701\n",
      "Epoch 479: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0191 - accuracy: 0.4701 - val_loss: 1.0269 - val_accuracy: 0.4602 - lr: 4.0960e-04\n",
      "Epoch 480/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0188 - accuracy: 0.4709\n",
      "Epoch 480: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0188 - accuracy: 0.4710 - val_loss: 1.0259 - val_accuracy: 0.4659 - lr: 4.0960e-04\n",
      "Epoch 481/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0190 - accuracy: 0.4709\n",
      "Epoch 481: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0190 - accuracy: 0.4709 - val_loss: 1.0332 - val_accuracy: 0.4570 - lr: 4.0960e-04\n",
      "Epoch 482/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0191 - accuracy: 0.4694\n",
      "Epoch 482: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0191 - accuracy: 0.4693 - val_loss: 1.0283 - val_accuracy: 0.4631 - lr: 4.0960e-04\n",
      "Epoch 483/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0194 - accuracy: 0.4693\n",
      "Epoch 483: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0194 - accuracy: 0.4693 - val_loss: 1.0239 - val_accuracy: 0.4660 - lr: 4.0960e-04\n",
      "Epoch 484/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0190 - accuracy: 0.4699\n",
      "Epoch 484: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0190 - accuracy: 0.4698 - val_loss: 1.0261 - val_accuracy: 0.4633 - lr: 4.0960e-04\n",
      "Epoch 485/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4713\n",
      "Epoch 485: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0189 - accuracy: 0.4713 - val_loss: 1.0268 - val_accuracy: 0.4585 - lr: 4.0960e-04\n",
      "Epoch 486/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4699\n",
      "Epoch 486: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0188 - accuracy: 0.4699 - val_loss: 1.0237 - val_accuracy: 0.4676 - lr: 4.0960e-04\n",
      "Epoch 487/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0194 - accuracy: 0.4718\n",
      "Epoch 487: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0194 - accuracy: 0.4718 - val_loss: 1.0319 - val_accuracy: 0.4544 - lr: 4.0960e-04\n",
      "Epoch 488/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0193 - accuracy: 0.4689\n",
      "Epoch 488: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0193 - accuracy: 0.4689 - val_loss: 1.0333 - val_accuracy: 0.4600 - lr: 4.0960e-04\n",
      "Epoch 489/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4706\n",
      "Epoch 489: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0189 - accuracy: 0.4706 - val_loss: 1.0358 - val_accuracy: 0.4528 - lr: 4.0960e-04\n",
      "Epoch 490/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0190 - accuracy: 0.4702\n",
      "Epoch 490: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0190 - accuracy: 0.4702 - val_loss: 1.0373 - val_accuracy: 0.4550 - lr: 4.0960e-04\n",
      "Epoch 491/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4688\n",
      "Epoch 491: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0195 - accuracy: 0.4688 - val_loss: 1.0422 - val_accuracy: 0.4591 - lr: 4.0960e-04\n",
      "Epoch 492/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0192 - accuracy: 0.4709\n",
      "Epoch 492: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0192 - accuracy: 0.4710 - val_loss: 1.0247 - val_accuracy: 0.4623 - lr: 4.0960e-04\n",
      "Epoch 493/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0193 - accuracy: 0.4707\n",
      "Epoch 493: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0193 - accuracy: 0.4706 - val_loss: 1.0306 - val_accuracy: 0.4621 - lr: 4.0960e-04\n",
      "Epoch 494/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.4696\n",
      "Epoch 494: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0196 - accuracy: 0.4697 - val_loss: 1.0278 - val_accuracy: 0.4610 - lr: 4.0960e-04\n",
      "Epoch 495/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0201 - accuracy: 0.4686\n",
      "Epoch 495: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0201 - accuracy: 0.4686 - val_loss: 1.0329 - val_accuracy: 0.4558 - lr: 4.0960e-04\n",
      "Epoch 496/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4713\n",
      "Epoch 496: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0199 - accuracy: 0.4713 - val_loss: 1.0241 - val_accuracy: 0.4681 - lr: 4.0960e-04\n",
      "Epoch 497/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0186 - accuracy: 0.4717\n",
      "Epoch 497: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0186 - accuracy: 0.4718 - val_loss: 1.0289 - val_accuracy: 0.4620 - lr: 4.0960e-04\n",
      "Epoch 498/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.4692\n",
      "Epoch 498: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0196 - accuracy: 0.4693 - val_loss: 1.0315 - val_accuracy: 0.4625 - lr: 4.0960e-04\n",
      "Epoch 499/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0194 - accuracy: 0.4704\n",
      "Epoch 499: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0194 - accuracy: 0.4703 - val_loss: 1.0267 - val_accuracy: 0.4599 - lr: 4.0960e-04\n",
      "Epoch 500/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0199 - accuracy: 0.4701\n",
      "Epoch 500: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0199 - accuracy: 0.4701 - val_loss: 1.0219 - val_accuracy: 0.4656 - lr: 4.0960e-04\n",
      "Epoch 501/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0194 - accuracy: 0.4688\n",
      "Epoch 501: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0194 - accuracy: 0.4687 - val_loss: 1.0305 - val_accuracy: 0.4629 - lr: 4.0960e-04\n",
      "Epoch 502/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0196 - accuracy: 0.4712\n",
      "Epoch 502: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0196 - accuracy: 0.4712 - val_loss: 1.0350 - val_accuracy: 0.4528 - lr: 4.0960e-04\n",
      "Epoch 503/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.4694\n",
      "Epoch 503: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0200 - accuracy: 0.4694 - val_loss: 1.0262 - val_accuracy: 0.4669 - lr: 4.0960e-04\n",
      "Epoch 504/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0171 - accuracy: 0.4720\n",
      "Epoch 504: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0171 - accuracy: 0.4720 - val_loss: 1.0279 - val_accuracy: 0.4629 - lr: 3.2768e-04\n",
      "Epoch 505/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0179 - accuracy: 0.4707\n",
      "Epoch 505: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0179 - accuracy: 0.4707 - val_loss: 1.0256 - val_accuracy: 0.4614 - lr: 3.2768e-04\n",
      "Epoch 506/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0180 - accuracy: 0.4705\n",
      "Epoch 506: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0180 - accuracy: 0.4705 - val_loss: 1.0304 - val_accuracy: 0.4668 - lr: 3.2768e-04\n",
      "Epoch 507/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0182 - accuracy: 0.4715\n",
      "Epoch 507: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 82s 70ms/step - loss: 1.0182 - accuracy: 0.4715 - val_loss: 1.0291 - val_accuracy: 0.4617 - lr: 3.2768e-04\n",
      "Epoch 508/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0183 - accuracy: 0.4700\n",
      "Epoch 508: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0183 - accuracy: 0.4700 - val_loss: 1.0281 - val_accuracy: 0.4656 - lr: 3.2768e-04\n",
      "Epoch 509/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0184 - accuracy: 0.4721\n",
      "Epoch 509: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0184 - accuracy: 0.4721 - val_loss: 1.0260 - val_accuracy: 0.4640 - lr: 3.2768e-04\n",
      "Epoch 510/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0188 - accuracy: 0.4717\n",
      "Epoch 510: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0188 - accuracy: 0.4717 - val_loss: 1.0296 - val_accuracy: 0.4595 - lr: 3.2768e-04\n",
      "Epoch 511/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0184 - accuracy: 0.4712\n",
      "Epoch 511: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0184 - accuracy: 0.4712 - val_loss: 1.0270 - val_accuracy: 0.4655 - lr: 3.2768e-04\n",
      "Epoch 512/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0187 - accuracy: 0.4712\n",
      "Epoch 512: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0187 - accuracy: 0.4712 - val_loss: 1.0345 - val_accuracy: 0.4619 - lr: 3.2768e-04\n",
      "Epoch 513/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0183 - accuracy: 0.4709\n",
      "Epoch 513: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0183 - accuracy: 0.4709 - val_loss: 1.0247 - val_accuracy: 0.4638 - lr: 3.2768e-04\n",
      "Epoch 514/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0186 - accuracy: 0.4717\n",
      "Epoch 514: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0186 - accuracy: 0.4717 - val_loss: 1.0239 - val_accuracy: 0.4663 - lr: 3.2768e-04\n",
      "Epoch 515/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0191 - accuracy: 0.4690\n",
      "Epoch 515: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0192 - accuracy: 0.4690 - val_loss: 1.0349 - val_accuracy: 0.4553 - lr: 3.2768e-04\n",
      "Epoch 516/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0191 - accuracy: 0.4704\n",
      "Epoch 516: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0192 - accuracy: 0.4704 - val_loss: 1.0218 - val_accuracy: 0.4682 - lr: 3.2768e-04\n",
      "Epoch 517/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0193 - accuracy: 0.4711\n",
      "Epoch 517: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0193 - accuracy: 0.4711 - val_loss: 1.0284 - val_accuracy: 0.4618 - lr: 3.2768e-04\n",
      "Epoch 518/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0195 - accuracy: 0.4710\n",
      "Epoch 518: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0195 - accuracy: 0.4710 - val_loss: 1.0260 - val_accuracy: 0.4639 - lr: 3.2768e-04\n",
      "Epoch 519/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0194 - accuracy: 0.4694\n",
      "Epoch 519: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0194 - accuracy: 0.4694 - val_loss: 1.0321 - val_accuracy: 0.4574 - lr: 3.2768e-04\n",
      "Epoch 520/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0189 - accuracy: 0.4708\n",
      "Epoch 520: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 98s 84ms/step - loss: 1.0189 - accuracy: 0.4708 - val_loss: 1.0319 - val_accuracy: 0.4604 - lr: 3.2768e-04\n",
      "Epoch 521/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0201 - accuracy: 0.4683\n",
      "Epoch 521: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 112s 96ms/step - loss: 1.0201 - accuracy: 0.4683 - val_loss: 1.0360 - val_accuracy: 0.4597 - lr: 3.2768e-04\n",
      "Epoch 522/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0204 - accuracy: 0.4694\n",
      "Epoch 522: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 111s 95ms/step - loss: 1.0204 - accuracy: 0.4694 - val_loss: 1.0239 - val_accuracy: 0.4655 - lr: 3.2768e-04\n",
      "Epoch 523/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0197 - accuracy: 0.4712\n",
      "Epoch 523: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 98s 84ms/step - loss: 1.0197 - accuracy: 0.4712 - val_loss: 1.0316 - val_accuracy: 0.4589 - lr: 3.2768e-04\n",
      "Epoch 524/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0205 - accuracy: 0.4704\n",
      "Epoch 524: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0205 - accuracy: 0.4704 - val_loss: 1.0352 - val_accuracy: 0.4536 - lr: 3.2768e-04\n",
      "Epoch 525/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0206 - accuracy: 0.4682\n",
      "Epoch 525: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0206 - accuracy: 0.4682 - val_loss: 1.0368 - val_accuracy: 0.4583 - lr: 3.2768e-04\n",
      "Epoch 526/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.4693\n",
      "Epoch 526: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 98s 84ms/step - loss: 1.0202 - accuracy: 0.4693 - val_loss: 1.0230 - val_accuracy: 0.4666 - lr: 3.2768e-04\n",
      "Epoch 527/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.4699\n",
      "Epoch 527: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0202 - accuracy: 0.4699 - val_loss: 1.0243 - val_accuracy: 0.4605 - lr: 3.2768e-04\n",
      "Epoch 528/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.4691\n",
      "Epoch 528: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 98s 84ms/step - loss: 1.0202 - accuracy: 0.4690 - val_loss: 1.0335 - val_accuracy: 0.4638 - lr: 3.2768e-04\n",
      "Epoch 529/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0205 - accuracy: 0.4687\n",
      "Epoch 529: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 100s 86ms/step - loss: 1.0205 - accuracy: 0.4687 - val_loss: 1.0347 - val_accuracy: 0.4554 - lr: 3.2768e-04\n",
      "Epoch 530/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0205 - accuracy: 0.4692\n",
      "Epoch 530: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0205 - accuracy: 0.4692 - val_loss: 1.0246 - val_accuracy: 0.4649 - lr: 3.2768e-04\n",
      "Epoch 531/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0203 - accuracy: 0.4694\n",
      "Epoch 531: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 98s 84ms/step - loss: 1.0203 - accuracy: 0.4694 - val_loss: 1.0422 - val_accuracy: 0.4482 - lr: 3.2768e-04\n",
      "Epoch 532/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.4689\n",
      "Epoch 532: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 100s 85ms/step - loss: 1.0202 - accuracy: 0.4689 - val_loss: 1.0331 - val_accuracy: 0.4583 - lr: 3.2768e-04\n",
      "Epoch 533/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0203 - accuracy: 0.4686\n",
      "Epoch 533: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 84ms/step - loss: 1.0203 - accuracy: 0.4685 - val_loss: 1.0221 - val_accuracy: 0.4658 - lr: 3.2768e-04\n",
      "Epoch 534/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0212 - accuracy: 0.4668\n",
      "Epoch 534: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 99s 85ms/step - loss: 1.0212 - accuracy: 0.4668 - val_loss: 1.0212 - val_accuracy: 0.4677 - lr: 3.2768e-04\n",
      "Epoch 535/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0208 - accuracy: 0.4687\n",
      "Epoch 535: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 100s 86ms/step - loss: 1.0208 - accuracy: 0.4687 - val_loss: 1.0433 - val_accuracy: 0.4573 - lr: 3.2768e-04\n",
      "Epoch 536/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0206 - accuracy: 0.4694\n",
      "Epoch 536: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 98s 84ms/step - loss: 1.0206 - accuracy: 0.4694 - val_loss: 1.0362 - val_accuracy: 0.4570 - lr: 3.2768e-04\n",
      "Epoch 537/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0202 - accuracy: 0.4687\n",
      "Epoch 537: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 100s 85ms/step - loss: 1.0202 - accuracy: 0.4687 - val_loss: 1.0316 - val_accuracy: 0.4587 - lr: 3.2768e-04\n",
      "Epoch 538/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0205 - accuracy: 0.4691\n",
      "Epoch 538: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 90s 77ms/step - loss: 1.0205 - accuracy: 0.4691 - val_loss: 1.0296 - val_accuracy: 0.4576 - lr: 3.2768e-04\n",
      "Epoch 539/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0203 - accuracy: 0.4693\n",
      "Epoch 539: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 79ms/step - loss: 1.0203 - accuracy: 0.4693 - val_loss: 1.0294 - val_accuracy: 0.4617 - lr: 3.2768e-04\n",
      "Epoch 540/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0208 - accuracy: 0.4687\n",
      "Epoch 540: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0208 - accuracy: 0.4686 - val_loss: 1.0242 - val_accuracy: 0.4638 - lr: 3.2768e-04\n",
      "Epoch 541/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0198 - accuracy: 0.4699\n",
      "Epoch 541: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0198 - accuracy: 0.4699 - val_loss: 1.0265 - val_accuracy: 0.4617 - lr: 3.2768e-04\n",
      "Epoch 542/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.4680\n",
      "Epoch 542: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0222 - accuracy: 0.4680 - val_loss: 1.0328 - val_accuracy: 0.4586 - lr: 3.2768e-04\n",
      "Epoch 543/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0232 - accuracy: 0.4641\n",
      "Epoch 543: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0232 - accuracy: 0.4641 - val_loss: 1.0341 - val_accuracy: 0.4592 - lr: 3.2768e-04\n",
      "Epoch 544/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0218 - accuracy: 0.4667\n",
      "Epoch 544: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0218 - accuracy: 0.4667 - val_loss: 1.0382 - val_accuracy: 0.4542 - lr: 3.2768e-04\n",
      "Epoch 545/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.4661\n",
      "Epoch 545: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 85s 73ms/step - loss: 1.0222 - accuracy: 0.4661 - val_loss: 1.0273 - val_accuracy: 0.4619 - lr: 3.2768e-04\n",
      "Epoch 546/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0224 - accuracy: 0.4663\n",
      "Epoch 546: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0224 - accuracy: 0.4663 - val_loss: 1.0283 - val_accuracy: 0.4647 - lr: 3.2768e-04\n",
      "Epoch 547/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0226 - accuracy: 0.4665\n",
      "Epoch 547: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0226 - accuracy: 0.4665 - val_loss: 1.0270 - val_accuracy: 0.4587 - lr: 3.2768e-04\n",
      "Epoch 548/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4654\n",
      "Epoch 548: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0227 - accuracy: 0.4654 - val_loss: 1.0298 - val_accuracy: 0.4590 - lr: 3.2768e-04\n",
      "Epoch 549/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0224 - accuracy: 0.4658\n",
      "Epoch 549: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0224 - accuracy: 0.4658 - val_loss: 1.0336 - val_accuracy: 0.4513 - lr: 3.2768e-04\n",
      "Epoch 550/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0217 - accuracy: 0.4667\n",
      "Epoch 550: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0217 - accuracy: 0.4668 - val_loss: 1.0241 - val_accuracy: 0.4639 - lr: 3.2768e-04\n",
      "Epoch 551/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.4667\n",
      "Epoch 551: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 85s 73ms/step - loss: 1.0220 - accuracy: 0.4667 - val_loss: 1.0296 - val_accuracy: 0.4554 - lr: 3.2768e-04\n",
      "Epoch 552/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0232 - accuracy: 0.4653\n",
      "Epoch 552: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0232 - accuracy: 0.4653 - val_loss: 1.0353 - val_accuracy: 0.4580 - lr: 3.2768e-04\n",
      "Epoch 553/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4656\n",
      "Epoch 553: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0227 - accuracy: 0.4656 - val_loss: 1.0221 - val_accuracy: 0.4652 - lr: 3.2768e-04\n",
      "Epoch 554/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0213 - accuracy: 0.4685\n",
      "Epoch 554: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 85s 72ms/step - loss: 1.0213 - accuracy: 0.4685 - val_loss: 1.0235 - val_accuracy: 0.4645 - lr: 2.6214e-04\n",
      "Epoch 555/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0209 - accuracy: 0.4677\n",
      "Epoch 555: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0209 - accuracy: 0.4677 - val_loss: 1.0326 - val_accuracy: 0.4524 - lr: 2.6214e-04\n",
      "Epoch 556/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0212 - accuracy: 0.4687\n",
      "Epoch 556: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0213 - accuracy: 0.4687 - val_loss: 1.0259 - val_accuracy: 0.4614 - lr: 2.6214e-04\n",
      "Epoch 557/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0209 - accuracy: 0.4687\n",
      "Epoch 557: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 85s 72ms/step - loss: 1.0209 - accuracy: 0.4687 - val_loss: 1.0289 - val_accuracy: 0.4599 - lr: 2.6214e-04\n",
      "Epoch 558/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0209 - accuracy: 0.4697\n",
      "Epoch 558: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0209 - accuracy: 0.4696 - val_loss: 1.0229 - val_accuracy: 0.4636 - lr: 2.6214e-04\n",
      "Epoch 559/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4669\n",
      "Epoch 559: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 82s 70ms/step - loss: 1.0219 - accuracy: 0.4669 - val_loss: 1.0364 - val_accuracy: 0.4553 - lr: 2.6214e-04\n",
      "Epoch 560/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0218 - accuracy: 0.4687\n",
      "Epoch 560: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0218 - accuracy: 0.4687 - val_loss: 1.0394 - val_accuracy: 0.4537 - lr: 2.6214e-04\n",
      "Epoch 561/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0217 - accuracy: 0.4680\n",
      "Epoch 561: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0217 - accuracy: 0.4680 - val_loss: 1.0222 - val_accuracy: 0.4679 - lr: 2.6214e-04\n",
      "Epoch 562/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.4676\n",
      "Epoch 562: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 111s 95ms/step - loss: 1.0220 - accuracy: 0.4676 - val_loss: 1.0310 - val_accuracy: 0.4541 - lr: 2.6214e-04\n",
      "Epoch 563/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.4665\n",
      "Epoch 563: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0222 - accuracy: 0.4664 - val_loss: 1.0273 - val_accuracy: 0.4659 - lr: 2.6214e-04\n",
      "Epoch 564/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.4669\n",
      "Epoch 564: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 107s 92ms/step - loss: 1.0220 - accuracy: 0.4670 - val_loss: 1.0259 - val_accuracy: 0.4622 - lr: 2.6214e-04\n",
      "Epoch 565/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4671\n",
      "Epoch 565: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 80ms/step - loss: 1.0219 - accuracy: 0.4671 - val_loss: 1.0259 - val_accuracy: 0.4637 - lr: 2.6214e-04\n",
      "Epoch 566/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0220 - accuracy: 0.4669\n",
      "Epoch 566: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 81s 69ms/step - loss: 1.0219 - accuracy: 0.4669 - val_loss: 1.0249 - val_accuracy: 0.4615 - lr: 2.6214e-04\n",
      "Epoch 567/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4653\n",
      "Epoch 567: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0227 - accuracy: 0.4653 - val_loss: 1.0310 - val_accuracy: 0.4574 - lr: 2.6214e-04\n",
      "Epoch 568/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0226 - accuracy: 0.4677\n",
      "Epoch 568: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 81s 69ms/step - loss: 1.0226 - accuracy: 0.4677 - val_loss: 1.0288 - val_accuracy: 0.4601 - lr: 2.6214e-04\n",
      "Epoch 569/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0225 - accuracy: 0.4664\n",
      "Epoch 569: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 82s 71ms/step - loss: 1.0225 - accuracy: 0.4664 - val_loss: 1.0271 - val_accuracy: 0.4603 - lr: 2.6214e-04\n",
      "Epoch 570/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0229 - accuracy: 0.4649\n",
      "Epoch 570: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0229 - accuracy: 0.4649 - val_loss: 1.0257 - val_accuracy: 0.4625 - lr: 2.6214e-04\n",
      "Epoch 571/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0225 - accuracy: 0.4663\n",
      "Epoch 571: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0225 - accuracy: 0.4663 - val_loss: 1.0241 - val_accuracy: 0.4658 - lr: 2.6214e-04\n",
      "Epoch 572/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.4672\n",
      "Epoch 572: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0222 - accuracy: 0.4672 - val_loss: 1.0261 - val_accuracy: 0.4613 - lr: 2.6214e-04\n",
      "Epoch 573/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0225 - accuracy: 0.4673\n",
      "Epoch 573: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0225 - accuracy: 0.4673 - val_loss: 1.0259 - val_accuracy: 0.4635 - lr: 2.6214e-04\n",
      "Epoch 574/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0229 - accuracy: 0.4657\n",
      "Epoch 574: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 86s 74ms/step - loss: 1.0229 - accuracy: 0.4657 - val_loss: 1.0245 - val_accuracy: 0.4667 - lr: 2.6214e-04\n",
      "Epoch 575/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4663\n",
      "Epoch 575: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0219 - accuracy: 0.4663 - val_loss: 1.0406 - val_accuracy: 0.4522 - lr: 2.6214e-04\n",
      "Epoch 576/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0235 - accuracy: 0.4651\n",
      "Epoch 576: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 87s 75ms/step - loss: 1.0235 - accuracy: 0.4651 - val_loss: 1.0304 - val_accuracy: 0.4554 - lr: 2.6214e-04\n",
      "Epoch 577/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0232 - accuracy: 0.4648\n",
      "Epoch 577: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0232 - accuracy: 0.4649 - val_loss: 1.0252 - val_accuracy: 0.4626 - lr: 2.6214e-04\n",
      "Epoch 578/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0232 - accuracy: 0.4654\n",
      "Epoch 578: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 81s 69ms/step - loss: 1.0232 - accuracy: 0.4654 - val_loss: 1.0498 - val_accuracy: 0.4392 - lr: 2.6214e-04\n",
      "Epoch 579/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0236 - accuracy: 0.4651\n",
      "Epoch 579: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 83s 71ms/step - loss: 1.0236 - accuracy: 0.4651 - val_loss: 1.0273 - val_accuracy: 0.4602 - lr: 2.6214e-04\n",
      "Epoch 580/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4665\n",
      "Epoch 580: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 81s 70ms/step - loss: 1.0227 - accuracy: 0.4665 - val_loss: 1.0230 - val_accuracy: 0.4639 - lr: 2.6214e-04\n",
      "Epoch 581/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0231 - accuracy: 0.4668\n",
      "Epoch 581: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 79ms/step - loss: 1.0231 - accuracy: 0.4669 - val_loss: 1.0230 - val_accuracy: 0.4660 - lr: 2.6214e-04\n",
      "Epoch 582/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0221 - accuracy: 0.4658\n",
      "Epoch 582: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 114s 98ms/step - loss: 1.0222 - accuracy: 0.4658 - val_loss: 1.0255 - val_accuracy: 0.4639 - lr: 2.6214e-04\n",
      "Epoch 583/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0226 - accuracy: 0.4667\n",
      "Epoch 583: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0226 - accuracy: 0.4667 - val_loss: 1.0251 - val_accuracy: 0.4657 - lr: 2.6214e-04\n",
      "Epoch 584/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0230 - accuracy: 0.4656\n",
      "Epoch 584: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0230 - accuracy: 0.4656 - val_loss: 1.0339 - val_accuracy: 0.4531 - lr: 2.6214e-04\n",
      "Epoch 585/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0233 - accuracy: 0.4660\n",
      "Epoch 585: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0233 - accuracy: 0.4661 - val_loss: 1.0257 - val_accuracy: 0.4632 - lr: 2.6214e-04\n",
      "Epoch 586/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0229 - accuracy: 0.4665\n",
      "Epoch 586: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0229 - accuracy: 0.4665 - val_loss: 1.0251 - val_accuracy: 0.4608 - lr: 2.6214e-04\n",
      "Epoch 587/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0233 - accuracy: 0.4644\n",
      "Epoch 587: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0233 - accuracy: 0.4644 - val_loss: 1.0262 - val_accuracy: 0.4624 - lr: 2.6214e-04\n",
      "Epoch 588/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0228 - accuracy: 0.4667\n",
      "Epoch 588: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0228 - accuracy: 0.4667 - val_loss: 1.0273 - val_accuracy: 0.4599 - lr: 2.6214e-04\n",
      "Epoch 589/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0235 - accuracy: 0.4644\n",
      "Epoch 589: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0234 - accuracy: 0.4644 - val_loss: 1.0323 - val_accuracy: 0.4586 - lr: 2.6214e-04\n",
      "Epoch 590/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0232 - accuracy: 0.4650\n",
      "Epoch 590: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0232 - accuracy: 0.4650 - val_loss: 1.0239 - val_accuracy: 0.4659 - lr: 2.6214e-04\n",
      "Epoch 591/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0229 - accuracy: 0.4664\n",
      "Epoch 591: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0229 - accuracy: 0.4664 - val_loss: 1.0264 - val_accuracy: 0.4607 - lr: 2.6214e-04\n",
      "Epoch 592/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0231 - accuracy: 0.4657\n",
      "Epoch 592: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0231 - accuracy: 0.4657 - val_loss: 1.0320 - val_accuracy: 0.4577 - lr: 2.6214e-04\n",
      "Epoch 593/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0238 - accuracy: 0.4653\n",
      "Epoch 593: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0237 - accuracy: 0.4654 - val_loss: 1.0308 - val_accuracy: 0.4594 - lr: 2.6214e-04\n",
      "Epoch 594/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0231 - accuracy: 0.4665\n",
      "Epoch 594: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0231 - accuracy: 0.4665 - val_loss: 1.0249 - val_accuracy: 0.4662 - lr: 2.6214e-04\n",
      "Epoch 595/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0230 - accuracy: 0.4660\n",
      "Epoch 595: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0230 - accuracy: 0.4660 - val_loss: 1.0280 - val_accuracy: 0.4606 - lr: 2.6214e-04\n",
      "Epoch 596/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0244 - accuracy: 0.4643\n",
      "Epoch 596: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0244 - accuracy: 0.4643 - val_loss: 1.0545 - val_accuracy: 0.4365 - lr: 2.6214e-04\n",
      "Epoch 597/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0231 - accuracy: 0.4670\n",
      "Epoch 597: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 82s 70ms/step - loss: 1.0231 - accuracy: 0.4670 - val_loss: 1.0281 - val_accuracy: 0.4598 - lr: 2.6214e-04\n",
      "Epoch 598/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0235 - accuracy: 0.4661\n",
      "Epoch 598: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0235 - accuracy: 0.4661 - val_loss: 1.0377 - val_accuracy: 0.4561 - lr: 2.6214e-04\n",
      "Epoch 599/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0244 - accuracy: 0.4645\n",
      "Epoch 599: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0244 - accuracy: 0.4645 - val_loss: 1.0285 - val_accuracy: 0.4651 - lr: 2.6214e-04\n",
      "Epoch 600/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0239 - accuracy: 0.4649\n",
      "Epoch 600: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0239 - accuracy: 0.4650 - val_loss: 1.0375 - val_accuracy: 0.4530 - lr: 2.6214e-04\n",
      "Epoch 601/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0241 - accuracy: 0.4641\n",
      "Epoch 601: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0241 - accuracy: 0.4641 - val_loss: 1.0314 - val_accuracy: 0.4651 - lr: 2.6214e-04\n",
      "Epoch 602/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0233 - accuracy: 0.4659\n",
      "Epoch 602: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0233 - accuracy: 0.4660 - val_loss: 1.0266 - val_accuracy: 0.4635 - lr: 2.6214e-04\n",
      "Epoch 603/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0237 - accuracy: 0.4662\n",
      "Epoch 603: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 80s 69ms/step - loss: 1.0237 - accuracy: 0.4662 - val_loss: 1.0298 - val_accuracy: 0.4585 - lr: 2.6214e-04\n",
      "Epoch 604/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0224 - accuracy: 0.4661\n",
      "Epoch 604: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0224 - accuracy: 0.4661 - val_loss: 1.0275 - val_accuracy: 0.4622 - lr: 2.0972e-04\n",
      "Epoch 605/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0228 - accuracy: 0.4656\n",
      "Epoch 605: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0228 - accuracy: 0.4657 - val_loss: 1.0289 - val_accuracy: 0.4611 - lr: 2.0972e-04\n",
      "Epoch 606/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.4673\n",
      "Epoch 606: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0222 - accuracy: 0.4673 - val_loss: 1.0256 - val_accuracy: 0.4609 - lr: 2.0972e-04\n",
      "Epoch 607/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0226 - accuracy: 0.4666\n",
      "Epoch 607: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0226 - accuracy: 0.4666 - val_loss: 1.0376 - val_accuracy: 0.4528 - lr: 2.0972e-04\n",
      "Epoch 608/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0236 - accuracy: 0.4639\n",
      "Epoch 608: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0236 - accuracy: 0.4639 - val_loss: 1.0268 - val_accuracy: 0.4599 - lr: 2.0972e-04\n",
      "Epoch 609/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0234 - accuracy: 0.4650\n",
      "Epoch 609: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0234 - accuracy: 0.4650 - val_loss: 1.0285 - val_accuracy: 0.4640 - lr: 2.0972e-04\n",
      "Epoch 610/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0231 - accuracy: 0.4656\n",
      "Epoch 610: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0231 - accuracy: 0.4656 - val_loss: 1.0253 - val_accuracy: 0.4654 - lr: 2.0972e-04\n",
      "Epoch 611/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0233 - accuracy: 0.4654\n",
      "Epoch 611: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0233 - accuracy: 0.4654 - val_loss: 1.0254 - val_accuracy: 0.4667 - lr: 2.0972e-04\n",
      "Epoch 612/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.4647\n",
      "Epoch 612: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0246 - accuracy: 0.4647 - val_loss: 1.0432 - val_accuracy: 0.4520 - lr: 2.0972e-04\n",
      "Epoch 613/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0237 - accuracy: 0.4644\n",
      "Epoch 613: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0237 - accuracy: 0.4644 - val_loss: 1.0244 - val_accuracy: 0.4646 - lr: 2.0972e-04\n",
      "Epoch 614/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0243 - accuracy: 0.4637\n",
      "Epoch 614: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0243 - accuracy: 0.4637 - val_loss: 1.0261 - val_accuracy: 0.4613 - lr: 2.0972e-04\n",
      "Epoch 615/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0242 - accuracy: 0.4633\n",
      "Epoch 615: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0242 - accuracy: 0.4633 - val_loss: 1.0316 - val_accuracy: 0.4577 - lr: 2.0972e-04\n",
      "Epoch 616/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4628\n",
      "Epoch 616: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0250 - accuracy: 0.4628 - val_loss: 1.0254 - val_accuracy: 0.4612 - lr: 2.0972e-04\n",
      "Epoch 617/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.4636\n",
      "Epoch 617: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 79ms/step - loss: 1.0245 - accuracy: 0.4636 - val_loss: 1.0284 - val_accuracy: 0.4579 - lr: 2.0972e-04\n",
      "Epoch 618/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4643\n",
      "Epoch 618: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0250 - accuracy: 0.4643 - val_loss: 1.0223 - val_accuracy: 0.4660 - lr: 2.0972e-04\n",
      "Epoch 619/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0245 - accuracy: 0.4635\n",
      "Epoch 619: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0245 - accuracy: 0.4635 - val_loss: 1.0395 - val_accuracy: 0.4480 - lr: 2.0972e-04\n",
      "Epoch 620/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0248 - accuracy: 0.4634\n",
      "Epoch 620: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0248 - accuracy: 0.4634 - val_loss: 1.0275 - val_accuracy: 0.4642 - lr: 2.0972e-04\n",
      "Epoch 621/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0254 - accuracy: 0.4630\n",
      "Epoch 621: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 90s 77ms/step - loss: 1.0254 - accuracy: 0.4630 - val_loss: 1.0328 - val_accuracy: 0.4543 - lr: 2.0972e-04\n",
      "Epoch 622/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0247 - accuracy: 0.4632\n",
      "Epoch 622: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0247 - accuracy: 0.4632 - val_loss: 1.0249 - val_accuracy: 0.4648 - lr: 2.0972e-04\n",
      "Epoch 623/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0248 - accuracy: 0.4649\n",
      "Epoch 623: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0248 - accuracy: 0.4649 - val_loss: 1.0272 - val_accuracy: 0.4626 - lr: 2.0972e-04\n",
      "Epoch 624/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4638\n",
      "Epoch 624: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0250 - accuracy: 0.4638 - val_loss: 1.0328 - val_accuracy: 0.4540 - lr: 2.0972e-04\n",
      "Epoch 625/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4629\n",
      "Epoch 625: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0253 - accuracy: 0.4629 - val_loss: 1.0393 - val_accuracy: 0.4465 - lr: 2.0972e-04\n",
      "Epoch 626/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0250 - accuracy: 0.4636\n",
      "Epoch 626: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 90s 77ms/step - loss: 1.0250 - accuracy: 0.4636 - val_loss: 1.0368 - val_accuracy: 0.4593 - lr: 2.0972e-04\n",
      "Epoch 627/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.4623\n",
      "Epoch 627: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 77ms/step - loss: 1.0251 - accuracy: 0.4623 - val_loss: 1.0361 - val_accuracy: 0.4473 - lr: 2.0972e-04\n",
      "Epoch 628/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4627\n",
      "Epoch 628: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 77ms/step - loss: 1.0259 - accuracy: 0.4628 - val_loss: 1.0267 - val_accuracy: 0.4639 - lr: 2.0972e-04\n",
      "Epoch 629/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0254 - accuracy: 0.4630\n",
      "Epoch 629: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 90s 77ms/step - loss: 1.0254 - accuracy: 0.4630 - val_loss: 1.0365 - val_accuracy: 0.4531 - lr: 2.0972e-04\n",
      "Epoch 630/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0258 - accuracy: 0.4618\n",
      "Epoch 630: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0258 - accuracy: 0.4618 - val_loss: 1.0354 - val_accuracy: 0.4506 - lr: 2.0972e-04\n",
      "Epoch 631/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.4623\n",
      "Epoch 631: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0260 - accuracy: 0.4623 - val_loss: 1.0258 - val_accuracy: 0.4637 - lr: 2.0972e-04\n",
      "Epoch 632/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0261 - accuracy: 0.4625\n",
      "Epoch 632: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0261 - accuracy: 0.4625 - val_loss: 1.0283 - val_accuracy: 0.4589 - lr: 2.0972e-04\n",
      "Epoch 633/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4621\n",
      "Epoch 633: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0263 - accuracy: 0.4621 - val_loss: 1.0262 - val_accuracy: 0.4644 - lr: 2.0972e-04\n",
      "Epoch 634/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4617\n",
      "Epoch 634: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 90s 77ms/step - loss: 1.0263 - accuracy: 0.4617 - val_loss: 1.0249 - val_accuracy: 0.4618 - lr: 2.0972e-04\n",
      "Epoch 635/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4623\n",
      "Epoch 635: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0259 - accuracy: 0.4623 - val_loss: 1.0237 - val_accuracy: 0.4643 - lr: 2.0972e-04\n",
      "Epoch 636/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0255 - accuracy: 0.4630\n",
      "Epoch 636: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0255 - accuracy: 0.4630 - val_loss: 1.0352 - val_accuracy: 0.4530 - lr: 2.0972e-04\n",
      "Epoch 637/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0258 - accuracy: 0.4627\n",
      "Epoch 637: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 77ms/step - loss: 1.0258 - accuracy: 0.4627 - val_loss: 1.0352 - val_accuracy: 0.4511 - lr: 2.0972e-04\n",
      "Epoch 638/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0256 - accuracy: 0.4633\n",
      "Epoch 638: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0256 - accuracy: 0.4633 - val_loss: 1.0255 - val_accuracy: 0.4647 - lr: 2.0972e-04\n",
      "Epoch 639/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4617\n",
      "Epoch 639: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0258 - accuracy: 0.4617 - val_loss: 1.0263 - val_accuracy: 0.4590 - lr: 2.0972e-04\n",
      "Epoch 640/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.4626\n",
      "Epoch 640: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0259 - accuracy: 0.4626 - val_loss: 1.0326 - val_accuracy: 0.4533 - lr: 2.0972e-04\n",
      "Epoch 641/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0261 - accuracy: 0.4627\n",
      "Epoch 641: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0261 - accuracy: 0.4627 - val_loss: 1.0273 - val_accuracy: 0.4570 - lr: 2.0972e-04\n",
      "Epoch 642/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0262 - accuracy: 0.4621\n",
      "Epoch 642: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0262 - accuracy: 0.4620 - val_loss: 1.0275 - val_accuracy: 0.4588 - lr: 2.0972e-04\n",
      "Epoch 643/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0258 - accuracy: 0.4624\n",
      "Epoch 643: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 95s 82ms/step - loss: 1.0258 - accuracy: 0.4624 - val_loss: 1.0435 - val_accuracy: 0.4396 - lr: 2.0972e-04\n",
      "Epoch 644/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0265 - accuracy: 0.4621\n",
      "Epoch 644: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 79ms/step - loss: 1.0265 - accuracy: 0.4621 - val_loss: 1.0318 - val_accuracy: 0.4533 - lr: 2.0972e-04\n",
      "Epoch 645/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0269 - accuracy: 0.4623\n",
      "Epoch 645: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 80ms/step - loss: 1.0268 - accuracy: 0.4623 - val_loss: 1.0337 - val_accuracy: 0.4559 - lr: 2.0972e-04\n",
      "Epoch 646/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4615\n",
      "Epoch 646: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 94s 80ms/step - loss: 1.0263 - accuracy: 0.4615 - val_loss: 1.0248 - val_accuracy: 0.4614 - lr: 2.0972e-04\n",
      "Epoch 647/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0267 - accuracy: 0.4616\n",
      "Epoch 647: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 94s 80ms/step - loss: 1.0267 - accuracy: 0.4616 - val_loss: 1.0238 - val_accuracy: 0.4641 - lr: 2.0972e-04\n",
      "Epoch 648/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4625\n",
      "Epoch 648: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 80ms/step - loss: 1.0263 - accuracy: 0.4625 - val_loss: 1.0249 - val_accuracy: 0.4615 - lr: 2.0972e-04\n",
      "Epoch 649/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0266 - accuracy: 0.4607\n",
      "Epoch 649: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 80ms/step - loss: 1.0267 - accuracy: 0.4608 - val_loss: 1.0388 - val_accuracy: 0.4527 - lr: 2.0972e-04\n",
      "Epoch 650/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0261 - accuracy: 0.4623\n",
      "Epoch 650: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 79ms/step - loss: 1.0260 - accuracy: 0.4623 - val_loss: 1.0319 - val_accuracy: 0.4610 - lr: 2.0972e-04\n",
      "Epoch 651/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0266 - accuracy: 0.4624\n",
      "Epoch 651: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 94s 80ms/step - loss: 1.0266 - accuracy: 0.4624 - val_loss: 1.0487 - val_accuracy: 0.4411 - lr: 2.0972e-04\n",
      "Epoch 652/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0272 - accuracy: 0.4619\n",
      "Epoch 652: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 80ms/step - loss: 1.0272 - accuracy: 0.4619 - val_loss: 1.0307 - val_accuracy: 0.4607 - lr: 2.0972e-04\n",
      "Epoch 653/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4632\n",
      "Epoch 653: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 79ms/step - loss: 1.0263 - accuracy: 0.4632 - val_loss: 1.0243 - val_accuracy: 0.4648 - lr: 2.0972e-04\n",
      "Epoch 654/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0253 - accuracy: 0.4629\n",
      "Epoch 654: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 80ms/step - loss: 1.0253 - accuracy: 0.4629 - val_loss: 1.0297 - val_accuracy: 0.4568 - lr: 1.6777e-04\n",
      "Epoch 655/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0249 - accuracy: 0.4627\n",
      "Epoch 655: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 78ms/step - loss: 1.0249 - accuracy: 0.4627 - val_loss: 1.0303 - val_accuracy: 0.4640 - lr: 1.6777e-04\n",
      "Epoch 656/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0265 - accuracy: 0.4623\n",
      "Epoch 656: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 94s 80ms/step - loss: 1.0264 - accuracy: 0.4624 - val_loss: 1.0275 - val_accuracy: 0.4586 - lr: 1.6777e-04\n",
      "Epoch 657/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4615\n",
      "Epoch 657: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 95s 81ms/step - loss: 1.0263 - accuracy: 0.4615 - val_loss: 1.0338 - val_accuracy: 0.4538 - lr: 1.6777e-04\n",
      "Epoch 658/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4605\n",
      "Epoch 658: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 79ms/step - loss: 1.0270 - accuracy: 0.4605 - val_loss: 1.0373 - val_accuracy: 0.4501 - lr: 1.6777e-04\n",
      "Epoch 659/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0263 - accuracy: 0.4622\n",
      "Epoch 659: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 94s 80ms/step - loss: 1.0263 - accuracy: 0.4622 - val_loss: 1.0391 - val_accuracy: 0.4469 - lr: 1.6777e-04\n",
      "Epoch 660/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0268 - accuracy: 0.4615\n",
      "Epoch 660: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 79ms/step - loss: 1.0268 - accuracy: 0.4615 - val_loss: 1.0335 - val_accuracy: 0.4549 - lr: 1.6777e-04\n",
      "Epoch 661/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0270 - accuracy: 0.4610\n",
      "Epoch 661: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 66ms/step - loss: 1.0270 - accuracy: 0.4610 - val_loss: 1.0233 - val_accuracy: 0.4650 - lr: 1.6777e-04\n",
      "Epoch 662/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0274 - accuracy: 0.4612\n",
      "Epoch 662: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0274 - accuracy: 0.4612 - val_loss: 1.0402 - val_accuracy: 0.4517 - lr: 1.6777e-04\n",
      "Epoch 663/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0271 - accuracy: 0.4616\n",
      "Epoch 663: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0271 - accuracy: 0.4615 - val_loss: 1.0318 - val_accuracy: 0.4604 - lr: 1.6777e-04\n",
      "Epoch 664/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0274 - accuracy: 0.4607\n",
      "Epoch 664: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0274 - accuracy: 0.4607 - val_loss: 1.0271 - val_accuracy: 0.4609 - lr: 1.6777e-04\n",
      "Epoch 665/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0273 - accuracy: 0.4612\n",
      "Epoch 665: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0273 - accuracy: 0.4612 - val_loss: 1.0315 - val_accuracy: 0.4544 - lr: 1.6777e-04\n",
      "Epoch 666/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0277 - accuracy: 0.4601\n",
      "Epoch 666: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0277 - accuracy: 0.4601 - val_loss: 1.0320 - val_accuracy: 0.4541 - lr: 1.6777e-04\n",
      "Epoch 667/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0277 - accuracy: 0.4604\n",
      "Epoch 667: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0277 - accuracy: 0.4604 - val_loss: 1.0318 - val_accuracy: 0.4534 - lr: 1.6777e-04\n",
      "Epoch 668/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0281 - accuracy: 0.4605\n",
      "Epoch 668: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0281 - accuracy: 0.4605 - val_loss: 1.0331 - val_accuracy: 0.4549 - lr: 1.6777e-04\n",
      "Epoch 669/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0278 - accuracy: 0.4597\n",
      "Epoch 669: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 68ms/step - loss: 1.0278 - accuracy: 0.4597 - val_loss: 1.0250 - val_accuracy: 0.4638 - lr: 1.6777e-04\n",
      "Epoch 670/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0284 - accuracy: 0.4591\n",
      "Epoch 670: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 79s 67ms/step - loss: 1.0285 - accuracy: 0.4591 - val_loss: 1.0256 - val_accuracy: 0.4639 - lr: 1.6777e-04\n",
      "Epoch 671/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0281 - accuracy: 0.4593\n",
      "Epoch 671: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0281 - accuracy: 0.4593 - val_loss: 1.0302 - val_accuracy: 0.4549 - lr: 1.6777e-04\n",
      "Epoch 672/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4583\n",
      "Epoch 672: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0287 - accuracy: 0.4583 - val_loss: 1.0299 - val_accuracy: 0.4565 - lr: 1.6777e-04\n",
      "Epoch 673/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0286 - accuracy: 0.4593\n",
      "Epoch 673: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0286 - accuracy: 0.4593 - val_loss: 1.0287 - val_accuracy: 0.4559 - lr: 1.6777e-04\n",
      "Epoch 674/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.4603\n",
      "Epoch 674: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0282 - accuracy: 0.4603 - val_loss: 1.0255 - val_accuracy: 0.4632 - lr: 1.6777e-04\n",
      "Epoch 675/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0292 - accuracy: 0.4582\n",
      "Epoch 675: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0292 - accuracy: 0.4582 - val_loss: 1.0283 - val_accuracy: 0.4589 - lr: 1.6777e-04\n",
      "Epoch 676/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4587\n",
      "Epoch 676: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0287 - accuracy: 0.4588 - val_loss: 1.0261 - val_accuracy: 0.4635 - lr: 1.6777e-04\n",
      "Epoch 677/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.4571\n",
      "Epoch 677: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0291 - accuracy: 0.4572 - val_loss: 1.0249 - val_accuracy: 0.4632 - lr: 1.6777e-04\n",
      "Epoch 678/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0283 - accuracy: 0.4593\n",
      "Epoch 678: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0283 - accuracy: 0.4593 - val_loss: 1.0266 - val_accuracy: 0.4648 - lr: 1.6777e-04\n",
      "Epoch 679/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0286 - accuracy: 0.4577\n",
      "Epoch 679: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0285 - accuracy: 0.4577 - val_loss: 1.0307 - val_accuracy: 0.4581 - lr: 1.6777e-04\n",
      "Epoch 680/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.4589\n",
      "Epoch 680: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0291 - accuracy: 0.4589 - val_loss: 1.0461 - val_accuracy: 0.4484 - lr: 1.6777e-04\n",
      "Epoch 681/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.4600\n",
      "Epoch 681: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0291 - accuracy: 0.4600 - val_loss: 1.0292 - val_accuracy: 0.4581 - lr: 1.6777e-04\n",
      "Epoch 682/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.4586\n",
      "Epoch 682: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0295 - accuracy: 0.4586 - val_loss: 1.0265 - val_accuracy: 0.4603 - lr: 1.6777e-04\n",
      "Epoch 683/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.4576\n",
      "Epoch 683: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0297 - accuracy: 0.4576 - val_loss: 1.0276 - val_accuracy: 0.4619 - lr: 1.6777e-04\n",
      "Epoch 684/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.4582\n",
      "Epoch 684: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0291 - accuracy: 0.4582 - val_loss: 1.0364 - val_accuracy: 0.4446 - lr: 1.6777e-04\n",
      "Epoch 685/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0298 - accuracy: 0.4580\n",
      "Epoch 685: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0298 - accuracy: 0.4580 - val_loss: 1.0272 - val_accuracy: 0.4613 - lr: 1.6777e-04\n",
      "Epoch 686/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0294 - accuracy: 0.4586\n",
      "Epoch 686: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0294 - accuracy: 0.4586 - val_loss: 1.0308 - val_accuracy: 0.4616 - lr: 1.6777e-04\n",
      "Epoch 687/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0293 - accuracy: 0.4577\n",
      "Epoch 687: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0293 - accuracy: 0.4577 - val_loss: 1.0296 - val_accuracy: 0.4529 - lr: 1.6777e-04\n",
      "Epoch 688/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.4561\n",
      "Epoch 688: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0296 - accuracy: 0.4561 - val_loss: 1.0243 - val_accuracy: 0.4628 - lr: 1.6777e-04\n",
      "Epoch 689/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0290 - accuracy: 0.4579\n",
      "Epoch 689: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0290 - accuracy: 0.4580 - val_loss: 1.0301 - val_accuracy: 0.4606 - lr: 1.6777e-04\n",
      "Epoch 690/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.4575\n",
      "Epoch 690: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0296 - accuracy: 0.4576 - val_loss: 1.0288 - val_accuracy: 0.4577 - lr: 1.6777e-04\n",
      "Epoch 691/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0292 - accuracy: 0.4576\n",
      "Epoch 691: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0292 - accuracy: 0.4576 - val_loss: 1.0366 - val_accuracy: 0.4487 - lr: 1.6777e-04\n",
      "Epoch 692/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0301 - accuracy: 0.4565\n",
      "Epoch 692: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0301 - accuracy: 0.4565 - val_loss: 1.0248 - val_accuracy: 0.4643 - lr: 1.6777e-04\n",
      "Epoch 693/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0302 - accuracy: 0.4575\n",
      "Epoch 693: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 77s 66ms/step - loss: 1.0302 - accuracy: 0.4575 - val_loss: 1.0281 - val_accuracy: 0.4602 - lr: 1.6777e-04\n",
      "Epoch 694/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.4585\n",
      "Epoch 694: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 84s 72ms/step - loss: 1.0297 - accuracy: 0.4585 - val_loss: 1.0328 - val_accuracy: 0.4513 - lr: 1.6777e-04\n",
      "Epoch 695/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.4588\n",
      "Epoch 695: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0296 - accuracy: 0.4588 - val_loss: 1.0272 - val_accuracy: 0.4614 - lr: 1.6777e-04\n",
      "Epoch 696/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0293 - accuracy: 0.4584\n",
      "Epoch 696: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0293 - accuracy: 0.4584 - val_loss: 1.0252 - val_accuracy: 0.4622 - lr: 1.6777e-04\n",
      "Epoch 697/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0297 - accuracy: 0.4578\n",
      "Epoch 697: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0296 - accuracy: 0.4578 - val_loss: 1.0250 - val_accuracy: 0.4654 - lr: 1.6777e-04\n",
      "Epoch 698/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0303 - accuracy: 0.4579\n",
      "Epoch 698: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0302 - accuracy: 0.4579 - val_loss: 1.0345 - val_accuracy: 0.4535 - lr: 1.6777e-04\n",
      "Epoch 699/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0298 - accuracy: 0.4574\n",
      "Epoch 699: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0298 - accuracy: 0.4574 - val_loss: 1.0265 - val_accuracy: 0.4619 - lr: 1.6777e-04\n",
      "Epoch 700/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0294 - accuracy: 0.4583\n",
      "Epoch 700: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0294 - accuracy: 0.4584 - val_loss: 1.0246 - val_accuracy: 0.4619 - lr: 1.6777e-04\n",
      "Epoch 701/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0294 - accuracy: 0.4575\n",
      "Epoch 701: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0294 - accuracy: 0.4575 - val_loss: 1.0361 - val_accuracy: 0.4493 - lr: 1.6777e-04\n",
      "Epoch 702/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0295 - accuracy: 0.4604\n",
      "Epoch 702: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0295 - accuracy: 0.4604 - val_loss: 1.0338 - val_accuracy: 0.4564 - lr: 1.6777e-04\n",
      "Epoch 703/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0299 - accuracy: 0.4572\n",
      "Epoch 703: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 87s 74ms/step - loss: 1.0299 - accuracy: 0.4572 - val_loss: 1.0252 - val_accuracy: 0.4609 - lr: 1.6777e-04\n",
      "Epoch 704/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.4604\n",
      "Epoch 704: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 78s 67ms/step - loss: 1.0287 - accuracy: 0.4603 - val_loss: 1.0511 - val_accuracy: 0.4363 - lr: 1.3422e-04\n",
      "Epoch 705/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0290 - accuracy: 0.4586\n",
      "Epoch 705: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 85s 73ms/step - loss: 1.0290 - accuracy: 0.4586 - val_loss: 1.0229 - val_accuracy: 0.4652 - lr: 1.3422e-04\n",
      "Epoch 706/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0294 - accuracy: 0.4574\n",
      "Epoch 706: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 96s 82ms/step - loss: 1.0294 - accuracy: 0.4574 - val_loss: 1.0262 - val_accuracy: 0.4624 - lr: 1.3422e-04\n",
      "Epoch 707/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.4567\n",
      "Epoch 707: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 79ms/step - loss: 1.0296 - accuracy: 0.4567 - val_loss: 1.0284 - val_accuracy: 0.4583 - lr: 1.3422e-04\n",
      "Epoch 708/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0299 - accuracy: 0.4556\n",
      "Epoch 708: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0299 - accuracy: 0.4555 - val_loss: 1.0263 - val_accuracy: 0.4613 - lr: 1.3422e-04\n",
      "Epoch 709/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0298 - accuracy: 0.4575\n",
      "Epoch 709: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0297 - accuracy: 0.4575 - val_loss: 1.0295 - val_accuracy: 0.4562 - lr: 1.3422e-04\n",
      "Epoch 710/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0296 - accuracy: 0.4576\n",
      "Epoch 710: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0296 - accuracy: 0.4576 - val_loss: 1.0267 - val_accuracy: 0.4605 - lr: 1.3422e-04\n",
      "Epoch 711/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0303 - accuracy: 0.4569\n",
      "Epoch 711: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0303 - accuracy: 0.4568 - val_loss: 1.0258 - val_accuracy: 0.4633 - lr: 1.3422e-04\n",
      "Epoch 712/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0305 - accuracy: 0.4555\n",
      "Epoch 712: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 90s 77ms/step - loss: 1.0305 - accuracy: 0.4555 - val_loss: 1.0291 - val_accuracy: 0.4611 - lr: 1.3422e-04\n",
      "Epoch 713/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0306 - accuracy: 0.4576\n",
      "Epoch 713: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0306 - accuracy: 0.4576 - val_loss: 1.0278 - val_accuracy: 0.4600 - lr: 1.3422e-04\n",
      "Epoch 714/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0307 - accuracy: 0.4568\n",
      "Epoch 714: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 90s 77ms/step - loss: 1.0307 - accuracy: 0.4568 - val_loss: 1.0294 - val_accuracy: 0.4585 - lr: 1.3422e-04\n",
      "Epoch 715/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0302 - accuracy: 0.4568\n",
      "Epoch 715: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 93s 79ms/step - loss: 1.0303 - accuracy: 0.4568 - val_loss: 1.0288 - val_accuracy: 0.4561 - lr: 1.3422e-04\n",
      "Epoch 716/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0305 - accuracy: 0.4574\n",
      "Epoch 716: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0305 - accuracy: 0.4574 - val_loss: 1.0308 - val_accuracy: 0.4565 - lr: 1.3422e-04\n",
      "Epoch 717/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0302 - accuracy: 0.4575\n",
      "Epoch 717: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 76ms/step - loss: 1.0302 - accuracy: 0.4575 - val_loss: 1.0306 - val_accuracy: 0.4543 - lr: 1.3422e-04\n",
      "Epoch 718/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0307 - accuracy: 0.4558\n",
      "Epoch 718: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 92s 78ms/step - loss: 1.0307 - accuracy: 0.4558 - val_loss: 1.0255 - val_accuracy: 0.4640 - lr: 1.3422e-04\n",
      "Epoch 719/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0308 - accuracy: 0.4556\n",
      "Epoch 719: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0308 - accuracy: 0.4556 - val_loss: 1.0266 - val_accuracy: 0.4624 - lr: 1.3422e-04\n",
      "Epoch 720/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4564\n",
      "Epoch 720: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0313 - accuracy: 0.4564 - val_loss: 1.0301 - val_accuracy: 0.4557 - lr: 1.3422e-04\n",
      "Epoch 721/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4558\n",
      "Epoch 721: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0312 - accuracy: 0.4558 - val_loss: 1.0260 - val_accuracy: 0.4626 - lr: 1.3422e-04\n",
      "Epoch 722/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0327 - accuracy: 0.4516\n",
      "Epoch 722: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0327 - accuracy: 0.4516 - val_loss: 1.0290 - val_accuracy: 0.4566 - lr: 1.3422e-04\n",
      "Epoch 723/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0309 - accuracy: 0.4558\n",
      "Epoch 723: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 90s 77ms/step - loss: 1.0309 - accuracy: 0.4558 - val_loss: 1.0277 - val_accuracy: 0.4586 - lr: 1.3422e-04\n",
      "Epoch 724/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0311 - accuracy: 0.4556\n",
      "Epoch 724: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0311 - accuracy: 0.4555 - val_loss: 1.0337 - val_accuracy: 0.4602 - lr: 1.3422e-04\n",
      "Epoch 725/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4558\n",
      "Epoch 725: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0313 - accuracy: 0.4558 - val_loss: 1.0282 - val_accuracy: 0.4575 - lr: 1.3422e-04\n",
      "Epoch 726/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0317 - accuracy: 0.4559\n",
      "Epoch 726: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0317 - accuracy: 0.4559 - val_loss: 1.0356 - val_accuracy: 0.4409 - lr: 1.3422e-04\n",
      "Epoch 727/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0319 - accuracy: 0.4554\n",
      "Epoch 727: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 76ms/step - loss: 1.0319 - accuracy: 0.4553 - val_loss: 1.0286 - val_accuracy: 0.4565 - lr: 1.3422e-04\n",
      "Epoch 728/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4553\n",
      "Epoch 728: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 88s 75ms/step - loss: 1.0312 - accuracy: 0.4553 - val_loss: 1.0293 - val_accuracy: 0.4584 - lr: 1.3422e-04\n",
      "Epoch 729/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0320 - accuracy: 0.4555\n",
      "Epoch 729: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 91s 78ms/step - loss: 1.0320 - accuracy: 0.4555 - val_loss: 1.0288 - val_accuracy: 0.4571 - lr: 1.3422e-04\n",
      "Epoch 730/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0314 - accuracy: 0.4550\n",
      "Epoch 730: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 89s 77ms/step - loss: 1.0314 - accuracy: 0.4550 - val_loss: 1.0260 - val_accuracy: 0.4609 - lr: 1.3422e-04\n",
      "Epoch 731/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4559\n",
      "Epoch 731: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 106s 91ms/step - loss: 1.0313 - accuracy: 0.4559 - val_loss: 1.0329 - val_accuracy: 0.4498 - lr: 1.3422e-04\n",
      "Epoch 732/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0320 - accuracy: 0.4545\n",
      "Epoch 732: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0320 - accuracy: 0.4545 - val_loss: 1.0309 - val_accuracy: 0.4544 - lr: 1.3422e-04\n",
      "Epoch 733/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0307 - accuracy: 0.4570\n",
      "Epoch 733: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0307 - accuracy: 0.4569 - val_loss: 1.0282 - val_accuracy: 0.4603 - lr: 1.3422e-04\n",
      "Epoch 734/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0318 - accuracy: 0.4549\n",
      "Epoch 734: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0318 - accuracy: 0.4549 - val_loss: 1.0428 - val_accuracy: 0.4410 - lr: 1.3422e-04\n",
      "Epoch 735/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4557\n",
      "Epoch 735: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0313 - accuracy: 0.4557 - val_loss: 1.0296 - val_accuracy: 0.4595 - lr: 1.3422e-04\n",
      "Epoch 736/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4567\n",
      "Epoch 736: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0313 - accuracy: 0.4567 - val_loss: 1.0290 - val_accuracy: 0.4541 - lr: 1.3422e-04\n",
      "Epoch 737/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0315 - accuracy: 0.4567\n",
      "Epoch 737: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0315 - accuracy: 0.4568 - val_loss: 1.0320 - val_accuracy: 0.4579 - lr: 1.3422e-04\n",
      "Epoch 738/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0316 - accuracy: 0.4549\n",
      "Epoch 738: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0316 - accuracy: 0.4549 - val_loss: 1.0324 - val_accuracy: 0.4562 - lr: 1.3422e-04\n",
      "Epoch 739/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4569\n",
      "Epoch 739: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0312 - accuracy: 0.4568 - val_loss: 1.0391 - val_accuracy: 0.4480 - lr: 1.3422e-04\n",
      "Epoch 740/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0314 - accuracy: 0.4545\n",
      "Epoch 740: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0314 - accuracy: 0.4545 - val_loss: 1.0328 - val_accuracy: 0.4524 - lr: 1.3422e-04\n",
      "Epoch 741/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0315 - accuracy: 0.4551\n",
      "Epoch 741: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0315 - accuracy: 0.4551 - val_loss: 1.0350 - val_accuracy: 0.4504 - lr: 1.3422e-04\n",
      "Epoch 742/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4566\n",
      "Epoch 742: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0312 - accuracy: 0.4566 - val_loss: 1.0247 - val_accuracy: 0.4620 - lr: 1.3422e-04\n",
      "Epoch 743/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0310 - accuracy: 0.4559\n",
      "Epoch 743: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0310 - accuracy: 0.4559 - val_loss: 1.0267 - val_accuracy: 0.4624 - lr: 1.3422e-04\n",
      "Epoch 744/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0317 - accuracy: 0.4546\n",
      "Epoch 744: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0318 - accuracy: 0.4546 - val_loss: 1.0294 - val_accuracy: 0.4549 - lr: 1.3422e-04\n",
      "Epoch 745/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0314 - accuracy: 0.4562\n",
      "Epoch 745: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0314 - accuracy: 0.4562 - val_loss: 1.0277 - val_accuracy: 0.4601 - lr: 1.3422e-04\n",
      "Epoch 746/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4564\n",
      "Epoch 746: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0313 - accuracy: 0.4564 - val_loss: 1.0253 - val_accuracy: 0.4626 - lr: 1.3422e-04\n",
      "Epoch 747/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0314 - accuracy: 0.4553\n",
      "Epoch 747: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0314 - accuracy: 0.4553 - val_loss: 1.0275 - val_accuracy: 0.4633 - lr: 1.3422e-04\n",
      "Epoch 748/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0317 - accuracy: 0.4550\n",
      "Epoch 748: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0317 - accuracy: 0.4550 - val_loss: 1.0246 - val_accuracy: 0.4634 - lr: 1.3422e-04\n",
      "Epoch 749/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0317 - accuracy: 0.4553\n",
      "Epoch 749: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0317 - accuracy: 0.4553 - val_loss: 1.0272 - val_accuracy: 0.4584 - lr: 1.3422e-04\n",
      "Epoch 750/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0318 - accuracy: 0.4560\n",
      "Epoch 750: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0318 - accuracy: 0.4560 - val_loss: 1.0277 - val_accuracy: 0.4600 - lr: 1.3422e-04\n",
      "Epoch 751/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0319 - accuracy: 0.4535\n",
      "Epoch 751: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0319 - accuracy: 0.4535 - val_loss: 1.0294 - val_accuracy: 0.4583 - lr: 1.3422e-04\n",
      "Epoch 752/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4564\n",
      "Epoch 752: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0312 - accuracy: 0.4564 - val_loss: 1.0431 - val_accuracy: 0.4445 - lr: 1.3422e-04\n",
      "Epoch 753/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0313 - accuracy: 0.4565\n",
      "Epoch 753: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0312 - accuracy: 0.4565 - val_loss: 1.0279 - val_accuracy: 0.4582 - lr: 1.3422e-04\n",
      "Epoch 754/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0306 - accuracy: 0.4555\n",
      "Epoch 754: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0306 - accuracy: 0.4554 - val_loss: 1.0284 - val_accuracy: 0.4595 - lr: 1.0737e-04\n",
      "Epoch 755/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4561\n",
      "Epoch 755: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0312 - accuracy: 0.4561 - val_loss: 1.0283 - val_accuracy: 0.4595 - lr: 1.0737e-04\n",
      "Epoch 756/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0315 - accuracy: 0.4558\n",
      "Epoch 756: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0315 - accuracy: 0.4558 - val_loss: 1.0333 - val_accuracy: 0.4562 - lr: 1.0737e-04\n",
      "Epoch 757/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0317 - accuracy: 0.4544\n",
      "Epoch 757: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0317 - accuracy: 0.4543 - val_loss: 1.0284 - val_accuracy: 0.4596 - lr: 1.0737e-04\n",
      "Epoch 758/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.4565\n",
      "Epoch 758: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0312 - accuracy: 0.4565 - val_loss: 1.0332 - val_accuracy: 0.4537 - lr: 1.0737e-04\n",
      "Epoch 759/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0320 - accuracy: 0.4551\n",
      "Epoch 759: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0320 - accuracy: 0.4551 - val_loss: 1.0263 - val_accuracy: 0.4604 - lr: 1.0737e-04\n",
      "Epoch 760/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0325 - accuracy: 0.4554\n",
      "Epoch 760: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0325 - accuracy: 0.4554 - val_loss: 1.0365 - val_accuracy: 0.4513 - lr: 1.0737e-04\n",
      "Epoch 761/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0319 - accuracy: 0.4559\n",
      "Epoch 761: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0319 - accuracy: 0.4560 - val_loss: 1.0310 - val_accuracy: 0.4547 - lr: 1.0737e-04\n",
      "Epoch 762/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0321 - accuracy: 0.4547\n",
      "Epoch 762: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0321 - accuracy: 0.4547 - val_loss: 1.0317 - val_accuracy: 0.4540 - lr: 1.0737e-04\n",
      "Epoch 763/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.4549\n",
      "Epoch 763: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0329 - accuracy: 0.4549 - val_loss: 1.0291 - val_accuracy: 0.4578 - lr: 1.0737e-04\n",
      "Epoch 764/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0328 - accuracy: 0.4549\n",
      "Epoch 764: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0327 - accuracy: 0.4549 - val_loss: 1.0289 - val_accuracy: 0.4623 - lr: 1.0737e-04\n",
      "Epoch 765/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.4545\n",
      "Epoch 765: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0329 - accuracy: 0.4545 - val_loss: 1.0268 - val_accuracy: 0.4615 - lr: 1.0737e-04\n",
      "Epoch 766/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0324 - accuracy: 0.4556\n",
      "Epoch 766: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0324 - accuracy: 0.4556 - val_loss: 1.0378 - val_accuracy: 0.4488 - lr: 1.0737e-04\n",
      "Epoch 767/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0322 - accuracy: 0.4553\n",
      "Epoch 767: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0322 - accuracy: 0.4553 - val_loss: 1.0348 - val_accuracy: 0.4566 - lr: 1.0737e-04\n",
      "Epoch 768/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0324 - accuracy: 0.4545\n",
      "Epoch 768: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0324 - accuracy: 0.4544 - val_loss: 1.0248 - val_accuracy: 0.4631 - lr: 1.0737e-04\n",
      "Epoch 769/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.4537\n",
      "Epoch 769: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0329 - accuracy: 0.4537 - val_loss: 1.0362 - val_accuracy: 0.4570 - lr: 1.0737e-04\n",
      "Epoch 770/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0326 - accuracy: 0.4554\n",
      "Epoch 770: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0326 - accuracy: 0.4554 - val_loss: 1.0321 - val_accuracy: 0.4518 - lr: 1.0737e-04\n",
      "Epoch 771/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4521\n",
      "Epoch 771: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0338 - accuracy: 0.4521 - val_loss: 1.0293 - val_accuracy: 0.4578 - lr: 1.0737e-04\n",
      "Epoch 772/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0335 - accuracy: 0.4536\n",
      "Epoch 772: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0335 - accuracy: 0.4535 - val_loss: 1.0343 - val_accuracy: 0.4549 - lr: 1.0737e-04\n",
      "Epoch 773/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0331 - accuracy: 0.4543\n",
      "Epoch 773: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0331 - accuracy: 0.4544 - val_loss: 1.0278 - val_accuracy: 0.4604 - lr: 1.0737e-04\n",
      "Epoch 774/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0331 - accuracy: 0.4545\n",
      "Epoch 774: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0331 - accuracy: 0.4545 - val_loss: 1.0319 - val_accuracy: 0.4611 - lr: 1.0737e-04\n",
      "Epoch 775/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0328 - accuracy: 0.4538\n",
      "Epoch 775: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0327 - accuracy: 0.4538 - val_loss: 1.0533 - val_accuracy: 0.4380 - lr: 1.0737e-04\n",
      "Epoch 776/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0330 - accuracy: 0.4543\n",
      "Epoch 776: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0330 - accuracy: 0.4543 - val_loss: 1.0301 - val_accuracy: 0.4594 - lr: 1.0737e-04\n",
      "Epoch 777/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0334 - accuracy: 0.4531\n",
      "Epoch 777: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0335 - accuracy: 0.4531 - val_loss: 1.0307 - val_accuracy: 0.4584 - lr: 1.0737e-04\n",
      "Epoch 778/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4534\n",
      "Epoch 778: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0336 - accuracy: 0.4534 - val_loss: 1.0293 - val_accuracy: 0.4618 - lr: 1.0737e-04\n",
      "Epoch 779/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0329 - accuracy: 0.4552\n",
      "Epoch 779: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0329 - accuracy: 0.4552 - val_loss: 1.0368 - val_accuracy: 0.4564 - lr: 1.0737e-04\n",
      "Epoch 780/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0333 - accuracy: 0.4535\n",
      "Epoch 780: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0333 - accuracy: 0.4535 - val_loss: 1.0301 - val_accuracy: 0.4521 - lr: 1.0737e-04\n",
      "Epoch 781/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0334 - accuracy: 0.4536\n",
      "Epoch 781: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0334 - accuracy: 0.4535 - val_loss: 1.0291 - val_accuracy: 0.4582 - lr: 1.0737e-04\n",
      "Epoch 782/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4517\n",
      "Epoch 782: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0348 - accuracy: 0.4517 - val_loss: 1.0376 - val_accuracy: 0.4513 - lr: 1.0737e-04\n",
      "Epoch 783/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.4542\n",
      "Epoch 783: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0331 - accuracy: 0.4542 - val_loss: 1.0415 - val_accuracy: 0.4412 - lr: 1.0737e-04\n",
      "Epoch 784/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4534\n",
      "Epoch 784: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0341 - accuracy: 0.4534 - val_loss: 1.0320 - val_accuracy: 0.4568 - lr: 1.0737e-04\n",
      "Epoch 785/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0335 - accuracy: 0.4540\n",
      "Epoch 785: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0335 - accuracy: 0.4540 - val_loss: 1.0347 - val_accuracy: 0.4510 - lr: 1.0737e-04\n",
      "Epoch 786/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4519\n",
      "Epoch 786: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0340 - accuracy: 0.4519 - val_loss: 1.0387 - val_accuracy: 0.4505 - lr: 1.0737e-04\n",
      "Epoch 787/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4537\n",
      "Epoch 787: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0341 - accuracy: 0.4537 - val_loss: 1.0377 - val_accuracy: 0.4497 - lr: 1.0737e-04\n",
      "Epoch 788/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4534\n",
      "Epoch 788: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0338 - accuracy: 0.4534 - val_loss: 1.0318 - val_accuracy: 0.4568 - lr: 1.0737e-04\n",
      "Epoch 789/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0333 - accuracy: 0.4552\n",
      "Epoch 789: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0333 - accuracy: 0.4552 - val_loss: 1.0400 - val_accuracy: 0.4484 - lr: 1.0737e-04\n",
      "Epoch 790/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4536\n",
      "Epoch 790: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0338 - accuracy: 0.4536 - val_loss: 1.0323 - val_accuracy: 0.4601 - lr: 1.0737e-04\n",
      "Epoch 791/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4531\n",
      "Epoch 791: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0336 - accuracy: 0.4531 - val_loss: 1.0282 - val_accuracy: 0.4601 - lr: 1.0737e-04\n",
      "Epoch 792/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4531\n",
      "Epoch 792: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0342 - accuracy: 0.4531 - val_loss: 1.0361 - val_accuracy: 0.4522 - lr: 1.0737e-04\n",
      "Epoch 793/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0334 - accuracy: 0.4532\n",
      "Epoch 793: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0334 - accuracy: 0.4532 - val_loss: 1.0311 - val_accuracy: 0.4553 - lr: 1.0737e-04\n",
      "Epoch 794/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4532\n",
      "Epoch 794: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0339 - accuracy: 0.4532 - val_loss: 1.0281 - val_accuracy: 0.4592 - lr: 1.0737e-04\n",
      "Epoch 795/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0334 - accuracy: 0.4535\n",
      "Epoch 795: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0334 - accuracy: 0.4535 - val_loss: 1.0311 - val_accuracy: 0.4574 - lr: 1.0737e-04\n",
      "Epoch 796/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4532\n",
      "Epoch 796: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0338 - accuracy: 0.4532 - val_loss: 1.0282 - val_accuracy: 0.4577 - lr: 1.0737e-04\n",
      "Epoch 797/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4537\n",
      "Epoch 797: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0336 - accuracy: 0.4537 - val_loss: 1.0309 - val_accuracy: 0.4551 - lr: 1.0737e-04\n",
      "Epoch 798/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.4527\n",
      "Epoch 798: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0338 - accuracy: 0.4528 - val_loss: 1.0345 - val_accuracy: 0.4547 - lr: 1.0737e-04\n",
      "Epoch 799/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4533\n",
      "Epoch 799: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0336 - accuracy: 0.4533 - val_loss: 1.0313 - val_accuracy: 0.4579 - lr: 1.0737e-04\n",
      "Epoch 800/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4543\n",
      "Epoch 800: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0339 - accuracy: 0.4543 - val_loss: 1.0315 - val_accuracy: 0.4600 - lr: 1.0737e-04\n",
      "Epoch 801/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0337 - accuracy: 0.4534\n",
      "Epoch 801: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0337 - accuracy: 0.4533 - val_loss: 1.0319 - val_accuracy: 0.4593 - lr: 1.0737e-04\n",
      "Epoch 802/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0332 - accuracy: 0.4544\n",
      "Epoch 802: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0332 - accuracy: 0.4544 - val_loss: 1.0314 - val_accuracy: 0.4595 - lr: 1.0737e-04\n",
      "Epoch 803/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4526\n",
      "Epoch 803: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0340 - accuracy: 0.4525 - val_loss: 1.0470 - val_accuracy: 0.4513 - lr: 1.0737e-04\n",
      "Epoch 804/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0331 - accuracy: 0.4543\n",
      "Epoch 804: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0332 - accuracy: 0.4543 - val_loss: 1.0338 - val_accuracy: 0.4598 - lr: 8.5899e-05\n",
      "Epoch 805/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0337 - accuracy: 0.4533\n",
      "Epoch 805: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0336 - accuracy: 0.4533 - val_loss: 1.0408 - val_accuracy: 0.4534 - lr: 8.5899e-05\n",
      "Epoch 806/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4531\n",
      "Epoch 806: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0336 - accuracy: 0.4531 - val_loss: 1.0291 - val_accuracy: 0.4618 - lr: 8.5899e-05\n",
      "Epoch 807/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.4535\n",
      "Epoch 807: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0339 - accuracy: 0.4535 - val_loss: 1.0302 - val_accuracy: 0.4589 - lr: 8.5899e-05\n",
      "Epoch 808/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4523\n",
      "Epoch 808: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0341 - accuracy: 0.4523 - val_loss: 1.0310 - val_accuracy: 0.4584 - lr: 8.5899e-05\n",
      "Epoch 809/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0340 - accuracy: 0.4532\n",
      "Epoch 809: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0340 - accuracy: 0.4532 - val_loss: 1.0387 - val_accuracy: 0.4541 - lr: 8.5899e-05\n",
      "Epoch 810/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4518\n",
      "Epoch 810: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0341 - accuracy: 0.4518 - val_loss: 1.0318 - val_accuracy: 0.4607 - lr: 8.5899e-05\n",
      "Epoch 811/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4525\n",
      "Epoch 811: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0343 - accuracy: 0.4525 - val_loss: 1.0331 - val_accuracy: 0.4603 - lr: 8.5899e-05\n",
      "Epoch 812/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0341 - accuracy: 0.4536\n",
      "Epoch 812: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0341 - accuracy: 0.4536 - val_loss: 1.0291 - val_accuracy: 0.4587 - lr: 8.5899e-05\n",
      "Epoch 813/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0336 - accuracy: 0.4540\n",
      "Epoch 813: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0336 - accuracy: 0.4541 - val_loss: 1.0300 - val_accuracy: 0.4586 - lr: 8.5899e-05\n",
      "Epoch 814/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4523\n",
      "Epoch 814: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0345 - accuracy: 0.4523 - val_loss: 1.0366 - val_accuracy: 0.4531 - lr: 8.5899e-05\n",
      "Epoch 815/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4511\n",
      "Epoch 815: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0349 - accuracy: 0.4512 - val_loss: 1.0361 - val_accuracy: 0.4527 - lr: 8.5899e-05\n",
      "Epoch 816/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4518\n",
      "Epoch 816: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0344 - accuracy: 0.4518 - val_loss: 1.0357 - val_accuracy: 0.4524 - lr: 8.5899e-05\n",
      "Epoch 817/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4530\n",
      "Epoch 817: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0345 - accuracy: 0.4530 - val_loss: 1.0338 - val_accuracy: 0.4581 - lr: 8.5899e-05\n",
      "Epoch 818/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4522\n",
      "Epoch 818: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0345 - accuracy: 0.4523 - val_loss: 1.0482 - val_accuracy: 0.4465 - lr: 8.5899e-05\n",
      "Epoch 819/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4521\n",
      "Epoch 819: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0349 - accuracy: 0.4521 - val_loss: 1.0326 - val_accuracy: 0.4565 - lr: 8.5899e-05\n",
      "Epoch 820/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4514\n",
      "Epoch 820: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0346 - accuracy: 0.4514 - val_loss: 1.0318 - val_accuracy: 0.4566 - lr: 8.5899e-05\n",
      "Epoch 821/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4539\n",
      "Epoch 821: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0342 - accuracy: 0.4539 - val_loss: 1.0581 - val_accuracy: 0.4382 - lr: 8.5899e-05\n",
      "Epoch 822/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4515\n",
      "Epoch 822: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0350 - accuracy: 0.4515 - val_loss: 1.0392 - val_accuracy: 0.4501 - lr: 8.5899e-05\n",
      "Epoch 823/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0342 - accuracy: 0.4535\n",
      "Epoch 823: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0342 - accuracy: 0.4535 - val_loss: 1.0337 - val_accuracy: 0.4568 - lr: 8.5899e-05\n",
      "Epoch 824/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4529\n",
      "Epoch 824: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0344 - accuracy: 0.4529 - val_loss: 1.0367 - val_accuracy: 0.4547 - lr: 8.5899e-05\n",
      "Epoch 825/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4525\n",
      "Epoch 825: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0344 - accuracy: 0.4525 - val_loss: 1.0412 - val_accuracy: 0.4527 - lr: 8.5899e-05\n",
      "Epoch 826/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4502\n",
      "Epoch 826: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0350 - accuracy: 0.4503 - val_loss: 1.0377 - val_accuracy: 0.4468 - lr: 8.5899e-05\n",
      "Epoch 827/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4509\n",
      "Epoch 827: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0352 - accuracy: 0.4509 - val_loss: 1.0336 - val_accuracy: 0.4596 - lr: 8.5899e-05\n",
      "Epoch 828/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0343 - accuracy: 0.4526\n",
      "Epoch 828: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0344 - accuracy: 0.4526 - val_loss: 1.0439 - val_accuracy: 0.4504 - lr: 8.5899e-05\n",
      "Epoch 829/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4512\n",
      "Epoch 829: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0348 - accuracy: 0.4512 - val_loss: 1.0320 - val_accuracy: 0.4551 - lr: 8.5899e-05\n",
      "Epoch 830/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4525\n",
      "Epoch 830: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0349 - accuracy: 0.4525 - val_loss: 1.0359 - val_accuracy: 0.4509 - lr: 8.5899e-05\n",
      "Epoch 831/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4511\n",
      "Epoch 831: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0349 - accuracy: 0.4511 - val_loss: 1.0334 - val_accuracy: 0.4551 - lr: 8.5899e-05\n",
      "Epoch 832/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4524\n",
      "Epoch 832: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0344 - accuracy: 0.4524 - val_loss: 1.0360 - val_accuracy: 0.4566 - lr: 8.5899e-05\n",
      "Epoch 833/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4512\n",
      "Epoch 833: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0346 - accuracy: 0.4512 - val_loss: 1.0563 - val_accuracy: 0.4439 - lr: 8.5899e-05\n",
      "Epoch 834/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4506\n",
      "Epoch 834: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0348 - accuracy: 0.4507 - val_loss: 1.0383 - val_accuracy: 0.4588 - lr: 8.5899e-05\n",
      "Epoch 835/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0349 - accuracy: 0.4512\n",
      "Epoch 835: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0349 - accuracy: 0.4512 - val_loss: 1.0314 - val_accuracy: 0.4589 - lr: 8.5899e-05\n",
      "Epoch 836/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4535\n",
      "Epoch 836: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0345 - accuracy: 0.4535 - val_loss: 1.0531 - val_accuracy: 0.4472 - lr: 8.5899e-05\n",
      "Epoch 837/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4525\n",
      "Epoch 837: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0345 - accuracy: 0.4525 - val_loss: 1.0310 - val_accuracy: 0.4599 - lr: 8.5899e-05\n",
      "Epoch 838/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.4529\n",
      "Epoch 838: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0346 - accuracy: 0.4529 - val_loss: 1.0301 - val_accuracy: 0.4595 - lr: 8.5899e-05\n",
      "Epoch 839/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0345 - accuracy: 0.4519\n",
      "Epoch 839: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0345 - accuracy: 0.4519 - val_loss: 1.0413 - val_accuracy: 0.4501 - lr: 8.5899e-05\n",
      "Epoch 840/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4526\n",
      "Epoch 840: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0347 - accuracy: 0.4526 - val_loss: 1.0366 - val_accuracy: 0.4591 - lr: 8.5899e-05\n",
      "Epoch 841/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4508\n",
      "Epoch 841: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0352 - accuracy: 0.4508 - val_loss: 1.0433 - val_accuracy: 0.4482 - lr: 8.5899e-05\n",
      "Epoch 842/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4509\n",
      "Epoch 842: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0347 - accuracy: 0.4509 - val_loss: 1.0415 - val_accuracy: 0.4454 - lr: 8.5899e-05\n",
      "Epoch 843/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0358 - accuracy: 0.4509\n",
      "Epoch 843: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0358 - accuracy: 0.4509 - val_loss: 1.0356 - val_accuracy: 0.4498 - lr: 8.5899e-05\n",
      "Epoch 844/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4512\n",
      "Epoch 844: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0352 - accuracy: 0.4513 - val_loss: 1.0290 - val_accuracy: 0.4595 - lr: 8.5899e-05\n",
      "Epoch 845/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4503\n",
      "Epoch 845: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0352 - accuracy: 0.4503 - val_loss: 1.0330 - val_accuracy: 0.4573 - lr: 8.5899e-05\n",
      "Epoch 846/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4516\n",
      "Epoch 846: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0350 - accuracy: 0.4516 - val_loss: 1.0332 - val_accuracy: 0.4538 - lr: 8.5899e-05\n",
      "Epoch 847/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4519\n",
      "Epoch 847: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0347 - accuracy: 0.4519 - val_loss: 1.0339 - val_accuracy: 0.4563 - lr: 8.5899e-05\n",
      "Epoch 848/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.4513\n",
      "Epoch 848: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0347 - accuracy: 0.4512 - val_loss: 1.0508 - val_accuracy: 0.4407 - lr: 8.5899e-05\n",
      "Epoch 849/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0351 - accuracy: 0.4520\n",
      "Epoch 849: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0351 - accuracy: 0.4520 - val_loss: 1.0331 - val_accuracy: 0.4566 - lr: 8.5899e-05\n",
      "Epoch 850/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0347 - accuracy: 0.4531\n",
      "Epoch 850: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0347 - accuracy: 0.4531 - val_loss: 1.0300 - val_accuracy: 0.4616 - lr: 8.5899e-05\n",
      "Epoch 851/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.4509\n",
      "Epoch 851: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0353 - accuracy: 0.4510 - val_loss: 1.0436 - val_accuracy: 0.4485 - lr: 8.5899e-05\n",
      "Epoch 852/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.4505\n",
      "Epoch 852: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0352 - accuracy: 0.4505 - val_loss: 1.0333 - val_accuracy: 0.4581 - lr: 8.5899e-05\n",
      "Epoch 853/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4516\n",
      "Epoch 853: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0350 - accuracy: 0.4516 - val_loss: 1.0346 - val_accuracy: 0.4563 - lr: 8.5899e-05\n",
      "Epoch 854/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0344 - accuracy: 0.4530\n",
      "Epoch 854: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0344 - accuracy: 0.4530 - val_loss: 1.0354 - val_accuracy: 0.4528 - lr: 6.8719e-05\n",
      "Epoch 855/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0350 - accuracy: 0.4519\n",
      "Epoch 855: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0351 - accuracy: 0.4518 - val_loss: 1.0350 - val_accuracy: 0.4550 - lr: 6.8719e-05\n",
      "Epoch 856/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0354 - accuracy: 0.4514\n",
      "Epoch 856: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0354 - accuracy: 0.4514 - val_loss: 1.0395 - val_accuracy: 0.4557 - lr: 6.8719e-05\n",
      "Epoch 857/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0356 - accuracy: 0.4504\n",
      "Epoch 857: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0356 - accuracy: 0.4504 - val_loss: 1.0464 - val_accuracy: 0.4443 - lr: 6.8719e-05\n",
      "Epoch 858/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.4525\n",
      "Epoch 858: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0357 - accuracy: 0.4525 - val_loss: 1.0470 - val_accuracy: 0.4428 - lr: 6.8719e-05\n",
      "Epoch 859/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0358 - accuracy: 0.4517\n",
      "Epoch 859: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0358 - accuracy: 0.4517 - val_loss: 1.0364 - val_accuracy: 0.4547 - lr: 6.8719e-05\n",
      "Epoch 860/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0360 - accuracy: 0.4502\n",
      "Epoch 860: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0360 - accuracy: 0.4502 - val_loss: 1.0418 - val_accuracy: 0.4539 - lr: 6.8719e-05\n",
      "Epoch 861/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.4489\n",
      "Epoch 861: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0359 - accuracy: 0.4489 - val_loss: 1.0493 - val_accuracy: 0.4363 - lr: 6.8719e-05\n",
      "Epoch 862/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.4500\n",
      "Epoch 862: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0364 - accuracy: 0.4500 - val_loss: 1.0477 - val_accuracy: 0.4485 - lr: 6.8719e-05\n",
      "Epoch 863/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0367 - accuracy: 0.4494\n",
      "Epoch 863: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0367 - accuracy: 0.4494 - val_loss: 1.0388 - val_accuracy: 0.4533 - lr: 6.8719e-05\n",
      "Epoch 864/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0363 - accuracy: 0.4511\n",
      "Epoch 864: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0363 - accuracy: 0.4511 - val_loss: 1.0468 - val_accuracy: 0.4411 - lr: 6.8719e-05\n",
      "Epoch 865/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0357 - accuracy: 0.4511\n",
      "Epoch 865: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0357 - accuracy: 0.4511 - val_loss: 1.0391 - val_accuracy: 0.4488 - lr: 6.8719e-05\n",
      "Epoch 866/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.4505\n",
      "Epoch 866: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0364 - accuracy: 0.4505 - val_loss: 1.0442 - val_accuracy: 0.4430 - lr: 6.8719e-05\n",
      "Epoch 867/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0365 - accuracy: 0.4504\n",
      "Epoch 867: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0364 - accuracy: 0.4505 - val_loss: 1.0432 - val_accuracy: 0.4468 - lr: 6.8719e-05\n",
      "Epoch 868/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0371 - accuracy: 0.4491\n",
      "Epoch 868: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0371 - accuracy: 0.4491 - val_loss: 1.0482 - val_accuracy: 0.4448 - lr: 6.8719e-05\n",
      "Epoch 869/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0373 - accuracy: 0.4482\n",
      "Epoch 869: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0372 - accuracy: 0.4482 - val_loss: 1.0536 - val_accuracy: 0.4443 - lr: 6.8719e-05\n",
      "Epoch 870/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0372 - accuracy: 0.4505\n",
      "Epoch 870: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0372 - accuracy: 0.4505 - val_loss: 1.0347 - val_accuracy: 0.4518 - lr: 6.8719e-05\n",
      "Epoch 871/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0372 - accuracy: 0.4488\n",
      "Epoch 871: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0372 - accuracy: 0.4488 - val_loss: 1.0379 - val_accuracy: 0.4559 - lr: 6.8719e-05\n",
      "Epoch 872/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0371 - accuracy: 0.4495\n",
      "Epoch 872: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0371 - accuracy: 0.4495 - val_loss: 1.0336 - val_accuracy: 0.4573 - lr: 6.8719e-05\n",
      "Epoch 873/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0369 - accuracy: 0.4505\n",
      "Epoch 873: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0369 - accuracy: 0.4505 - val_loss: 1.0353 - val_accuracy: 0.4540 - lr: 6.8719e-05\n",
      "Epoch 874/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0369 - accuracy: 0.4503\n",
      "Epoch 874: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0369 - accuracy: 0.4502 - val_loss: 1.0323 - val_accuracy: 0.4576 - lr: 6.8719e-05\n",
      "Epoch 875/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0371 - accuracy: 0.4498\n",
      "Epoch 875: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0371 - accuracy: 0.4498 - val_loss: 1.0414 - val_accuracy: 0.4471 - lr: 6.8719e-05\n",
      "Epoch 876/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0370 - accuracy: 0.4499\n",
      "Epoch 876: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0371 - accuracy: 0.4499 - val_loss: 1.0587 - val_accuracy: 0.4390 - lr: 6.8719e-05\n",
      "Epoch 877/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0370 - accuracy: 0.4493\n",
      "Epoch 877: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0370 - accuracy: 0.4493 - val_loss: 1.0541 - val_accuracy: 0.4268 - lr: 6.8719e-05\n",
      "Epoch 878/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0373 - accuracy: 0.4494\n",
      "Epoch 878: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0373 - accuracy: 0.4494 - val_loss: 1.0430 - val_accuracy: 0.4501 - lr: 6.8719e-05\n",
      "Epoch 879/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0367 - accuracy: 0.4495\n",
      "Epoch 879: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0366 - accuracy: 0.4494 - val_loss: 1.0558 - val_accuracy: 0.4367 - lr: 6.8719e-05\n",
      "Epoch 880/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0377 - accuracy: 0.4489\n",
      "Epoch 880: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0377 - accuracy: 0.4489 - val_loss: 1.0377 - val_accuracy: 0.4550 - lr: 6.8719e-05\n",
      "Epoch 881/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0370 - accuracy: 0.4496\n",
      "Epoch 881: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0370 - accuracy: 0.4496 - val_loss: 1.0463 - val_accuracy: 0.4404 - lr: 6.8719e-05\n",
      "Epoch 882/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0371 - accuracy: 0.4485\n",
      "Epoch 882: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0371 - accuracy: 0.4484 - val_loss: 1.0555 - val_accuracy: 0.4371 - lr: 6.8719e-05\n",
      "Epoch 883/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0375 - accuracy: 0.4488\n",
      "Epoch 883: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0375 - accuracy: 0.4488 - val_loss: 1.0499 - val_accuracy: 0.4424 - lr: 6.8719e-05\n",
      "Epoch 884/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0376 - accuracy: 0.4462\n",
      "Epoch 884: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0376 - accuracy: 0.4462 - val_loss: 1.0386 - val_accuracy: 0.4513 - lr: 6.8719e-05\n",
      "Epoch 885/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0374 - accuracy: 0.4479\n",
      "Epoch 885: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0374 - accuracy: 0.4480 - val_loss: 1.0368 - val_accuracy: 0.4545 - lr: 6.8719e-05\n",
      "Epoch 886/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0377 - accuracy: 0.4493\n",
      "Epoch 886: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0377 - accuracy: 0.4493 - val_loss: 1.0664 - val_accuracy: 0.4353 - lr: 6.8719e-05\n",
      "Epoch 887/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0376 - accuracy: 0.4493\n",
      "Epoch 887: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0376 - accuracy: 0.4493 - val_loss: 1.0528 - val_accuracy: 0.4342 - lr: 6.8719e-05\n",
      "Epoch 888/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0379 - accuracy: 0.4489\n",
      "Epoch 888: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0379 - accuracy: 0.4489 - val_loss: 1.0440 - val_accuracy: 0.4502 - lr: 6.8719e-05\n",
      "Epoch 889/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0375 - accuracy: 0.4470\n",
      "Epoch 889: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0375 - accuracy: 0.4469 - val_loss: 1.0397 - val_accuracy: 0.4511 - lr: 6.8719e-05\n",
      "Epoch 890/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0373 - accuracy: 0.4482\n",
      "Epoch 890: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0373 - accuracy: 0.4482 - val_loss: 1.0530 - val_accuracy: 0.4327 - lr: 6.8719e-05\n",
      "Epoch 891/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0373 - accuracy: 0.4495\n",
      "Epoch 891: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0373 - accuracy: 0.4495 - val_loss: 1.0523 - val_accuracy: 0.4420 - lr: 6.8719e-05\n",
      "Epoch 892/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0375 - accuracy: 0.4490\n",
      "Epoch 892: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0375 - accuracy: 0.4490 - val_loss: 1.0676 - val_accuracy: 0.4162 - lr: 6.8719e-05\n",
      "Epoch 893/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0379 - accuracy: 0.4482\n",
      "Epoch 893: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0379 - accuracy: 0.4482 - val_loss: 1.0341 - val_accuracy: 0.4569 - lr: 6.8719e-05\n",
      "Epoch 894/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0377 - accuracy: 0.4482\n",
      "Epoch 894: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0377 - accuracy: 0.4482 - val_loss: 1.0355 - val_accuracy: 0.4543 - lr: 6.8719e-05\n",
      "Epoch 895/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0380 - accuracy: 0.4477\n",
      "Epoch 895: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0380 - accuracy: 0.4477 - val_loss: 1.0468 - val_accuracy: 0.4407 - lr: 6.8719e-05\n",
      "Epoch 896/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0378 - accuracy: 0.4478\n",
      "Epoch 896: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0378 - accuracy: 0.4477 - val_loss: 1.0366 - val_accuracy: 0.4527 - lr: 6.8719e-05\n",
      "Epoch 897/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0377 - accuracy: 0.4467\n",
      "Epoch 897: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0377 - accuracy: 0.4467 - val_loss: 1.0427 - val_accuracy: 0.4471 - lr: 6.8719e-05\n",
      "Epoch 898/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0373 - accuracy: 0.4485\n",
      "Epoch 898: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0373 - accuracy: 0.4485 - val_loss: 1.0677 - val_accuracy: 0.4314 - lr: 6.8719e-05\n",
      "Epoch 899/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0379 - accuracy: 0.4483\n",
      "Epoch 899: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0378 - accuracy: 0.4483 - val_loss: 1.0388 - val_accuracy: 0.4544 - lr: 6.8719e-05\n",
      "Epoch 900/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0378 - accuracy: 0.4464\n",
      "Epoch 900: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0378 - accuracy: 0.4464 - val_loss: 1.0351 - val_accuracy: 0.4559 - lr: 6.8719e-05\n",
      "Epoch 901/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0376 - accuracy: 0.4480\n",
      "Epoch 901: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0376 - accuracy: 0.4480 - val_loss: 1.0371 - val_accuracy: 0.4538 - lr: 6.8719e-05\n",
      "Epoch 902/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0378 - accuracy: 0.4464\n",
      "Epoch 902: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0378 - accuracy: 0.4464 - val_loss: 1.0504 - val_accuracy: 0.4387 - lr: 6.8719e-05\n",
      "Epoch 903/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0379 - accuracy: 0.4471\n",
      "Epoch 903: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0379 - accuracy: 0.4471 - val_loss: 1.0454 - val_accuracy: 0.4433 - lr: 6.8719e-05\n",
      "Epoch 904/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0376 - accuracy: 0.4503\n",
      "Epoch 904: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0376 - accuracy: 0.4503 - val_loss: 1.0392 - val_accuracy: 0.4453 - lr: 5.4976e-05\n",
      "Epoch 905/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0383 - accuracy: 0.4479\n",
      "Epoch 905: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0383 - accuracy: 0.4479 - val_loss: 1.0516 - val_accuracy: 0.4294 - lr: 5.4976e-05\n",
      "Epoch 906/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0386 - accuracy: 0.4479\n",
      "Epoch 906: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0386 - accuracy: 0.4479 - val_loss: 1.0503 - val_accuracy: 0.4438 - lr: 5.4976e-05\n",
      "Epoch 907/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0389 - accuracy: 0.4477\n",
      "Epoch 907: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0389 - accuracy: 0.4477 - val_loss: 1.0380 - val_accuracy: 0.4505 - lr: 5.4976e-05\n",
      "Epoch 908/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0399 - accuracy: 0.4466\n",
      "Epoch 908: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0399 - accuracy: 0.4466 - val_loss: 1.0363 - val_accuracy: 0.4539 - lr: 5.4976e-05\n",
      "Epoch 909/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0401 - accuracy: 0.4474\n",
      "Epoch 909: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0400 - accuracy: 0.4474 - val_loss: 1.0378 - val_accuracy: 0.4536 - lr: 5.4976e-05\n",
      "Epoch 910/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0406 - accuracy: 0.4466\n",
      "Epoch 910: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0406 - accuracy: 0.4466 - val_loss: 1.0529 - val_accuracy: 0.4313 - lr: 5.4976e-05\n",
      "Epoch 911/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0405 - accuracy: 0.4448\n",
      "Epoch 911: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0405 - accuracy: 0.4449 - val_loss: 1.0600 - val_accuracy: 0.4344 - lr: 5.4976e-05\n",
      "Epoch 912/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0411 - accuracy: 0.4447\n",
      "Epoch 912: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0411 - accuracy: 0.4447 - val_loss: 1.0503 - val_accuracy: 0.4359 - lr: 5.4976e-05\n",
      "Epoch 913/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0414 - accuracy: 0.4440\n",
      "Epoch 913: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0414 - accuracy: 0.4440 - val_loss: 1.0561 - val_accuracy: 0.4236 - lr: 5.4976e-05\n",
      "Epoch 914/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0410 - accuracy: 0.4440\n",
      "Epoch 914: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0409 - accuracy: 0.4440 - val_loss: 1.0485 - val_accuracy: 0.4393 - lr: 5.4976e-05\n",
      "Epoch 915/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0412 - accuracy: 0.4450\n",
      "Epoch 915: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0412 - accuracy: 0.4450 - val_loss: 1.0452 - val_accuracy: 0.4425 - lr: 5.4976e-05\n",
      "Epoch 916/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0414 - accuracy: 0.4454\n",
      "Epoch 916: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0414 - accuracy: 0.4454 - val_loss: 1.0480 - val_accuracy: 0.4400 - lr: 5.4976e-05\n",
      "Epoch 917/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0419 - accuracy: 0.4441\n",
      "Epoch 917: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0419 - accuracy: 0.4441 - val_loss: 1.0449 - val_accuracy: 0.4466 - lr: 5.4976e-05\n",
      "Epoch 918/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0416 - accuracy: 0.4459\n",
      "Epoch 918: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0417 - accuracy: 0.4459 - val_loss: 1.0609 - val_accuracy: 0.4260 - lr: 5.4976e-05\n",
      "Epoch 919/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0426 - accuracy: 0.4448\n",
      "Epoch 919: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0426 - accuracy: 0.4448 - val_loss: 1.0377 - val_accuracy: 0.4512 - lr: 5.4976e-05\n",
      "Epoch 920/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0426 - accuracy: 0.4432\n",
      "Epoch 920: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0426 - accuracy: 0.4432 - val_loss: 1.0523 - val_accuracy: 0.4299 - lr: 5.4976e-05\n",
      "Epoch 921/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0418 - accuracy: 0.4457\n",
      "Epoch 921: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0418 - accuracy: 0.4457 - val_loss: 1.0564 - val_accuracy: 0.4316 - lr: 5.4976e-05\n",
      "Epoch 922/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0435 - accuracy: 0.4434\n",
      "Epoch 922: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0435 - accuracy: 0.4434 - val_loss: 1.0475 - val_accuracy: 0.4353 - lr: 5.4976e-05\n",
      "Epoch 923/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0426 - accuracy: 0.4432\n",
      "Epoch 923: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0426 - accuracy: 0.4431 - val_loss: 1.0354 - val_accuracy: 0.4544 - lr: 5.4976e-05\n",
      "Epoch 924/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0435 - accuracy: 0.4429\n",
      "Epoch 924: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0435 - accuracy: 0.4429 - val_loss: 1.0387 - val_accuracy: 0.4504 - lr: 5.4976e-05\n",
      "Epoch 925/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0441 - accuracy: 0.4415\n",
      "Epoch 925: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 100ms/step - loss: 1.0441 - accuracy: 0.4414 - val_loss: 1.0536 - val_accuracy: 0.4265 - lr: 5.4976e-05\n",
      "Epoch 926/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0442 - accuracy: 0.4417\n",
      "Epoch 926: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0442 - accuracy: 0.4417 - val_loss: 1.0555 - val_accuracy: 0.4271 - lr: 5.4976e-05\n",
      "Epoch 927/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0441 - accuracy: 0.4415\n",
      "Epoch 927: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0441 - accuracy: 0.4415 - val_loss: 1.0499 - val_accuracy: 0.4333 - lr: 5.4976e-05\n",
      "Epoch 928/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0448 - accuracy: 0.4419\n",
      "Epoch 928: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0448 - accuracy: 0.4419 - val_loss: 1.0754 - val_accuracy: 0.4236 - lr: 5.4976e-05\n",
      "Epoch 929/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0446 - accuracy: 0.4413\n",
      "Epoch 929: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0446 - accuracy: 0.4413 - val_loss: 1.0394 - val_accuracy: 0.4512 - lr: 5.4976e-05\n",
      "Epoch 930/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0439 - accuracy: 0.4414\n",
      "Epoch 930: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0439 - accuracy: 0.4414 - val_loss: 1.0355 - val_accuracy: 0.4561 - lr: 5.4976e-05\n",
      "Epoch 931/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0449 - accuracy: 0.4403\n",
      "Epoch 931: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0449 - accuracy: 0.4403 - val_loss: 1.0489 - val_accuracy: 0.4345 - lr: 5.4976e-05\n",
      "Epoch 932/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0448 - accuracy: 0.4409\n",
      "Epoch 932: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0448 - accuracy: 0.4409 - val_loss: 1.0542 - val_accuracy: 0.4312 - lr: 5.4976e-05\n",
      "Epoch 933/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0452 - accuracy: 0.4395\n",
      "Epoch 933: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0452 - accuracy: 0.4395 - val_loss: 1.0786 - val_accuracy: 0.4146 - lr: 5.4976e-05\n",
      "Epoch 934/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0452 - accuracy: 0.4402\n",
      "Epoch 934: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0452 - accuracy: 0.4402 - val_loss: 1.0398 - val_accuracy: 0.4453 - lr: 5.4976e-05\n",
      "Epoch 935/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0452 - accuracy: 0.4416\n",
      "Epoch 935: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0452 - accuracy: 0.4416 - val_loss: 1.0385 - val_accuracy: 0.4531 - lr: 5.4976e-05\n",
      "Epoch 936/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0446 - accuracy: 0.4418\n",
      "Epoch 936: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0446 - accuracy: 0.4418 - val_loss: 1.0422 - val_accuracy: 0.4423 - lr: 5.4976e-05\n",
      "Epoch 937/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0438 - accuracy: 0.4436\n",
      "Epoch 937: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0438 - accuracy: 0.4436 - val_loss: 1.0463 - val_accuracy: 0.4403 - lr: 5.4976e-05\n",
      "Epoch 938/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0451 - accuracy: 0.4408\n",
      "Epoch 938: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0451 - accuracy: 0.4408 - val_loss: 1.0442 - val_accuracy: 0.4424 - lr: 5.4976e-05\n",
      "Epoch 939/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0453 - accuracy: 0.4409\n",
      "Epoch 939: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0453 - accuracy: 0.4409 - val_loss: 1.0493 - val_accuracy: 0.4311 - lr: 5.4976e-05\n",
      "Epoch 940/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0440 - accuracy: 0.4423\n",
      "Epoch 940: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0440 - accuracy: 0.4422 - val_loss: 1.0679 - val_accuracy: 0.4015 - lr: 5.4976e-05\n",
      "Epoch 941/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0439 - accuracy: 0.4429\n",
      "Epoch 941: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0439 - accuracy: 0.4429 - val_loss: 1.0391 - val_accuracy: 0.4492 - lr: 5.4976e-05\n",
      "Epoch 942/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0452 - accuracy: 0.4401\n",
      "Epoch 942: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0452 - accuracy: 0.4401 - val_loss: 1.0399 - val_accuracy: 0.4469 - lr: 5.4976e-05\n",
      "Epoch 943/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0447 - accuracy: 0.4404\n",
      "Epoch 943: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0447 - accuracy: 0.4404 - val_loss: 1.0452 - val_accuracy: 0.4427 - lr: 5.4976e-05\n",
      "Epoch 944/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0451 - accuracy: 0.4429\n",
      "Epoch 944: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0451 - accuracy: 0.4429 - val_loss: 1.0386 - val_accuracy: 0.4514 - lr: 5.4976e-05\n",
      "Epoch 945/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0448 - accuracy: 0.4408\n",
      "Epoch 945: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0448 - accuracy: 0.4408 - val_loss: 1.0638 - val_accuracy: 0.4212 - lr: 5.4976e-05\n",
      "Epoch 946/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0453 - accuracy: 0.4408\n",
      "Epoch 946: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0453 - accuracy: 0.4408 - val_loss: 1.0358 - val_accuracy: 0.4553 - lr: 5.4976e-05\n",
      "Epoch 947/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0446 - accuracy: 0.4413\n",
      "Epoch 947: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0447 - accuracy: 0.4413 - val_loss: 1.0457 - val_accuracy: 0.4384 - lr: 5.4976e-05\n",
      "Epoch 948/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0453 - accuracy: 0.4414\n",
      "Epoch 948: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0453 - accuracy: 0.4414 - val_loss: 1.0403 - val_accuracy: 0.4437 - lr: 5.4976e-05\n",
      "Epoch 949/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0455 - accuracy: 0.4404\n",
      "Epoch 949: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0455 - accuracy: 0.4405 - val_loss: 1.0410 - val_accuracy: 0.4464 - lr: 5.4976e-05\n",
      "Epoch 950/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0442 - accuracy: 0.4415\n",
      "Epoch 950: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0442 - accuracy: 0.4415 - val_loss: 1.0389 - val_accuracy: 0.4484 - lr: 5.4976e-05\n",
      "Epoch 951/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0444 - accuracy: 0.4429\n",
      "Epoch 951: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0444 - accuracy: 0.4429 - val_loss: 1.0397 - val_accuracy: 0.4479 - lr: 5.4976e-05\n",
      "Epoch 952/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0446 - accuracy: 0.4419\n",
      "Epoch 952: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0446 - accuracy: 0.4419 - val_loss: 1.0417 - val_accuracy: 0.4476 - lr: 5.4976e-05\n",
      "Epoch 953/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0443 - accuracy: 0.4415\n",
      "Epoch 953: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0443 - accuracy: 0.4415 - val_loss: 1.0600 - val_accuracy: 0.4269 - lr: 5.4976e-05\n",
      "Epoch 954/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0447 - accuracy: 0.4424\n",
      "Epoch 954: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0447 - accuracy: 0.4424 - val_loss: 1.0362 - val_accuracy: 0.4562 - lr: 4.3980e-05\n",
      "Epoch 955/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0465 - accuracy: 0.4401\n",
      "Epoch 955: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0465 - accuracy: 0.4401 - val_loss: 1.0882 - val_accuracy: 0.4076 - lr: 4.3980e-05\n",
      "Epoch 956/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0468 - accuracy: 0.4393\n",
      "Epoch 956: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0468 - accuracy: 0.4393 - val_loss: 1.0388 - val_accuracy: 0.4517 - lr: 4.3980e-05\n",
      "Epoch 957/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0461 - accuracy: 0.4407\n",
      "Epoch 957: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0461 - accuracy: 0.4407 - val_loss: 1.0649 - val_accuracy: 0.4219 - lr: 4.3980e-05\n",
      "Epoch 958/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0472 - accuracy: 0.4386\n",
      "Epoch 958: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0472 - accuracy: 0.4386 - val_loss: 1.0477 - val_accuracy: 0.4387 - lr: 4.3980e-05\n",
      "Epoch 959/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0473 - accuracy: 0.4382\n",
      "Epoch 959: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0473 - accuracy: 0.4382 - val_loss: 1.0405 - val_accuracy: 0.4491 - lr: 4.3980e-05\n",
      "Epoch 960/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0485 - accuracy: 0.4375\n",
      "Epoch 960: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0485 - accuracy: 0.4375 - val_loss: 1.0461 - val_accuracy: 0.4400 - lr: 4.3980e-05\n",
      "Epoch 961/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0475 - accuracy: 0.4399\n",
      "Epoch 961: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0475 - accuracy: 0.4399 - val_loss: 1.0485 - val_accuracy: 0.4344 - lr: 4.3980e-05\n",
      "Epoch 962/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0474 - accuracy: 0.4386\n",
      "Epoch 962: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0474 - accuracy: 0.4386 - val_loss: 1.0839 - val_accuracy: 0.4079 - lr: 4.3980e-05\n",
      "Epoch 963/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0481 - accuracy: 0.4382\n",
      "Epoch 963: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0481 - accuracy: 0.4382 - val_loss: 1.0517 - val_accuracy: 0.4240 - lr: 4.3980e-05\n",
      "Epoch 964/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0471 - accuracy: 0.4397\n",
      "Epoch 964: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0471 - accuracy: 0.4397 - val_loss: 1.0508 - val_accuracy: 0.4311 - lr: 4.3980e-05\n",
      "Epoch 965/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0488 - accuracy: 0.4365\n",
      "Epoch 965: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0488 - accuracy: 0.4366 - val_loss: 1.0732 - val_accuracy: 0.4201 - lr: 4.3980e-05\n",
      "Epoch 966/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0485 - accuracy: 0.4361\n",
      "Epoch 966: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0485 - accuracy: 0.4361 - val_loss: 1.0482 - val_accuracy: 0.4330 - lr: 4.3980e-05\n",
      "Epoch 967/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0482 - accuracy: 0.4381\n",
      "Epoch 967: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0482 - accuracy: 0.4381 - val_loss: 1.0427 - val_accuracy: 0.4439 - lr: 4.3980e-05\n",
      "Epoch 968/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0482 - accuracy: 0.4378\n",
      "Epoch 968: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0482 - accuracy: 0.4378 - val_loss: 1.0419 - val_accuracy: 0.4473 - lr: 4.3980e-05\n",
      "Epoch 969/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0479 - accuracy: 0.4379\n",
      "Epoch 969: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0479 - accuracy: 0.4379 - val_loss: 1.0377 - val_accuracy: 0.4558 - lr: 4.3980e-05\n",
      "Epoch 970/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0479 - accuracy: 0.4386\n",
      "Epoch 970: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0479 - accuracy: 0.4386 - val_loss: 1.0666 - val_accuracy: 0.4210 - lr: 4.3980e-05\n",
      "Epoch 971/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0487 - accuracy: 0.4351\n",
      "Epoch 971: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0487 - accuracy: 0.4351 - val_loss: 1.0433 - val_accuracy: 0.4420 - lr: 4.3980e-05\n",
      "Epoch 972/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0482 - accuracy: 0.4373\n",
      "Epoch 972: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0482 - accuracy: 0.4373 - val_loss: 1.0529 - val_accuracy: 0.4249 - lr: 4.3980e-05\n",
      "Epoch 973/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0482 - accuracy: 0.4377\n",
      "Epoch 973: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0482 - accuracy: 0.4377 - val_loss: 1.0492 - val_accuracy: 0.4368 - lr: 4.3980e-05\n",
      "Epoch 974/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0485 - accuracy: 0.4376\n",
      "Epoch 974: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0485 - accuracy: 0.4376 - val_loss: 1.0439 - val_accuracy: 0.4425 - lr: 4.3980e-05\n",
      "Epoch 975/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0486 - accuracy: 0.4377\n",
      "Epoch 975: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0485 - accuracy: 0.4377 - val_loss: 1.0416 - val_accuracy: 0.4477 - lr: 4.3980e-05\n",
      "Epoch 976/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0489 - accuracy: 0.4362\n",
      "Epoch 976: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0489 - accuracy: 0.4361 - val_loss: 1.0422 - val_accuracy: 0.4505 - lr: 4.3980e-05\n",
      "Epoch 977/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0484 - accuracy: 0.4371\n",
      "Epoch 977: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0484 - accuracy: 0.4372 - val_loss: 1.0409 - val_accuracy: 0.4459 - lr: 4.3980e-05\n",
      "Epoch 978/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0478 - accuracy: 0.4379\n",
      "Epoch 978: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0478 - accuracy: 0.4379 - val_loss: 1.0403 - val_accuracy: 0.4513 - lr: 4.3980e-05\n",
      "Epoch 979/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0491 - accuracy: 0.4354\n",
      "Epoch 979: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0491 - accuracy: 0.4354 - val_loss: 1.0398 - val_accuracy: 0.4485 - lr: 4.3980e-05\n",
      "Epoch 980/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0485 - accuracy: 0.4368\n",
      "Epoch 980: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0485 - accuracy: 0.4368 - val_loss: 1.0441 - val_accuracy: 0.4456 - lr: 4.3980e-05\n",
      "Epoch 981/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0498 - accuracy: 0.4352\n",
      "Epoch 981: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0498 - accuracy: 0.4352 - val_loss: 1.0390 - val_accuracy: 0.4547 - lr: 4.3980e-05\n",
      "Epoch 982/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0493 - accuracy: 0.4353\n",
      "Epoch 982: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0493 - accuracy: 0.4353 - val_loss: 1.0402 - val_accuracy: 0.4493 - lr: 4.3980e-05\n",
      "Epoch 983/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0485 - accuracy: 0.4371\n",
      "Epoch 983: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0486 - accuracy: 0.4371 - val_loss: 1.0485 - val_accuracy: 0.4370 - lr: 4.3980e-05\n",
      "Epoch 984/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0493 - accuracy: 0.4366\n",
      "Epoch 984: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0493 - accuracy: 0.4366 - val_loss: 1.0416 - val_accuracy: 0.4477 - lr: 4.3980e-05\n",
      "Epoch 985/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0483 - accuracy: 0.4381\n",
      "Epoch 985: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0483 - accuracy: 0.4381 - val_loss: 1.0464 - val_accuracy: 0.4411 - lr: 4.3980e-05\n",
      "Epoch 986/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0501 - accuracy: 0.4347\n",
      "Epoch 986: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0501 - accuracy: 0.4347 - val_loss: 1.0493 - val_accuracy: 0.4376 - lr: 4.3980e-05\n",
      "Epoch 987/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0494 - accuracy: 0.4362\n",
      "Epoch 987: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0494 - accuracy: 0.4362 - val_loss: 1.0404 - val_accuracy: 0.4496 - lr: 4.3980e-05\n",
      "Epoch 988/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0492 - accuracy: 0.4362\n",
      "Epoch 988: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0492 - accuracy: 0.4363 - val_loss: 1.0395 - val_accuracy: 0.4565 - lr: 4.3980e-05\n",
      "Epoch 989/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0494 - accuracy: 0.4361\n",
      "Epoch 989: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 98ms/step - loss: 1.0494 - accuracy: 0.4361 - val_loss: 1.0401 - val_accuracy: 0.4516 - lr: 4.3980e-05\n",
      "Epoch 990/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0498 - accuracy: 0.4354\n",
      "Epoch 990: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0499 - accuracy: 0.4354 - val_loss: 1.0389 - val_accuracy: 0.4534 - lr: 4.3980e-05\n",
      "Epoch 991/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0501 - accuracy: 0.4354\n",
      "Epoch 991: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0501 - accuracy: 0.4354 - val_loss: 1.0470 - val_accuracy: 0.4331 - lr: 4.3980e-05\n",
      "Epoch 992/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0493 - accuracy: 0.4368\n",
      "Epoch 992: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0493 - accuracy: 0.4368 - val_loss: 1.0565 - val_accuracy: 0.4280 - lr: 4.3980e-05\n",
      "Epoch 993/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0496 - accuracy: 0.4358\n",
      "Epoch 993: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0496 - accuracy: 0.4358 - val_loss: 1.0697 - val_accuracy: 0.4221 - lr: 4.3980e-05\n",
      "Epoch 994/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0496 - accuracy: 0.4353\n",
      "Epoch 994: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 116s 99ms/step - loss: 1.0496 - accuracy: 0.4353 - val_loss: 1.0411 - val_accuracy: 0.4509 - lr: 4.3980e-05\n",
      "Epoch 995/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0499 - accuracy: 0.4364\n",
      "Epoch 995: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 117s 100ms/step - loss: 1.0499 - accuracy: 0.4364 - val_loss: 1.0534 - val_accuracy: 0.4320 - lr: 4.3980e-05\n",
      "Epoch 996/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0503 - accuracy: 0.4356\n",
      "Epoch 996: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0504 - accuracy: 0.4356 - val_loss: 1.0462 - val_accuracy: 0.4421 - lr: 4.3980e-05\n",
      "Epoch 997/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0497 - accuracy: 0.4359\n",
      "Epoch 997: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0497 - accuracy: 0.4359 - val_loss: 1.0519 - val_accuracy: 0.4280 - lr: 4.3980e-05\n",
      "Epoch 998/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0501 - accuracy: 0.4345\n",
      "Epoch 998: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0501 - accuracy: 0.4345 - val_loss: 1.0524 - val_accuracy: 0.4269 - lr: 4.3980e-05\n",
      "Epoch 999/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0502 - accuracy: 0.4348\n",
      "Epoch 999: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0502 - accuracy: 0.4348 - val_loss: 1.0858 - val_accuracy: 0.4039 - lr: 4.3980e-05\n",
      "Epoch 1000/1000\n",
      "1166/1167 [============================>.] - ETA: 0s - loss: 1.0506 - accuracy: 0.4350\n",
      "Epoch 1000: val_loss did not improve from 1.01934\n",
      "1167/1167 [==============================] - 115s 99ms/step - loss: 1.0506 - accuracy: 0.4350 - val_loss: 1.0514 - val_accuracy: 0.4298 - lr: 4.3980e-05\n"
     ]
    }
   ],
   "source": [
    "# 5. 모델 생성 및 학습\n",
    "input_shape = X.shape[1:]  # 10분 동안의 데이터 \n",
    "model = create_transformer_model(input_shape, num_heads=4, ff_dim=128, d_model=128, num_classes=3)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs=1000, \n",
    "                    batch_size=128, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    class_weight=class_weights_dict,\n",
    "                    callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 13s 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.33      0.40     13183\n",
      "           1       0.37      0.50      0.43     11301\n",
      "           2       0.45      0.47      0.46     12836\n",
      "\n",
      "    accuracy                           0.43     37320\n",
      "   macro avg       0.44      0.43      0.43     37320\n",
      "weighted avg       0.44      0.43      0.43     37320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 자세한 성능 평가\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 시각화\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 정확도 시각화\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동 행렬 계산 및 시각화\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

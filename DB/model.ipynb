{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import conn\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gstc_code(code):\n",
    "    try:\n",
    "        conn.connect_to_database()\n",
    "        query = f'''\n",
    "            SELECT ts.GSTC_CODE\n",
    "              FROM TB_STOCKCLASSIFY ts\n",
    "             WHERE ts.KSTC_CODE = '{code}'\n",
    "        '''\n",
    "        \n",
    "        conn.global_cursor.execute(query)\n",
    "        df = pd.read_sql(query, conn.global_conn)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error occurred while fetching data from database: {e}')\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close_database_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MariaDB 서버에 성공적으로 연결되었습니다. 서버 버전: 10.4.34-MariaDB\n",
      "현재 사용 중인 데이터베이스: cryptoStockTrading\n",
      "MariaDB 연결이 종료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "df = get_gstc_code('005930')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(gstc_code):\n",
    "    try:\n",
    "        conn.connect_to_database()\n",
    "        query = f'''\n",
    "            SELECT td.STCK_BSOP_DATE, td.STCK_CLPR, td.STCK_OPRC, td.STCK_HGPR, td.STCK_LWPR, td.ACML_VOL, td.ACML_TR_PBMN\n",
    "              FROM TB_DAILYSTOCK td\n",
    "             WHERE td.GSTC_CODE = '{gstc_code}'\n",
    "             ORDER BY td.STCK_BSOP_DATE ASC\n",
    "        '''\n",
    "        \n",
    "        conn.global_cursor.execute(query)\n",
    "        df = pd.read_sql(query, conn.global_conn)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error occurred while fetching data from database: {e}')\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close_database_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(code):\n",
    "    gstc_df = get_gstc_code(code)\n",
    "    gstc_code = gstc_df.iloc[0, 0]\n",
    "    data = select_data(gstc_code)\n",
    "    \n",
    "    data = data.drop_duplicates(subset='STCK_BSOP_DATE')\n",
    "    \n",
    "    data['STCK_BSOP_DATE'] = pd.to_datetime(data['STCK_BSOP_DATE'], format='%Y%m%d')\n",
    "    data = data.sort_values('STCK_BSOP_DATE').reset_index(drop=True)\n",
    "\n",
    "    columns_to_convert = ['STCK_CLPR', 'STCK_OPRC', 'STCK_HGPR', 'STCK_LWPR', 'ACML_VOL', 'ACML_TR_PBMN']\n",
    "    data[columns_to_convert] = data[columns_to_convert].astype(float)\n",
    "\n",
    "    data['next_day_close'] = data['STCK_CLPR'].shift(-1)\n",
    "    data['label'] = np.where(data['next_day_close'] > data['STCK_CLPR'], 1, 0)\n",
    "    data = data.iloc[:-1]\n",
    "    \n",
    "    # Moving Averages\n",
    "    data['MA5'] = data['STCK_CLPR'].rolling(window=5).mean()\n",
    "    data['MA10'] = data['STCK_CLPR'].rolling(window=10).mean()\n",
    "    data['MA20'] = data['STCK_CLPR'].rolling(window=20).mean()\n",
    "    data['MA50'] = data['STCK_CLPR'].rolling(window=50).mean()\n",
    "\n",
    "    # Exponential Moving Averages\n",
    "    data['EMA5'] = data['STCK_CLPR'].ewm(span=5, adjust=False).mean()\n",
    "    data['EMA10'] = data['STCK_CLPR'].ewm(span=10, adjust=False).mean()\n",
    "    data['EMA20'] = data['STCK_CLPR'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "    # Relative Strength Index (RSI)\n",
    "    delta = data['STCK_CLPR'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    ema_up = up.ewm(com=13, adjust=False).mean()\n",
    "    ema_down = down.ewm(com=13, adjust=False).mean()\n",
    "    rs = ema_up / ema_down\n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Moving Average Convergence Divergence (MACD)\n",
    "    exp1 = data['STCK_CLPR'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = data['STCK_CLPR'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = exp1 - exp2\n",
    "    data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    data['20_day_MA'] = data['STCK_CLPR'].rolling(window=20).mean()\n",
    "    data['20_day_STD'] = data['STCK_CLPR'].rolling(window=20).std()\n",
    "    data['Bollinger_High'] = data['20_day_MA'] + (data['20_day_STD'] * 2)\n",
    "    data['Bollinger_Low'] = data['20_day_MA'] - (data['20_day_STD'] * 2)\n",
    "\n",
    "    # Stochastic Oscillator\n",
    "    low14 = data['STCK_LWPR'].rolling(window=14).min()\n",
    "    high14 = data['STCK_HGPR'].rolling(window=14).max()\n",
    "    data['%K'] = 100 * ((data['STCK_CLPR'] - low14) / (high14 - low14))\n",
    "\n",
    "    data['%D'] = data['%K'].rolling(window=3).mean()\n",
    "\n",
    "    # Drop rows with NaN values resulting from technical indicator calculations\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))  # [d_model/2]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_timesteps, num_features, feature_size=128, num_layers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(feature_size, dropout, num_timesteps)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=feature_size, nhead=8, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.embedding = nn.Linear(num_features, feature_size)\n",
    "        self.decoder = nn.Linear(feature_size, 2)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, seq_len, num_features]\n",
    "        src = self.embedding(src) * np.sqrt(self.feature_size)\n",
    "        src = self.pos_encoder(src)\n",
    "        # Transformer expects input of shape (seq_len, batch_size, feature_size)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Take the output from the last time step\n",
    "        output = output[-1, :, :]\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MariaDB 서버에 성공적으로 연결되었습니다. 서버 버전: 10.4.34-MariaDB\n",
      "현재 사용 중인 데이터베이스: cryptoStockTrading\n",
      "MariaDB 연결이 종료되었습니다.\n",
      "MariaDB 서버에 성공적으로 연결되었습니다. 서버 버전: 10.4.34-MariaDB\n",
      "현재 사용 중인 데이터베이스: cryptoStockTrading\n",
      "MariaDB 연결이 종료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "data = data_preprocess('005930')\n",
    "\n",
    "# Define features and labels\n",
    "features = data.drop(['STCK_BSOP_DATE', 'next_day_close', 'label'], axis=1)\n",
    "labels = data['label']\n",
    "\n",
    "# Convert to numpy arrays\n",
    "features = features.values\n",
    "labels = labels.values\n",
    "\n",
    "# Define sequence length\n",
    "sequence_length = 60  # Using 5 days of historical data\n",
    "\n",
    "# Create sequences\n",
    "X = []\n",
    "y = []\n",
    "for i in range(sequence_length, len(features)):\n",
    "    X.append(features[i-sequence_length:i])\n",
    "    y.append(labels[i])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "# Reshape for scaling\n",
    "num_samples_train, num_timesteps, num_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "X_test_reshaped = X_test.reshape(-1, num_features)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_reshaped)\n",
    "X_train_scaled = scaler.transform(X_train_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "\n",
    "with open('./scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape back to original shape\n",
    "X_train = X_train_scaled.reshape(num_samples_train, num_timesteps, num_features)\n",
    "X_test = X_test_scaled.reshape(X_test.shape[0], num_timesteps, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "feature_size = 64\n",
    "model = TransformerModel(num_timesteps=num_timesteps, num_features=num_features, feature_size=feature_size, num_layers=2, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embedding): Linear(in_features=22, out_features=64, bias=True)\n",
       "  (decoder): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    accuracy = correct / total\n",
    "    return accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.7353\n",
      "Validation Accuracy: 0.5308\n",
      "Best model saved to ./model.pth with accuracy: 0.5308\n",
      "Epoch 2/20, Loss: 0.6963\n",
      "Validation Accuracy: 0.4692\n",
      "Epoch 3/20, Loss: 0.6950\n",
      "Validation Accuracy: 0.5308\n",
      "Epoch 4/20, Loss: 0.6945\n",
      "Validation Accuracy: 0.5308\n",
      "Epoch 5/20, Loss: 0.6940\n",
      "Validation Accuracy: 0.5308\n",
      "Epoch 6/20, Loss: 0.6916\n",
      "Validation Accuracy: 0.4692\n",
      "Epoch 7/20, Loss: 0.6918\n",
      "Validation Accuracy: 0.4700\n",
      "Epoch 8/20, Loss: 0.6934\n",
      "Validation Accuracy: 0.5025\n",
      "Epoch 9/20, Loss: 0.6957\n",
      "Validation Accuracy: 0.4658\n",
      "Epoch 10/20, Loss: 0.6915\n",
      "Validation Accuracy: 0.4692\n",
      "Epoch 11/20, Loss: 0.6912\n",
      "Validation Accuracy: 0.4692\n",
      "Epoch 12/20, Loss: 0.6923\n",
      "Validation Accuracy: 0.5317\n",
      "Best model saved to ./model.pth with accuracy: 0.5317\n",
      "Epoch 13/20, Loss: 0.6920\n",
      "Validation Accuracy: 0.4650\n",
      "Epoch 14/20, Loss: 0.6913\n",
      "Validation Accuracy: 0.4700\n",
      "Epoch 15/20, Loss: 0.6884\n",
      "Validation Accuracy: 0.4692\n",
      "Epoch 16/20, Loss: 0.6937\n",
      "Validation Accuracy: 0.5317\n",
      "Epoch 17/20, Loss: 0.6905\n",
      "Validation Accuracy: 0.4683\n",
      "Epoch 18/20, Loss: 0.6908\n",
      "Validation Accuracy: 0.4717\n",
      "Epoch 19/20, Loss: 0.6907\n",
      "Validation Accuracy: 0.4683\n",
      "Epoch 20/20, Loss: 0.6888\n",
      "Validation Accuracy: 0.4958\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_accuracy = 0.0\n",
    "model_save_path = './model.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy, _, _ = evaluate(model, test_loader)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Best model saved to {model_save_path} with accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4958\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.25      0.34       637\n",
      "           1       0.48      0.77      0.59       563\n",
      "\n",
      "    accuracy                           0.50      1200\n",
      "   macro avg       0.52      0.51      0.47      1200\n",
      "weighted avg       0.52      0.50      0.46      1200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[159 478]\n",
      " [127 436]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy, test_preds, test_labels = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MariaDB 서버에 성공적으로 연결되었습니다. 서버 버전: 10.4.34-MariaDB\n",
      "현재 사용 중인 데이터베이스: cryptoStockTrading\n",
      "MariaDB 연결이 종료되었습니다.\n",
      "MariaDB 서버에 성공적으로 연결되었습니다. 서버 버전: 10.4.34-MariaDB\n",
      "현재 사용 중인 데이터베이스: cryptoStockTrading\n",
      "MariaDB 연결이 종료되었습니다.\n",
      "MariaDB 서버에 성공적으로 연결되었습니다. 서버 버전: 10.4.34-MariaDB\n",
      "현재 사용 중인 데이터베이스: cryptoStockTrading\n",
      "MariaDB 연결이 종료되었습니다.\n",
      "Epoch 1/20, Loss: 0.8460\n",
      "Validation Accuracy: 0.4693\n",
      "Best model saved to ./model.pth with accuracy: 0.4693\n",
      "Epoch 2/20, Loss: 0.8242\n",
      "Validation Accuracy: 0.4726\n",
      "Best model saved to ./model.pth with accuracy: 0.4726\n",
      "Epoch 3/20, Loss: 0.8230\n",
      "Validation Accuracy: 0.4834\n",
      "Best model saved to ./model.pth with accuracy: 0.4834\n",
      "Epoch 4/20, Loss: 0.8240\n",
      "Validation Accuracy: 0.4693\n",
      "Epoch 5/20, Loss: 0.8205\n",
      "Validation Accuracy: 0.4693\n",
      "Epoch 6/20, Loss: 0.8234\n",
      "Validation Accuracy: 0.4693\n",
      "Epoch 7/20, Loss: 0.8222\n",
      "Validation Accuracy: 0.4834\n",
      "Epoch 8/20, Loss: 0.8220\n",
      "Validation Accuracy: 0.4710\n",
      "Epoch 9/20, Loss: 0.8211\n",
      "Validation Accuracy: 0.4693\n",
      "Epoch 10/20, Loss: 0.8198\n",
      "Validation Accuracy: 0.4950\n",
      "Best model saved to ./model.pth with accuracy: 0.4950\n",
      "Epoch 11/20, Loss: 0.8200\n",
      "Validation Accuracy: 0.4726\n",
      "Epoch 12/20, Loss: 0.8169\n",
      "Validation Accuracy: 0.4735\n",
      "Epoch 13/20, Loss: 0.8226\n",
      "Validation Accuracy: 0.4635\n",
      "Epoch 14/20, Loss: 0.8188\n",
      "Validation Accuracy: 0.4784\n",
      "Epoch 15/20, Loss: 0.8186\n",
      "Validation Accuracy: 0.4619\n",
      "Epoch 16/20, Loss: 0.8167\n",
      "Validation Accuracy: 0.4826\n",
      "Epoch 17/20, Loss: 0.8174\n",
      "Validation Accuracy: 0.4693\n",
      "Epoch 18/20, Loss: 0.8183\n",
      "Validation Accuracy: 0.4726\n",
      "Epoch 19/20, Loss: 0.8202\n",
      "Validation Accuracy: 0.4627\n",
      "Epoch 20/20, Loss: 0.8186\n",
      "Validation Accuracy: 0.4900\n",
      "Test Accuracy: 0.4900\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4886    0.7354    0.5871       582\n",
      "           1     0.0000    0.0000    0.0000        58\n",
      "           2     0.4939    0.2880    0.3638       566\n",
      "\n",
      "    accuracy                         0.4900      1206\n",
      "   macro avg     0.3275    0.3411    0.3170      1206\n",
      "weighted avg     0.4676    0.4900    0.4541      1206\n",
      "\n",
      "Confusion Matrix:\n",
      "[[428   0 154]\n",
      " [ 45   0  13]\n",
      " [403   0 163]]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import conn\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_gstc_code(code):\n",
    "    try:\n",
    "        conn.connect_to_database()\n",
    "        query = f'''\n",
    "            SELECT ts.GSTC_CODE\n",
    "              FROM TB_STOCKCLASSIFY ts\n",
    "             WHERE ts.KSTC_CODE = '{code}'\n",
    "        '''\n",
    "        \n",
    "        conn.global_cursor.execute(query)\n",
    "        df = pd.read_sql(query, conn.global_conn)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error occurred while fetching data from database: {e}')\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close_database_connection()\n",
    "\n",
    "df = get_gstc_code('005930')\n",
    "\n",
    "def select_data(gstc_code):\n",
    "    try:\n",
    "        conn.connect_to_database()\n",
    "        query = f'''\n",
    "            SELECT td.STCK_BSOP_DATE, td.STCK_CLPR, td.STCK_OPRC, td.STCK_HGPR, td.STCK_LWPR, td.ACML_VOL, td.ACML_TR_PBMN\n",
    "              FROM TB_DAILYSTOCK td\n",
    "             WHERE td.GSTC_CODE = '{gstc_code}'\n",
    "             ORDER BY td.STCK_BSOP_DATE ASC\n",
    "        '''\n",
    "        \n",
    "        conn.global_cursor.execute(query)\n",
    "        df = pd.read_sql(query, conn.global_conn)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error occurred while fetching data from database: {e}')\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close_database_connection()\n",
    "\n",
    "def data_preprocess(code):\n",
    "    gstc_df = get_gstc_code(code)\n",
    "    gstc_code = gstc_df.iloc[0, 0]\n",
    "    data = select_data(gstc_code)\n",
    "    \n",
    "    data = data.drop_duplicates(subset='STCK_BSOP_DATE')\n",
    "    \n",
    "    data['STCK_BSOP_DATE'] = pd.to_datetime(data['STCK_BSOP_DATE'], format='%Y%m%d')\n",
    "    data = data.sort_values('STCK_BSOP_DATE').reset_index(drop=True)\n",
    "\n",
    "    columns_to_convert = ['STCK_CLPR', 'STCK_OPRC', 'STCK_HGPR', 'STCK_LWPR', 'ACML_VOL', 'ACML_TR_PBMN']\n",
    "    data[columns_to_convert] = data[columns_to_convert].astype(float)\n",
    "\n",
    "    data['next_day_close'] = data['STCK_CLPR'].shift(-1)\n",
    "    \n",
    "    # 3진 분류 라벨링\n",
    "    # 0: 하락, 1: 변동 없음, 2: 상승\n",
    "    data['label'] = np.where(\n",
    "        data['next_day_close'] > data['STCK_CLPR'], \n",
    "        2, \n",
    "        np.where(data['next_day_close'] < data['STCK_CLPR'], 0, 1)\n",
    "    )\n",
    "    data = data.iloc[:-1]\n",
    "    \n",
    "    # 이동 평균\n",
    "    data['MA5'] = data['STCK_CLPR'].rolling(window=5).mean()\n",
    "    data['MA10'] = data['STCK_CLPR'].rolling(window=10).mean()\n",
    "    data['MA20'] = data['STCK_CLPR'].rolling(window=20).mean()\n",
    "    data['MA50'] = data['STCK_CLPR'].rolling(window=50).mean()\n",
    "\n",
    "    # 지수 이동 평균\n",
    "    data['EMA5'] = data['STCK_CLPR'].ewm(span=5, adjust=False).mean()\n",
    "    data['EMA10'] = data['STCK_CLPR'].ewm(span=10, adjust=False).mean()\n",
    "    data['EMA20'] = data['STCK_CLPR'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "    # 상대 강도 지수 (RSI)\n",
    "    delta = data['STCK_CLPR'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    ema_up = up.ewm(com=13, adjust=False).mean()\n",
    "    ema_down = down.ewm(com=13, adjust=False).mean()\n",
    "    rs = ema_up / ema_down\n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # 이동 평균 수렴 발산 (MACD)\n",
    "    exp1 = data['STCK_CLPR'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = data['STCK_CLPR'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = exp1 - exp2\n",
    "    data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # 볼린저 밴드\n",
    "    data['20_day_MA'] = data['STCK_CLPR'].rolling(window=20).mean()\n",
    "    data['20_day_STD'] = data['STCK_CLPR'].rolling(window=20).std()\n",
    "    data['Bollinger_High'] = data['20_day_MA'] + (data['20_day_STD'] * 2)\n",
    "    data['Bollinger_Low'] = data['20_day_MA'] - (data['20_day_STD'] * 2)\n",
    "\n",
    "    # 스토캐스틱 오실레이터\n",
    "    low14 = data['STCK_LWPR'].rolling(window=14).min()\n",
    "    high14 = data['STCK_HGPR'].rolling(window=14).max()\n",
    "    data['%K'] = 100 * ((data['STCK_CLPR'] - low14) / (high14 - low14))\n",
    "\n",
    "    data['%D'] = data['%K'].rolling(window=3).mean()\n",
    "\n",
    "    # 기술 지표 계산으로 인해 발생하는 NaN 값 제거\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))  # [d_model/2]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_timesteps, num_features, feature_size=128, num_layers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(feature_size, dropout, num_timesteps)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=feature_size, nhead=8, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.embedding = nn.Linear(num_features, feature_size)\n",
    "        self.decoder = nn.Linear(feature_size, 3)  # 출력 클래스 수를 3으로 변경\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, seq_len, num_features]\n",
    "        src = self.embedding(src) * np.sqrt(self.feature_size)\n",
    "        src = self.pos_encoder(src)\n",
    "        # Transformer expects input of shape (seq_len, batch_size, feature_size)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # 마지막 시점의 출력 사용\n",
    "        output = output[-1, :, :]\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "data = data_preprocess('005930')\n",
    "\n",
    "# Define features and labels\n",
    "features = data.drop(['STCK_BSOP_DATE', 'next_day_close', 'label'], axis=1)\n",
    "labels = data['label']\n",
    "\n",
    "# Convert to numpy arrays\n",
    "features = features.values\n",
    "labels = labels.values\n",
    "\n",
    "# Define sequence length\n",
    "sequence_length = 30  # Using 60 timesteps\n",
    "\n",
    "# Create sequences\n",
    "X = []\n",
    "y = []\n",
    "for i in range(sequence_length, len(features)):\n",
    "    X.append(features[i-sequence_length:i])\n",
    "    y.append(labels[i])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "# Reshape for scaling\n",
    "num_samples_train, num_timesteps, num_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "X_test_reshaped = X_test.reshape(-1, num_features)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_reshaped)\n",
    "X_train_scaled = scaler.transform(X_train_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "\n",
    "# 스케일러 저장\n",
    "with open('./scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape back to original shape\n",
    "X_train = X_train_scaled.reshape(num_samples_train, num_timesteps, num_features)\n",
    "X_test = X_test_scaled.reshape(X_test.shape[0], num_timesteps, num_features)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "feature_size = 64\n",
    "model = TransformerModel(num_timesteps=num_timesteps, num_features=num_features, feature_size=feature_size, num_layers=2, dropout=0.1)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    accuracy = correct / total\n",
    "    return accuracy, all_preds, all_labels\n",
    "\n",
    "# %%\n",
    "num_epochs = 20\n",
    "best_accuracy = 0.0\n",
    "model_save_path = './model.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy, _, _ = evaluate(model, test_loader)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Best model saved to {model_save_path} with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy, test_preds, test_labels = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
